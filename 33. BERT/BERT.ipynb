{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zqb1fkHZwhNN"
   },
   "outputs": [],
   "source": [
    "#BERT understanding\n",
    "#https://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804,
     "referenced_widgets": [
      "acafc6d529024f64a9c5b40b3670d4d2",
      "6f8bfd51aa0e447fab242a7baef92039",
      "bfeb128b088a4443a4339dc0b42de33b",
      "e7d676aae4c9457c964191bf9793976e",
      "8e02f55ada2248fea95b7fbf226e6f58",
      "7cd9583072a848a897d6315c37f8cccb",
      "422a46003d45402eadc0b174fca0dab2",
      "d50e2d9bb3d2420887d6619b1dfe0278",
      "72b41d65443943b2a18e3e824ae89d40",
      "75254040faf740aeb88b3c9d7dcaf896",
      "1de7382f4a2842eb94a088208d7005fa",
      "6a85bcb83f1b4e4da15b74821ba05a98",
      "5201361c41e34d60804db17e71c3b820",
      "2007f72b75b04df4a1512a533863019d",
      "8361688e1c9a4116b73a56e495c21398",
      "b9707016e5664783b7b48be429dd4e36",
      "6fbcd29b821a4cfd84ac25dacbe75887",
      "a6b5fed0d8b643678d485677d3c0b4d5",
      "e3f5a07e42764bb6b0919df4f21ea92a",
      "27af4789fc0047ab9e9574879c155f91",
      "373612339a914805ad3fee2636ca837e",
      "711373a7e8fa42bf8e17b9e8512af798",
      "ce57c2cf35f4494cb5c48a5d1ff91896",
      "2c796fba1e624dbca87dc890f15fe841",
      "65f57146d3ad496ab3734df4d0f72368",
      "2aac223faea74855bfb300e045e88df4",
      "269ede2acdfd4661b56b4c6b1dbf60a7",
      "23f5dd3371d24ffa8b82c0f38f03095a",
      "41ff547efab8432883e7af8aa91cb41a",
      "2774499802124421ae66fdf108a4afb2",
      "dbdefb8bf1aa401aa8f2fb0a313209fd",
      "a06f282e1bb349658d05d7953f008934",
      "1427110e6ecb4fac8fb2f2f24c97206c",
      "fb5aaf21290a4e8dbbbe9491cc868f69",
      "b4ed82dab66f43a9a249db88469c5055",
      "0c8b419342dc4ac8bd62d3b9210c9197",
      "bcab05b9157644fc89bd852eb56f9e89",
      "d70d3e090e3648cd899e4231012f7802",
      "89868ed1915f4d79976c83371dfaeebb",
      "0119e63c9c554fb694b652ff88b42853",
      "532c0478d3fc47fbad4217184bedac1b",
      "98cad27072744ccaab57be5b0872fa06",
      "00a633ebef8241e58b73a6ef2c42c40b",
      "8ff3ecbcdd0947b781b5e30661670aff",
      "9584ba5cbf6149a8afb64a2dd3e94166",
      "f6a2a004b5ae4445abc3c1e0e40d9ee5",
      "b6d3ce6db1c14e92b0cfe4a03815d285",
      "27bc88f5edc3415ca906ed10e093053b",
      "11c02e828d7545dc8d7a0f3753884f2c",
      "b6ffb29a4d5141f7886840cf7778db83",
      "daaa6d996ba144a791d76cda9c6c8ff0",
      "99d74e283bd24371a9ed8858bac00cfe",
      "fd5f2059f3fa445c923dfc044d53609c",
      "c09fdeec476048fa89a8407d67ab3dd4",
      "631130b1beb744d4a3bd7deed299809b"
     ]
    },
    "id": "-t-zB5-1uA7G",
    "outputId": "b418faa6-6d1d-4406-87d3-c7da155f0d59"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"This movie is great and I loved it!\",\n",
    "    \"Terrible film, very boring.\",\n",
    "    \"Amazing storyline and acting!\",\n",
    "    \"I didn't enjoy this at all.\",\n",
    "    \"Fantastic experience, highly recommend!\",\n",
    "    \"Really bad, waste of time.\",\n",
    "\n",
    "    \"One of the best movies I've seen this year!\",\n",
    "    \"Completely disappointing and predictable.\",\n",
    "    \"Brilliant direction and stunning visuals.\",\n",
    "    \"I fell asleep halfway through, so dull.\",\n",
    "    \"Heartwarming and beautifully shot.\",\n",
    "    \"Poor acting and weak script.\",\n",
    "\n",
    "    \"Absolutely loved the plot twists!\",\n",
    "    \"Not worth the hype at all.\",\n",
    "    \"Engaging from start to finish!\",\n",
    "    \"The worst film I’ve ever watched.\",\n",
    "    \"Incredible performances by the cast!\",\n",
    "    \"Script was a mess and pacing was off.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0,\n",
    "          1, 0, 1, 0, 1, 0,\n",
    "          1, 0, 1, 0, 1, 0]\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_texts(texts, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "train_input_ids, train_attention_mask = tokenize_texts(train_texts)\n",
    "val_input_ids, val_attention_mask = tokenize_texts(val_texts)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(val_labels, dtype=tf.int32)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_input_ids, 'attention_mask': train_attention_mask},\n",
    "    train_labels\n",
    ")).batch(2)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': val_input_ids, 'attention_mask': val_attention_mask},\n",
    "    val_labels\n",
    ")).batch(2)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Inference function\n",
    "def predict_sentiment(text, model, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        [text],\n",
    "        max_length=max_len,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    outputs = model({'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']})\n",
    "    logits = outputs.logits\n",
    "    prediction = tf.argmax(logits, axis=-1).numpy()[0]\n",
    "    print(logits)\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Test inference\n",
    "test_text = \"This is an awesome movie!\"\n",
    "result = predict_sentiment(test_text, model, tokenizer)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ie7QJU9ouOzp",
    "outputId": "35036419-01d3-490b-f4a5-4465293b9322"
   },
   "outputs": [],
   "source": [
    "# Test inference\n",
    "test_text = \"Movie is very boring in first half and in the second half the movie is worth watching \"\n",
    "result = predict_sentiment(test_text, model, tokenizer)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afe8htWIvLJr",
    "outputId": "78314e2d-2147-44fd-bd29-09827e8ee449"
   },
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_sentiment(text, model, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        [text],\n",
    "        max_length=max_len,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    outputs = model({'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']})\n",
    "    logits = outputs.logits\n",
    "    print(outputs)\n",
    "\n",
    "    # Calculate softmax probabilities\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    print(\"Softmax Probabilities:\", probabilities)\n",
    "\n",
    "    prediction = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
    "    print(\"Predicted Class Index:\", prediction)\n",
    "\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Test inference\n",
    "test_text = \"worst movie\"\n",
    "result = predict_sentiment(test_text, model, tokenizer)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ten2wOqyVP3",
    "outputId": "7cf82d4f-eb71-45c1-9970-825694267af3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "texts = [\n",
    "    \"This movie is great and I loved it!\",\n",
    "    \"Terrible film, very boring.\",\n",
    "    \"Amazing storyline and acting!\",\n",
    "    \"I didn't enjoy this at all.\",\n",
    "    \"Fantastic experience, highly recommend!\",\n",
    "    \"Really bad, waste of time.\",\n",
    "    \"One of the best movies I've seen this year!\",\n",
    "    \"Completely disappointing and predictable.\",\n",
    "    \"Brilliant direction and stunning visuals.\",\n",
    "    \"I fell asleep halfway through, so dull.\",\n",
    "    \"Heartwarming and beautifully shot.\",\n",
    "    \"Poor acting and weak script.\",\n",
    "    \"Absolutely loved the plot twists!\",\n",
    "    \"Not worth the hype at all.\",\n",
    "    \"Engaging from start to finish!\",\n",
    "    \"The worst film I’ve ever watched.\",\n",
    "    \"Incredible performances by the cast!\",\n",
    "    \"Script was a mess and pacing was off.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0,\n",
    "          1, 0, 1, 0, 1, 0,\n",
    "          1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Train/Val Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_texts(texts, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "# Tokenize data\n",
    "train_input_ids, train_attention_mask = tokenize_texts(train_texts)\n",
    "val_input_ids, val_attention_mask = tokenize_texts(val_texts)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(val_labels, dtype=tf.int32)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_input_ids, 'attention_mask': train_attention_mask},\n",
    "    train_labels\n",
    ")).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': val_input_ids, 'attention_mask': val_attention_mask},\n",
    "    val_labels\n",
    ")).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ===== Custom Model with Softmax =====\n",
    "\n",
    "# Load base BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input layers with dynamic shape (fix for error)\n",
    "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Get pooled output from BERT\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "pooled_output = bert_outputs.pooler_output  # CLS token output\n",
    "\n",
    "# Optional dropout\n",
    "dropout = tf.keras.layers.Dropout(0.3)(pooled_output)\n",
    "\n",
    "# Classification head with softmax\n",
    "output = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ===== Train the Model =====\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ===== Inference Function =====\n",
    "def predict_sentiment(text, model, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        [text],\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    probs = model({'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']}).numpy()\n",
    "    prediction = np.argmax(probs, axis=-1)[0]\n",
    "    print(f\"Probabilities: {probs}\")\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# ===== Test Inference =====\n",
    "test_text = \"This is an awesome movie!\"\n",
    "result = predict_sentiment(test_text, model, tokenizer)\n",
    "print(f\"\\nText: {test_text}\")\n",
    "print(f\"Predicted Sentiment: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4unNEA2N7NQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
