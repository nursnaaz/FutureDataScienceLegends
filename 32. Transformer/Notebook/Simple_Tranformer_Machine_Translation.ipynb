{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v3LAvbHcL7-b",
        "outputId": "ae92e0e5-4477-4a30-f9e6-ba4e5103be3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simplified Transformer with Keras Built-ins ===\n",
            "\n",
            "\n",
            "=== Training Level 1 (Keras Built-in) ===\n",
            "Level 1: 5 examples, vocab size: 14\n",
            "Training with 50 examples...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 422ms/step - loss: 1.8219 - masked_accuracy: 0.3401 - val_loss: 0.8744 - val_masked_accuracy: 0.8125\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.8591 - masked_accuracy: 0.7155 - val_loss: 0.7777 - val_masked_accuracy: 0.6250\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.7275 - masked_accuracy: 0.6977 - val_loss: 0.5015 - val_masked_accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.4735 - masked_accuracy: 0.9483 - val_loss: 0.2002 - val_masked_accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.1682 - masked_accuracy: 0.9845 - val_loss: 0.0450 - val_masked_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0490 - masked_accuracy: 1.0000 - val_loss: 0.0175 - val_masked_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0195 - masked_accuracy: 1.0000 - val_loss: 0.0089 - val_masked_accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0129 - masked_accuracy: 1.0000 - val_loss: 0.0057 - val_masked_accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - loss: 0.0083 - masked_accuracy: 1.0000 - val_loss: 0.0046 - val_masked_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - loss: 0.0069 - masked_accuracy: 1.0000 - val_loss: 0.0037 - val_masked_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0053 - masked_accuracy: 1.0000 - val_loss: 0.0031 - val_masked_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0047 - masked_accuracy: 1.0000 - val_loss: 0.0027 - val_masked_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0041 - masked_accuracy: 1.0000 - val_loss: 0.0024 - val_masked_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0037 - masked_accuracy: 1.0000 - val_loss: 0.0021 - val_masked_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0032 - masked_accuracy: 1.0000 - val_loss: 0.0019 - val_masked_accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - loss: 0.0031 - masked_accuracy: 1.0000 - val_loss: 0.0017 - val_masked_accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - loss: 0.0027 - masked_accuracy: 1.0000 - val_loss: 0.0016 - val_masked_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0025 - masked_accuracy: 1.0000 - val_loss: 0.0015 - val_masked_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - loss: 0.0022 - masked_accuracy: 1.0000 - val_loss: 0.0014 - val_masked_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - loss: 0.0021 - masked_accuracy: 1.0000 - val_loss: 0.0013 - val_masked_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 0.0020 - masked_accuracy: 1.0000 - val_loss: 0.0012 - val_masked_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 0.0018 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 0.0017 - masked_accuracy: 1.0000 - val_loss: 0.0010 - val_masked_accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 9.8360e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 0.0016 - masked_accuracy: 1.0000 - val_loss: 9.2383e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 8.6992e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.2036e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 7.7510e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0013 - masked_accuracy: 1.0000 - val_loss: 7.3403e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0012 - masked_accuracy: 1.0000 - val_loss: 6.9630e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0011 - masked_accuracy: 1.0000 - val_loss: 6.6247e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0010 - masked_accuracy: 1.0000 - val_loss: 6.3257e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0011 - masked_accuracy: 1.0000 - val_loss: 6.0590e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 9.8672e-04 - masked_accuracy: 1.0000 - val_loss: 5.8203e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 9.8731e-04 - masked_accuracy: 1.0000 - val_loss: 5.6022e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 9.0865e-04 - masked_accuracy: 1.0000 - val_loss: 5.4048e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 9.4938e-04 - masked_accuracy: 1.0000 - val_loss: 5.2201e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 8.6346e-04 - masked_accuracy: 1.0000 - val_loss: 5.0496e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 8.5262e-04 - masked_accuracy: 1.0000 - val_loss: 4.8925e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 8.0638e-04 - masked_accuracy: 1.0000 - val_loss: 4.7470e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 8.2360e-04 - masked_accuracy: 1.0000 - val_loss: 4.6083e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 7.2154e-04 - masked_accuracy: 1.0000 - val_loss: 4.4760e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 7.1438e-04 - masked_accuracy: 1.0000 - val_loss: 4.3552e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 6.9679e-04 - masked_accuracy: 1.0000 - val_loss: 4.2435e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 7.2703e-04 - masked_accuracy: 1.0000 - val_loss: 4.1456e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.5625e-04 - masked_accuracy: 1.0000 - val_loss: 4.0523e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.6742e-04 - masked_accuracy: 1.0000 - val_loss: 3.9660e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.4830e-04 - masked_accuracy: 1.0000 - val_loss: 3.8843e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.4530e-04 - masked_accuracy: 1.0000 - val_loss: 3.8082e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 6.5778e-04 - masked_accuracy: 1.0000 - val_loss: 3.7378e-04 - val_masked_accuracy: 1.0000\n",
            "\n",
            "=== Testing Level 1 ===\n",
            "'hello' -> 'வணக்கம்' (expected: 'வணக்கம்')\n",
            "'good' -> 'நல்ல' (expected: 'நல்ல')\n",
            "'thank' -> 'நன்றி' (expected: 'நன்றி')\n",
            "'water' -> 'தண்ணீர்' (expected: 'தண்ணீர்')\n",
            "'food' -> 'உணவு' (expected: 'உணவு')\n",
            "Level 1 Accuracy: 100.0%\n",
            "✅ Level 1 passed!\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Training Level 2 (Keras Built-in) ===\n",
            "Level 2: 3 examples, vocab size: 15\n",
            "Training with 48 examples...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 345ms/step - loss: 2.7507 - masked_accuracy: 0.2731 - val_loss: 0.7113 - val_masked_accuracy: 0.7917\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.6660 - masked_accuracy: 0.7743 - val_loss: 0.4073 - val_masked_accuracy: 0.9375\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.4393 - masked_accuracy: 0.8213 - val_loss: 0.3082 - val_masked_accuracy: 0.8542\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.3082 - masked_accuracy: 0.8962 - val_loss: 0.1805 - val_masked_accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1495 - masked_accuracy: 0.9963 - val_loss: 0.0916 - val_masked_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0842 - masked_accuracy: 0.9828 - val_loss: 0.0306 - val_masked_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0370 - masked_accuracy: 1.0000 - val_loss: 0.0156 - val_masked_accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0193 - masked_accuracy: 1.0000 - val_loss: 0.0089 - val_masked_accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0116 - masked_accuracy: 1.0000 - val_loss: 0.0069 - val_masked_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0091 - masked_accuracy: 1.0000 - val_loss: 0.0058 - val_masked_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0083 - masked_accuracy: 1.0000 - val_loss: 0.0048 - val_masked_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0076 - masked_accuracy: 1.0000 - val_loss: 0.0041 - val_masked_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0057 - masked_accuracy: 1.0000 - val_loss: 0.0035 - val_masked_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0053 - masked_accuracy: 1.0000 - val_loss: 0.0031 - val_masked_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0047 - masked_accuracy: 1.0000 - val_loss: 0.0028 - val_masked_accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0043 - masked_accuracy: 1.0000 - val_loss: 0.0026 - val_masked_accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0040 - masked_accuracy: 1.0000 - val_loss: 0.0024 - val_masked_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0035 - masked_accuracy: 1.0000 - val_loss: 0.0023 - val_masked_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0033 - masked_accuracy: 1.0000 - val_loss: 0.0022 - val_masked_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0031 - masked_accuracy: 1.0000 - val_loss: 0.0020 - val_masked_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0030 - masked_accuracy: 1.0000 - val_loss: 0.0019 - val_masked_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0031 - masked_accuracy: 1.0000 - val_loss: 0.0019 - val_masked_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0028 - masked_accuracy: 1.0000 - val_loss: 0.0018 - val_masked_accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0028 - masked_accuracy: 1.0000 - val_loss: 0.0017 - val_masked_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0029 - masked_accuracy: 1.0000 - val_loss: 0.0016 - val_masked_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0027 - masked_accuracy: 1.0000 - val_loss: 0.0016 - val_masked_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0024 - masked_accuracy: 1.0000 - val_loss: 0.0015 - val_masked_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0024 - masked_accuracy: 1.0000 - val_loss: 0.0015 - val_masked_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0024 - masked_accuracy: 1.0000 - val_loss: 0.0014 - val_masked_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0021 - masked_accuracy: 1.0000 - val_loss: 0.0014 - val_masked_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0023 - masked_accuracy: 1.0000 - val_loss: 0.0013 - val_masked_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0019 - masked_accuracy: 1.0000 - val_loss: 0.0013 - val_masked_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0022 - masked_accuracy: 1.0000 - val_loss: 0.0012 - val_masked_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0019 - masked_accuracy: 1.0000 - val_loss: 0.0012 - val_masked_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0018 - masked_accuracy: 1.0000 - val_loss: 0.0012 - val_masked_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0019 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0018 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0017 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0017 - masked_accuracy: 1.0000 - val_loss: 0.0010 - val_masked_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0016 - masked_accuracy: 1.0000 - val_loss: 0.0010 - val_masked_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0016 - masked_accuracy: 1.0000 - val_loss: 9.7476e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 9.5075e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0016 - masked_accuracy: 1.0000 - val_loss: 9.2767e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 9.0604e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.8510e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.6512e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.4580e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.2713e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0013 - masked_accuracy: 1.0000 - val_loss: 8.0900e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0013 - masked_accuracy: 1.0000 - val_loss: 7.9155e-04 - val_masked_accuracy: 1.0000\n",
            "\n",
            "=== Testing Level 2 ===\n",
            "'good morning' -> 'காலை வணக்கம்' (expected: 'காலை வணக்கம்')\n",
            "'thank you' -> 'நன்றி நீங்கள்' (expected: 'நன்றி நீங்கள்')\n",
            "'good night' -> 'இனிய இரவு' (expected: 'இனிய இரவு')\n",
            "Level 2 Accuracy: 100.0%\n",
            "✅ Level 2 passed!\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Training Level 3 (Keras Built-in) ===\n",
            "Level 3: 2 examples, vocab size: 16\n",
            "Training with 50 examples...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 347ms/step - loss: 2.2895 - masked_accuracy: 0.3944 - val_loss: 0.3712 - val_masked_accuracy: 0.8750\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2995 - masked_accuracy: 0.9356 - val_loss: 0.1028 - val_masked_accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0966 - masked_accuracy: 0.9871 - val_loss: 0.0295 - val_masked_accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0339 - masked_accuracy: 1.0000 - val_loss: 0.0171 - val_masked_accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0217 - masked_accuracy: 1.0000 - val_loss: 0.0115 - val_masked_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0144 - masked_accuracy: 1.0000 - val_loss: 0.0077 - val_masked_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0101 - masked_accuracy: 1.0000 - val_loss: 0.0056 - val_masked_accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0075 - masked_accuracy: 1.0000 - val_loss: 0.0044 - val_masked_accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0062 - masked_accuracy: 1.0000 - val_loss: 0.0037 - val_masked_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0051 - masked_accuracy: 1.0000 - val_loss: 0.0033 - val_masked_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0046 - masked_accuracy: 1.0000 - val_loss: 0.0029 - val_masked_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0042 - masked_accuracy: 1.0000 - val_loss: 0.0027 - val_masked_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0043 - masked_accuracy: 1.0000 - val_loss: 0.0025 - val_masked_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0036 - masked_accuracy: 1.0000 - val_loss: 0.0023 - val_masked_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0034 - masked_accuracy: 1.0000 - val_loss: 0.0022 - val_masked_accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0033 - masked_accuracy: 1.0000 - val_loss: 0.0021 - val_masked_accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0030 - masked_accuracy: 1.0000 - val_loss: 0.0020 - val_masked_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0029 - masked_accuracy: 1.0000 - val_loss: 0.0019 - val_masked_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0028 - masked_accuracy: 1.0000 - val_loss: 0.0018 - val_masked_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0027 - masked_accuracy: 1.0000 - val_loss: 0.0017 - val_masked_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0025 - masked_accuracy: 1.0000 - val_loss: 0.0017 - val_masked_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0024 - masked_accuracy: 1.0000 - val_loss: 0.0016 - val_masked_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0024 - masked_accuracy: 1.0000 - val_loss: 0.0015 - val_masked_accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0023 - masked_accuracy: 1.0000 - val_loss: 0.0015 - val_masked_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0023 - masked_accuracy: 1.0000 - val_loss: 0.0014 - val_masked_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0022 - masked_accuracy: 1.0000 - val_loss: 0.0014 - val_masked_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0020 - masked_accuracy: 1.0000 - val_loss: 0.0013 - val_masked_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0020 - masked_accuracy: 1.0000 - val_loss: 0.0013 - val_masked_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0020 - masked_accuracy: 1.0000 - val_loss: 0.0013 - val_masked_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0019 - masked_accuracy: 1.0000 - val_loss: 0.0012 - val_masked_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0019 - masked_accuracy: 1.0000 - val_loss: 0.0012 - val_masked_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0018 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0017 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0017 - masked_accuracy: 1.0000 - val_loss: 0.0011 - val_masked_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0016 - masked_accuracy: 1.0000 - val_loss: 0.0010 - val_masked_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 0.0010 - val_masked_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 9.9174e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 9.6604e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0015 - masked_accuracy: 1.0000 - val_loss: 9.4151e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 9.1836e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.9611e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0014 - masked_accuracy: 1.0000 - val_loss: 8.7461e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0013 - masked_accuracy: 1.0000 - val_loss: 8.5418e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0013 - masked_accuracy: 1.0000 - val_loss: 8.3450e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0012 - masked_accuracy: 1.0000 - val_loss: 8.1567e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0012 - masked_accuracy: 1.0000 - val_loss: 7.9755e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0012 - masked_accuracy: 1.0000 - val_loss: 7.8004e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0012 - masked_accuracy: 1.0000 - val_loss: 7.6312e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0012 - masked_accuracy: 1.0000 - val_loss: 7.4678e-04 - val_masked_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0011 - masked_accuracy: 1.0000 - val_loss: 7.3110e-04 - val_masked_accuracy: 1.0000\n",
            "\n",
            "=== Testing Level 3 ===\n",
            "'how are you' -> 'நீங்கள் எப்படி இருக்கிறீர்கள்' (expected: 'நீங்கள் எப்படி இருக்கிறீர்கள்')\n",
            "'what is this' -> 'இது என்ன ஆகும்' (expected: 'இது என்ன ஆகும்')\n",
            "Level 3 Accuracy: 100.0%\n",
            "✅ Level 3 passed!\n",
            "--------------------------------------------------\n",
            "\n",
            "🎯 Simplified training complete!\n",
            "\n",
            "=== Minimal Transformer (One-liner style) ===\n",
            "Minimal model created with 69,604 parameters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_12      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_13        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m6,400\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_12        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m6,400\u001b[0m │ input_layer_12[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_12[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│                     │                   │            │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│                     │                   │            │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│                     │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │      \u001b[38;5;34m6,500\u001b[0m │ layer_normalizat… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_12      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_13        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_12        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│                     │                   │            │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│                     │                   │            │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│                     │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span> │ layer_normalizat… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m69,604\u001b[0m (271.89 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,604</span> (271.89 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m69,604\u001b[0m (271.89 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,604</span> (271.89 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "\n",
        "class SimpleKerasTransformer(Model):\n",
        "    \"\"\"Simplified Transformer using built-in Keras components\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, max_seq_len=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Embeddings (Keras handles positional encoding internally in newer versions)\n",
        "        self.encoder_embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.decoder_embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "\n",
        "        # Positional encoding (simple learned embeddings)\n",
        "        self.encoder_pos_embedding = layers.Embedding(max_seq_len, d_model)\n",
        "        self.decoder_pos_embedding = layers.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Encoder layers (using built-in components)\n",
        "        self.encoder_layers = []\n",
        "        for _ in range(num_layers):\n",
        "            encoder_layer = {\n",
        "                'attention': layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads),\n",
        "                'ffn': tf.keras.Sequential([\n",
        "                    layers.Dense(d_model * 2, activation='relu'),\n",
        "                    layers.Dense(d_model)\n",
        "                ]),\n",
        "                'norm1': layers.LayerNormalization(),\n",
        "                'norm2': layers.LayerNormalization(),\n",
        "                'dropout': layers.Dropout(0.1)\n",
        "            }\n",
        "            self.encoder_layers.append(encoder_layer)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_layers = []\n",
        "        for _ in range(num_layers):\n",
        "            decoder_layer = {\n",
        "                'self_attention': layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads),\n",
        "                'cross_attention': layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads),\n",
        "                'ffn': tf.keras.Sequential([\n",
        "                    layers.Dense(d_model * 2, activation='relu'),\n",
        "                    layers.Dense(d_model)\n",
        "                ]),\n",
        "                'norm1': layers.LayerNormalization(),\n",
        "                'norm2': layers.LayerNormalization(),\n",
        "                'norm3': layers.LayerNormalization(),\n",
        "                'dropout': layers.Dropout(0.1)\n",
        "            }\n",
        "            self.decoder_layers.append(decoder_layer)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = layers.Dense(vocab_size)\n",
        "\n",
        "    def create_causal_mask(self, size):\n",
        "        \"\"\"Create causal mask for decoder self-attention\"\"\"\n",
        "        mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "        return mask[tf.newaxis, tf.newaxis, :, :]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_input, decoder_input = inputs\n",
        "\n",
        "        # Get sequence lengths\n",
        "        enc_seq_len = tf.shape(encoder_input)[1]\n",
        "        dec_seq_len = tf.shape(decoder_input)[1]\n",
        "\n",
        "        # Encoder\n",
        "        # Embeddings + positional encoding\n",
        "        enc_positions = tf.range(enc_seq_len)[tf.newaxis, :]\n",
        "        enc_emb = self.encoder_embedding(encoder_input)\n",
        "        enc_pos_emb = self.encoder_pos_embedding(enc_positions)\n",
        "        enc_output = enc_emb + enc_pos_emb\n",
        "\n",
        "        # Encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            # Self-attention\n",
        "            attn_output = layer['attention'](enc_output, enc_output, training=training)\n",
        "            attn_output = layer['dropout'](attn_output, training=training)\n",
        "            enc_output = layer['norm1'](enc_output + attn_output)\n",
        "\n",
        "            # Feed forward\n",
        "            ffn_output = layer['ffn'](enc_output)\n",
        "            ffn_output = layer['dropout'](ffn_output, training=training)\n",
        "            enc_output = layer['norm2'](enc_output + ffn_output)\n",
        "\n",
        "        # Decoder\n",
        "        # Embeddings + positional encoding\n",
        "        dec_positions = tf.range(dec_seq_len)[tf.newaxis, :]\n",
        "        dec_emb = self.decoder_embedding(decoder_input)\n",
        "        dec_pos_emb = self.decoder_pos_embedding(dec_positions)\n",
        "        dec_output = dec_emb + dec_pos_emb\n",
        "\n",
        "        # Create causal mask\n",
        "        causal_mask = self.create_causal_mask(dec_seq_len)\n",
        "\n",
        "        # Decoder layers\n",
        "        for layer in self.decoder_layers:\n",
        "            # Masked self-attention\n",
        "            self_attn_output = layer['self_attention'](\n",
        "                dec_output, dec_output,\n",
        "                attention_mask=causal_mask,\n",
        "                training=training\n",
        "            )\n",
        "            self_attn_output = layer['dropout'](self_attn_output, training=training)\n",
        "            dec_output = layer['norm1'](dec_output + self_attn_output)\n",
        "\n",
        "            # Cross-attention\n",
        "            cross_attn_output = layer['cross_attention'](\n",
        "                dec_output, enc_output, training=training\n",
        "            )\n",
        "            cross_attn_output = layer['dropout'](cross_attn_output, training=training)\n",
        "            dec_output = layer['norm2'](dec_output + cross_attn_output)\n",
        "\n",
        "            # Feed forward\n",
        "            ffn_output = layer['ffn'](dec_output)\n",
        "            ffn_output = layer['dropout'](ffn_output, training=training)\n",
        "            dec_output = layer['norm3'](dec_output + ffn_output)\n",
        "\n",
        "        # Final output\n",
        "        output = self.output_layer(dec_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Simplified data creation (same as before but cleaner)\n",
        "def create_simple_data(level=1):\n",
        "    \"\"\"Simplified data creation\"\"\"\n",
        "    data_levels = {\n",
        "        1: [(\"hello\", \"வணக்கம்\"), (\"good\", \"நல்ல\"), (\"thank\", \"நன்றி\"), (\"water\", \"தண்ணீர்\"), (\"food\", \"உணவு\")],\n",
        "        2: [(\"good morning\", \"காலை வணக்கம்\"), (\"thank you\", \"நன்றி நீங்கள்\"), (\"good night\", \"இனிய இரவு\")],\n",
        "        3: [(\"how are you\", \"நீங்கள் எப்படி இருக்கிறீர்கள்\"), (\"what is this\", \"இது என்ன ஆகும்\")],\n",
        "    }\n",
        "\n",
        "    examples = data_levels.get(level, data_levels[1])\n",
        "    max_len = 4 + level\n",
        "    return examples, max_len\n",
        "\n",
        "def prepare_data_simple(examples, max_len):\n",
        "    \"\"\"Simplified data preparation using Keras utilities\"\"\"\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocab = {\"<PAD>\": 0, \"<START>\": 1, \"<END>\": 2, \"<UNK>\": 3}\n",
        "\n",
        "    all_words = set()\n",
        "    for eng, tam in examples:\n",
        "        all_words.update(eng.split() + tam.split())\n",
        "\n",
        "    for word in sorted(all_words):\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    # Prepare sequences\n",
        "    eng_seqs, tam_input_seqs, tam_target_seqs = [], [], []\n",
        "\n",
        "    for eng, tam in examples:\n",
        "        # English (encoder input)\n",
        "        eng_tokens = [vocab.get(w, vocab[\"<UNK>\"]) for w in eng.split()]\n",
        "        eng_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [eng_tokens], maxlen=max_len, padding='post')[0]\n",
        "\n",
        "        # Tamil input (decoder input) - with START token\n",
        "        tam_tokens = [vocab[\"<START>\"]] + [vocab.get(w, vocab[\"<UNK>\"]) for w in tam.split()]\n",
        "        tam_input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [tam_tokens], maxlen=max_len, padding='post')[0]\n",
        "\n",
        "        # Tamil target (decoder output) - with END token\n",
        "        tam_target_tokens = [vocab.get(w, vocab[\"<UNK>\"]) for w in tam.split()] + [vocab[\"<END>\"]]\n",
        "        tam_target_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [tam_target_tokens], maxlen=max_len, padding='post')[0]\n",
        "\n",
        "        eng_seqs.append(eng_seq)\n",
        "        tam_input_seqs.append(tam_input_seq)\n",
        "        tam_target_seqs.append(tam_target_seq)\n",
        "\n",
        "    return (np.array(eng_seqs), np.array(tam_input_seqs), np.array(tam_target_seqs),\n",
        "            vocab, reverse_vocab)\n",
        "\n",
        "# Simplified loss and metrics using Keras built-ins\n",
        "def create_masked_loss():\n",
        "    \"\"\"Create masked loss using Keras built-in functionality\"\"\"\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "    def masked_loss(y_true, y_pred):\n",
        "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        masked_loss = loss * mask\n",
        "        return tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    return masked_loss\n",
        "\n",
        "def create_masked_accuracy():\n",
        "    \"\"\"Create masked accuracy metric\"\"\"\n",
        "    def masked_accuracy(y_true, y_pred):\n",
        "        y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
        "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "        accuracy = tf.cast(tf.equal(y_true, y_pred_class), tf.float32) * mask\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    return masked_accuracy\n",
        "\n",
        "# Simplified translation function\n",
        "def translate_simple(model, sentence, vocab, reverse_vocab, max_len):\n",
        "    \"\"\"Simplified translation using the model\"\"\"\n",
        "\n",
        "    # Encode input\n",
        "    words = sentence.split()\n",
        "    eng_seq = [vocab.get(w, vocab[\"<UNK>\"]) for w in words]\n",
        "    eng_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [eng_seq], maxlen=max_len, padding='post')\n",
        "\n",
        "    # Start with START token\n",
        "    decoder_input = [vocab[\"<START>\"]]\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        # Pad and predict\n",
        "        dec_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [decoder_input], maxlen=max_len, padding='post')\n",
        "\n",
        "        predictions = model([eng_input, dec_input], training=False)\n",
        "\n",
        "        # Get next token\n",
        "        next_token = tf.argmax(predictions[0, len(decoder_input)-1, :]).numpy()\n",
        "\n",
        "        if next_token == vocab[\"<END>\"] or next_token == vocab[\"<PAD>\"]:\n",
        "            break\n",
        "\n",
        "        decoder_input.append(next_token)\n",
        "\n",
        "    # Convert to words\n",
        "    words = [reverse_vocab.get(token, \"\") for token in decoder_input[1:]]  # Skip START\n",
        "    return \" \".join([w for w in words if w not in [\"<START>\", \"<END>\", \"<PAD>\", \"<UNK>\", \"\"]])\n",
        "\n",
        "# Simplified training function\n",
        "def train_simple_level(level=1):\n",
        "    \"\"\"Simplified training using built-in Keras components\"\"\"\n",
        "\n",
        "    print(f\"\\n=== Training Level {level} (Keras Built-in) ===\")\n",
        "\n",
        "    # Get data\n",
        "    examples, max_len = create_simple_data(level)\n",
        "    eng_data, tam_input, tam_target, vocab, reverse_vocab = prepare_data_simple(examples, max_len)\n",
        "\n",
        "    print(f\"Level {level}: {len(examples)} examples, vocab size: {len(vocab)}\")\n",
        "\n",
        "    model = SimpleKerasTransformer(\n",
        "            vocab_size=len(vocab),\n",
        "            d_model=64,\n",
        "            num_heads=4,\n",
        "            num_layers=2,\n",
        "            max_seq_len=max_len\n",
        "        )\n",
        "\n",
        "    # Compile with built-in components\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss=create_masked_loss(),\n",
        "        metrics=[create_masked_accuracy()]\n",
        "    )\n",
        "\n",
        "    # Expand data for training\n",
        "    repetitions = max(10, 50 // len(examples))\n",
        "    eng_expanded = np.tile(eng_data, (repetitions, 1))\n",
        "    tam_input_expanded = np.tile(tam_input, (repetitions, 1))\n",
        "    tam_target_expanded = np.tile(tam_target, (repetitions, 1))\n",
        "\n",
        "    print(f\"Training with {len(eng_expanded)} examples...\")\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        [eng_expanded, tam_input_expanded],\n",
        "        tam_target_expanded,\n",
        "        epochs=50,\n",
        "        batch_size=8,\n",
        "        verbose=1,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Test translations\n",
        "    print(f\"\\n=== Testing Level {level} ===\")\n",
        "    correct = 0\n",
        "\n",
        "    for eng_sentence, expected_tam in examples:\n",
        "        predicted_tam = translate_simple(model, eng_sentence, vocab, reverse_vocab, max_len)\n",
        "\n",
        "        print(f\"'{eng_sentence}' -> '{predicted_tam}' (expected: '{expected_tam}')\")\n",
        "\n",
        "        # Simple accuracy check\n",
        "        if any(word in predicted_tam for word in expected_tam.split()):\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = (correct / len(examples)) * 100\n",
        "    print(f\"Level {level} Accuracy: {accuracy:.1f}%\")\n",
        "\n",
        "    return accuracy >= 50, model, vocab, reverse_vocab, max_len\n",
        "\n",
        "# Super simple one-liner approach\n",
        "def create_minimal_transformer(vocab_size):\n",
        "    \"\"\"Most minimal transformer possible with Keras\"\"\"\n",
        "\n",
        "    # Encoder\n",
        "    enc_input = layers.Input(shape=(None,))\n",
        "    enc_emb = layers.Embedding(vocab_size, 64, mask_zero=True)(enc_input)\n",
        "    enc_out = layers.MultiHeadAttention(num_heads=4, key_dim=16)(enc_emb, enc_emb)\n",
        "    enc_out = layers.LayerNormalization()(enc_out + enc_emb)\n",
        "\n",
        "    # Decoder\n",
        "    dec_input = layers.Input(shape=(None,))\n",
        "    dec_emb = layers.Embedding(vocab_size, 64, mask_zero=True)(dec_input)\n",
        "    dec_self = layers.MultiHeadAttention(num_heads=4, key_dim=16)(dec_emb, dec_emb, use_causal_mask=True)\n",
        "    dec_out = layers.LayerNormalization()(dec_self + dec_emb)\n",
        "    dec_cross = layers.MultiHeadAttention(num_heads=4, key_dim=16)(dec_out, enc_out)\n",
        "    dec_out = layers.LayerNormalization()(dec_cross + dec_out)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Dense(vocab_size)(dec_out)\n",
        "\n",
        "    return Model([enc_input, dec_input], outputs)\n",
        "\n",
        "# Run the simplified version\n",
        "def run_simple_training():\n",
        "    \"\"\"Run the simplified Keras built-in version\"\"\"\n",
        "\n",
        "    print(\"=== Simplified Transformer with Keras Built-ins ===\\n\")\n",
        "\n",
        "    for level in range(1, 4):\n",
        "        success, model, vocab, reverse_vocab, max_len = train_simple_level(level)\n",
        "\n",
        "        if success:\n",
        "            print(f\"✅ Level {level} passed!\")\n",
        "        else:\n",
        "            print(f\"❌ Level {level} needs work\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n🎯 Simplified training complete!\")\n",
        "\n",
        "    # Show minimal version\n",
        "    print(\"\\n=== Minimal Transformer (One-liner style) ===\")\n",
        "    minimal_model = create_minimal_transformer(vocab_size=100)\n",
        "    print(f\"Minimal model created with {minimal_model.count_params():,} parameters\")\n",
        "    minimal_model.summary()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_simple_training()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wbDn9hVTL8y0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}