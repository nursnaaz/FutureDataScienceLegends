{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO and COCO object recognition basics in Python\n",
    "\n",
    "By [Mikolaj Buchwald (LI profile)](https://www.linkedin.com/in/mikolaj-buchwald/) -- [mikbuch's GitHub](github.com/mikbuch)\n",
    "\n",
    "This tutorial is an adaptation of [the example](https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html) explained in the official OpenCV YOLO documentation. The reason for creating this Notebook is that I myself had difficulties recreating this example based on the aforementioned documentation. That is why I created this step-by-step Notebook when you can re-run all cells and get to know YOLO and coco better.\n",
    "\n",
    "## YOLO -- an object detection network\n",
    "\n",
    "> _YOLO — You Only Look Once — is an extremely fast multi object detection algorithm which uses convolutional neural network (CNN) to detect and identify objects._\n",
    ">\n",
    "> _The neural network has this network architecture._\n",
    ">\n",
    "> ![YOLO network archtecture](yolo1_net.png)\n",
    ">\n",
    "> Source: https://arxiv.org/pdf/1506.02640.pdf\n",
    "\n",
    "I quote this explanation from: [OpenCV YOLO tutorial](https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html)\n",
    "\n",
    "\n",
    "## COCO -- class labels for 80 objects\n",
    "\n",
    "The class names are stored in the `coco.names` text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      79\r\n"
     ]
    }
   ],
   "source": [
    "!cat yolov3-coco/coco-labels | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\r\n",
      "bicycle\r\n",
      "car\r\n",
      "motorbike\r\n",
      "aeroplane\r\n",
      "bus\r\n",
      "train\r\n",
      "truck\r\n",
      "boat\r\n",
      "traffic light\r\n",
      "fire hydrant\r\n",
      "stop sign\r\n",
      "parking meter\r\n",
      "bench\r\n",
      "bird\r\n",
      "cat\r\n",
      "dog\r\n",
      "horse\r\n",
      "sheep\r\n",
      "cow\r\n",
      "elephant\r\n",
      "bear\r\n",
      "zebra\r\n",
      "giraffe\r\n",
      "backpack\r\n",
      "umbrella\r\n",
      "handbag\r\n",
      "tie\r\n",
      "suitcase\r\n",
      "frisbee\r\n",
      "skis\r\n",
      "snowboard\r\n",
      "sports ball\r\n",
      "kite\r\n",
      "baseball bat\r\n",
      "baseball glove\r\n",
      "skateboard\r\n",
      "surfboard\r\n",
      "tennis racket\r\n",
      "bottle\r\n",
      "wine glass\r\n",
      "cup\r\n",
      "fork\r\n",
      "knife\r\n",
      "spoon\r\n",
      "bowl\r\n",
      "banana\r\n",
      "apple\r\n",
      "sandwich\r\n",
      "orange\r\n",
      "broccoli\r\n",
      "carrot\r\n",
      "hot dog\r\n",
      "pizza\r\n",
      "donut\r\n",
      "cake\r\n",
      "chair\r\n",
      "sofa\r\n",
      "pottedplant\r\n",
      "bed\r\n",
      "diningtable\r\n",
      "toilet\r\n",
      "tvmonitor\r\n",
      "laptop\r\n",
      "mouse\r\n",
      "remote\r\n",
      "keyboard\r\n",
      "cell phone\r\n",
      "microwave\r\n",
      "oven\r\n",
      "toaster\r\n",
      "sink\r\n",
      "refrigerator\r\n",
      "book\r\n",
      "clock\r\n",
      "vase\r\n",
      "scissors\r\n",
      "teddy bear\r\n",
      "hair drier\r\n",
      "toothbrush"
     ]
    }
   ],
   "source": [
    "!cat yolov3-coco/coco-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\r\n",
      "bicycle\r\n",
      "car\r\n",
      "motorbike\r\n",
      "aeroplane\r\n",
      "bus\r\n",
      "train\r\n",
      "truck\r\n",
      "boat\r\n",
      "traffic light\r\n"
     ]
    }
   ],
   "source": [
    "!cat yolov3-coco/coco-labels | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting/important notes\n",
    "\n",
    "### High resolution images\n",
    "\n",
    "Note, in order to use the code from this notebook with larger resolution images, you will have to adjust border width and font size in the cells below, else the borders will be very thin, almost unvisible on the final rendered picture, and the font size will be extremely small.\n",
    "\n",
    "### `net.getUnconnectedOutLayers()` vector or matrix\n",
    "\n",
    "It seems that, depending on the (OpenCV?) implementation, the `net.getUnconnectedOutLayers()` can be either a vector or a matrix. This problem is dealt with in this Notebook by `try`-`except` clause, but pay attention to that in your own use cases.\n",
    "\n",
    "\n",
    "## Import prerequisites\n",
    "\n",
    "As a matter of fact, we will need OpenCV for Python in this example (i.e., `cv2`). It is usually (by convention) imported like that: `import cv2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image\n",
    "\n",
    "OpenCV tutorial example image: https://opencv-tutorial.readthedocs.io/en/latest/_images/horse.jpg.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = 'horse.jpg'\n",
    "\n",
    "# Reading image to an object (we will display it in the next step).\n",
    "img = cv2.imread(img_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 500, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color conversion while displaying the image\n",
    "\n",
    "Matplotlib interprets images in RGB format, but OpenCV uses BGR format\n",
    "\n",
    "So to convert the image so that it's properly loaded, convert it before loading (for details, see: https://www.codegrepper.com/code-examples/python/imshow+wrong+colors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe9b80cb220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Load the YOLO network_\n",
    "> \n",
    "> _In order to run the network you will have to download the pre-trained YOLO weight file (237 MB). https://pjreddie.com/media/files/yolov3.weights_\n",
    "> \n",
    "> _Also download the the YOLO configuration file._\n",
    "> \n",
    "> _[yolov3.cfg](https://opencv-tutorial.readthedocs.io/en/latest/_downloads/10e685aad953495a95c17bfecd1649e5/yolov3.cfg)_\n",
    "\n",
    "Source: https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html#load-the-yolo-network\n",
    "\n",
    "#### Downloading the `yolov3.weights` file\n",
    "\n",
    "The `yolov3.weights` file is pretty large, hence I couldn't include it in the reporitory with this example. We will have to make sure that this file is available for you -- by downloading the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-03 00:07:56--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: 'yolov3.weights'\n",
      "\n",
      "yolov3.weights      100%[===================>] 236.52M  9.09MB/s    in 30s     \n",
      "\n",
      "2023-12-03 00:08:27 (8.02 MB/s) - 'yolov3.weights' saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget -O yolov3.weights https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.dnn.Net 0x7fcdf1cbcd10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load names of classes and get random colors for them.\n",
    "classes = open('yolov3-coco/coco-labels').read().strip().split('\\n')\n",
    "np.random.seed(42)\n",
    "colors = np.random.randint(0, 255, size=(len(classes), 3), dtype='uint8')\n",
    "\n",
    "# Give the configuration and weight files for the model and load the network.\n",
    "net = cv2.dnn.readNetFromDarknet('yolov3-coco/yolov3.cfg', 'yolov3-coco/yolov3.weights')\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "#net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Show the network object\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, get layer names\n",
    "ln = net.getLayerNames()\n",
    "len(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'shortcut_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'shortcut_15', 'conv_16', 'bn_16', 'leaky_17', 'conv_17', 'bn_17', 'leaky_18', 'shortcut_18', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'shortcut_21', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'shortcut_24', 'conv_25', 'bn_25', 'leaky_26', 'conv_26', 'bn_26', 'leaky_27', 'shortcut_27', 'conv_28', 'bn_28', 'leaky_29', 'conv_29', 'bn_29', 'leaky_30', 'shortcut_30', 'conv_31', 'bn_31', 'leaky_32', 'conv_32', 'bn_32', 'leaky_33', 'shortcut_33', 'conv_34', 'bn_34', 'leaky_35', 'conv_35', 'bn_35', 'leaky_36', 'shortcut_36', 'conv_37', 'bn_37', 'leaky_38', 'conv_38', 'bn_38', 'leaky_39', 'conv_39', 'bn_39', 'leaky_40', 'shortcut_40', 'conv_41', 'bn_41', 'leaky_42', 'conv_42', 'bn_42', 'leaky_43', 'shortcut_43', 'conv_44', 'bn_44', 'leaky_45', 'conv_45', 'bn_45', 'leaky_46', 'shortcut_46', 'conv_47', 'bn_47', 'leaky_48', 'conv_48', 'bn_48', 'leaky_49', 'shortcut_49', 'conv_50', 'bn_50', 'leaky_51', 'conv_51', 'bn_51', 'leaky_52', 'shortcut_52', 'conv_53', 'bn_53', 'leaky_54', 'conv_54', 'bn_54', 'leaky_55', 'shortcut_55', 'conv_56', 'bn_56', 'leaky_57', 'conv_57', 'bn_57', 'leaky_58', 'shortcut_58', 'conv_59', 'bn_59', 'leaky_60', 'conv_60', 'bn_60', 'leaky_61', 'shortcut_61', 'conv_62', 'bn_62', 'leaky_63', 'conv_63', 'bn_63', 'leaky_64', 'conv_64', 'bn_64', 'leaky_65', 'shortcut_65', 'conv_66', 'bn_66', 'leaky_67', 'conv_67', 'bn_67', 'leaky_68', 'shortcut_68', 'conv_69', 'bn_69', 'leaky_70', 'conv_70', 'bn_70', 'leaky_71', 'shortcut_71', 'conv_72', 'bn_72', 'leaky_73', 'conv_73', 'bn_73', 'leaky_74', 'shortcut_74', 'conv_75', 'bn_75', 'leaky_76', 'conv_76', 'bn_76', 'leaky_77', 'conv_77', 'bn_77', 'leaky_78', 'conv_78', 'bn_78', 'leaky_79', 'conv_79', 'bn_79', 'leaky_80', 'conv_80', 'bn_80', 'leaky_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'leaky_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'leaky_88', 'conv_88', 'bn_88', 'leaky_89', 'conv_89', 'bn_89', 'leaky_90', 'conv_90', 'bn_90', 'leaky_91', 'conv_91', 'bn_91', 'leaky_92', 'conv_92', 'bn_92', 'leaky_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'leaky_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'leaky_100', 'conv_100', 'bn_100', 'leaky_101', 'conv_101', 'bn_101', 'leaky_102', 'conv_102', 'bn_102', 'leaky_103', 'conv_103', 'bn_103', 'leaky_104', 'conv_104', 'bn_104', 'leaky_105', 'conv_105', 'permute_106', 'yolo_106')\n"
     ]
    }
   ],
   "source": [
    "# Show names of all layers\n",
    "print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([200, 227, 254], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get output layers\n",
    "net.getUnconnectedOutLayers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that, depending on the (OpenCV?) implementation, the `net.getUnconnectedOutLayers()` can be either a vector or a matrix. This problem is dealt with in this Notebook by `try`-`except` clause, but pay attention to that in your own use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "# Show the names of this layers\n",
    "try:\n",
    "    print([ln[i[0] - 1] for i in net.getUnconnectedOutLayers()])\n",
    "except IndexError:\n",
    "    print([ln[i - 1] for i in net.getUnconnectedOutLayers()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolo_82', 'yolo_94', 'yolo_106']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this layers\n",
    "try:\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blob is an input to the YOLO network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob shape=(1, 3, 416, 416)\n"
     ]
    }
   ],
   "source": [
    "# Construct a blob from the image\n",
    "blob = cv2.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "r = blob[0, 0, :, :]\n",
    "\n",
    "plt.imshow(r)\n",
    "print('Blob shape=%s' % (blob.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.697 seconds to process the image.\n"
     ]
    }
   ],
   "source": [
    "net.setInput(blob)\n",
    "t0 = time.time()\n",
    "outputs = net.forward(ln)\n",
    "t = time.time()\n",
    "print('It took %.3f seconds to process the image.' % (t-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs object are vectors of lenght 85 (4 + 1 + 80):\n",
    "\n",
    "    4x the bounding box (centerx, centery, width, height)\n",
    "    1x box confidence\n",
    "    80x class confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 objects.\n",
      "\n",
      "Enlisting objects:\n",
      "[(507, 85), (2028, 85), (8112, 85)]\n"
     ]
    }
   ],
   "source": [
    "print('Found %s objects.' % len(outputs))\n",
    "\n",
    "print('\\nEnlisting objects:')\n",
    "print([out.shape for out in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image preview:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcdc3800fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = []\n",
    "confidences = []\n",
    "classIDs = []\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "for output in outputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        classID = np.argmax(scores)\n",
    "        confidence = scores[classID]\n",
    "        if confidence > 0.5:\n",
    "            box = detection[:4] * np.array([w, h, w, h])\n",
    "            (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "            x = int(centerX - (width / 2))\n",
    "            y = int(centerY - (height / 2))\n",
    "            box = [x, y, int(width), int(height)]\n",
    "            boxes.append(box)\n",
    "            confidences.append(float(confidence))\n",
    "            classIDs.append(classID)\n",
    "\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():\n",
    "        (x, y) = (boxes[i][0], boxes[i][1])\n",
    "        (w, h) = (boxes[i][2], boxes[i][3])\n",
    "        color = [int(c) for c in colors[classIDs[i]]]\n",
    "        \n",
    "        #  -- Arguments for CV2 rectangle:\n",
    "        # cv2.rect   (img,  x, y,   width, height, color, line width)\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 4)\n",
    "        \n",
    "        # Labels and confidences for the image\n",
    "        text = \"{}: {:.4f}\".format(classes[classIDs[i]], confidences[i])\n",
    "        cv2.putText(img, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "print('Image preview:')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write image to the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('output_horse.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show image with bounding boxes\n",
    "\n",
    "### Color conversion\n",
    "\n",
    "Matplotlib interprets images in RGB format, but OpenCV uses BGR format. So to convert the image so that it's properly loaded, convert it before loading (for details, see: https://www.codegrepper.com/code-examples/python/imshow+wrong+colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef70078d90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 12), dpi=90)\n",
    "ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
