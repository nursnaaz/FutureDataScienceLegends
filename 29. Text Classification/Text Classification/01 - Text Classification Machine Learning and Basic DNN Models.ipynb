{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLOatR1LRaE_"
   },
   "source": [
    "# Sentiment Analysis - Machine Learning and Basic Deep Neural Network Models\n",
    "\n",
    "We have already discussed that sentiment analysis, also popularly known as opinion analysis or opinion mining is one of the most important applications of NLP. The key idea is to predict the potential sentiment of a body of text based on the textual content. In this sub-unit, we will be exploring supervised learning models. \n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_cover.png?raw=1)\n",
    "\n",
    "Another way to build a model to understand the text content and predict the sentiment of the text based reviews is to use supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
    "\n",
    "1.\tPrepare train and test datasets (optionally a validation dataset)\n",
    "2.\tPre-process and normalize text documents\n",
    "3.\tFeature Engineering \n",
    "4.\tModel training\n",
    "5.\tModel prediction and evaluation\n",
    "\n",
    "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_classifier_workflow.png?raw=1)\n",
    "\n",
    "\n",
    "In our scenario, documents indicate the movie reviews and classes indicate the review sentiments which can either be positive or negative making it a binary classification problem. We will build models using both traditional machine learning methods and newer deep learning in the subsequent sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "610bNp_4SQma",
    "outputId": "36f2fc5a-681b-4e2d-ef3d-b2473bc10298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /anaconda3/lib/python3.6/site-packages (0.0.21)\n",
      "\u001b[31mmodin 0.2.2 has requirement pandas==0.23.4, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mjupyter-console 5.2.0 has requirement prompt_toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mdash 0.42.0 has requirement Flask>=0.12, but you'll have flask 0.10.1 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textsearch in /anaconda3/lib/python3.6/site-packages (0.0.17)\n",
      "Requirement already satisfied: pyahocorasick in /anaconda3/lib/python3.6/site-packages (from textsearch) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in /anaconda3/lib/python3.6/site-packages (from textsearch) (1.0.22)\n",
      "\u001b[31mmodin 0.2.2 has requirement pandas==0.23.4, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mjupyter-console 5.2.0 has requirement prompt_toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mdash 0.42.0 has requirement Flask>=0.12, but you'll have flask 0.10.1 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.6/site-packages (4.31.1)\n",
      "\u001b[31mmodin 0.2.2 has requirement pandas==0.23.4, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mjupyter-console 5.2.0 has requirement prompt_toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mdash 0.42.0 has requirement Flask>=0.12, but you'll have flask 0.10.1 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to /Users/mnoordeen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMFvLtEHRaFM"
   },
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "qgphrYufRaFR",
    "outputId": "101f3c9a-7cb9-4934-edd6-47057e65f1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "5nkEEGExRaFc",
    "outputId": "54bba768-f477-43f3-b385-debbdb226209"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-V1NWGhcRaFi"
   },
   "source": [
    "# Build Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JP10IEYRaFj"
   },
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "reviews = dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values\n",
    "\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i45AFxXNRaFn"
   },
   "source": [
    "# Text Wrangling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHZ0lEGNRaFo"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "CO3ESug2RaFr",
    "outputId": "6b6d1bc1-9dfa-4d67-d139-52b002c64f6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35000/35000 [00:18<00:00, 1934.86it/s]\n",
      "100%|██████████| 15000/15000 [00:07<00:00, 1994.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 313 ms, total: 25.6 s\n",
      "Wall time: 25.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucDv8n50RaFu"
   },
   "source": [
    "# Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOZ7Rn0jRaFv"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JD8q5QoERaFw",
    "outputId": "e4a69fee-f05c-4ac1-b8b2-d07720b41042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.5 s, sys: 1.52 s, total: 52 s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sSvqpHRYRaFz",
    "outputId": "94a9b9c5-a51d-49cb-eff2-9d29e2801e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 167 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mfQPYw8PRaF2",
    "outputId": "52f70387-6880-4008-a71f-936a8bacf789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 194922)  Test features shape: (15000, 194922)\n",
      "TFIDF model:> Train features shape: (35000, 194922)  Test features shape: (15000, 194922)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aUUsxMrRaF7"
   },
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPA5UtYFRaF8"
   },
   "source": [
    "### Try out Logistic Regression\n",
    "\n",
    "The logistic regression model is actually a statistical model developed by statistician\n",
    "David Cox in 1958. It is also known as the logit or logistic model since it uses the\n",
    "logistic (popularly also known as sigmoid) mathematical function to estimate the\n",
    "parameter values. These are the coefficients of all our features such that the overall loss\n",
    "is minimized when predicting the outcome—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JGHQErnMRaF9",
    "outputId": "7a94f0a8-26f0-4dac-81ce-ef84a8f74af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 37s, sys: 1min 40s, total: 6min 18s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
    "\n",
    "# train model\n",
    "lr.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "wqeRyayrRaGA",
    "outputId": "c1a86261-88d3-4b8b-b354-97cc5a54f75c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90      7490\n",
      "    positive       0.90      0.91      0.90      7510\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6754</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>711</td>\n",
       "      <td>6799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6754       736\n",
       "positive       711      6799"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, lr_bow_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uQEAz6O6RaGC",
    "outputId": "4a280237-1323-4458-f021-696e77457cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 4.67 s, total: 16.2 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on TF-IDF features\n",
    "\n",
    "# train model\n",
    "lr.fit(tv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "lr_tfidf_predictions = lr.predict(tv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "HNCfZnUORaGE",
    "outputId": "0a4f17bf-1d18-48c8-c998-eeba611ebcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      7490\n",
      "    positive       0.90      0.91      0.90      7510\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6694</td>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>665</td>\n",
       "      <td>6845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6694       796\n",
       "positive       665      6845"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, lr_tfidf_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoOoEAiXRaGH"
   },
   "source": [
    "### Try out Random Forest\n",
    "\n",
    "Decision trees are a family of supervised machine learning algorithms that can represent\n",
    "and interpret sets of rules automatically from the underlying data. They use metrics like\n",
    "information gain and gini-index to build the tree. However, a major drawback of decision\n",
    "trees is that since they are non-parametric, the more data there is, greater the depth of\n",
    "the tree. We can end up with really huge and deep trees that are prone to overfitting. The\n",
    "model might work really well on training data, but instead of learning, it just memorizes\n",
    "all the training samples and builds very specific rules to them. Hence, it performs really\n",
    "poorly on the test data. Random forests try to tackle this problem.\n",
    "\n",
    "A random forest is a meta-estimator or an ensemble model that fits a number of\n",
    "decision tree classifiers on various sub-samples of the dataset and uses averaging to\n",
    "improve the predictive accuracy and control over-fitting. The sub-sample size is always\n",
    "the same as the original input sample size, but the samples are drawn with replacement\n",
    "(bootstrap samples). In random forests, all the trees are trained in parallel (bagging\n",
    "model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a\n",
    "sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also,\n",
    "when splitting a node during the construction of the tree, the split that is chosen is no\n",
    "longer the best split among all features. Instead, the split that is picked is the best split\n",
    "among a random subset of the features. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YzaSSOpYRaGH",
    "outputId": "98e50c3d-c50f-4757-aceb-5c19f67b7bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1.18 s, total: 3min 34s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest model on BOW features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate model\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "# train model\n",
    "rf.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "rf_bow_predictions = rf.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "617Kuv7_RaGJ",
    "outputId": "f21b91c0-1a7d-467c-c865-5398189a1f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86      7490\n",
      "    positive       0.86      0.86      0.86      7510\n",
      "\n",
      "    accuracy                           0.86     15000\n",
      "   macro avg       0.86      0.86      0.86     15000\n",
      "weighted avg       0.86      0.86      0.86     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6406</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1080</td>\n",
       "      <td>6430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6406      1084\n",
       "positive      1080      6430"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, rf_bow_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "WdvmBOrPRaGM",
    "outputId": "7b9bac4d-d4c4-40cb-d1c5-6ed649443d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 21s, sys: 1.02 s, total: 3min 22s\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest model on TF-IDF features\n",
    "\n",
    "# train model\n",
    "rf.fit(tv_train_features, train_sentiments)\n",
    "\n",
    "# predict on test data\n",
    "rf_tfidf_predictions = rf.predict(tv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "8hBOMh6uRaGP",
    "outputId": "f36e34e4-2133-44f6-90e2-306db219a69e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.86      0.85      7490\n",
      "    positive       0.86      0.84      0.85      7510\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.85      0.85      0.85     15000\n",
      "weighted avg       0.85      0.85      0.85     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6458</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1175</td>\n",
       "      <td>6335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6458      1032\n",
       "positive      1175      6335"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoqZhMQFRaGS"
   },
   "source": [
    "# Newer Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZw6LYNHRaGT"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiZbcv_gRaGZ"
   },
   "source": [
    "## Prediction class label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnhC4rWaRaGb"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                       for text in norm_train_reviews]\n",
    "y_train = le.fit_transform(train_sentiments)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                       for text in norm_test_reviews]\n",
    "y_test = le.fit_transform(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "4ogDRDh4RaGg",
    "outputId": "877e762d-4e4a-4aae-9477-64d540541396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['negative' 'positive' 'negative'] \n",
      "Encoded Labels: [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdexbrYXRaGk"
   },
   "source": [
    "## Feature Engineering with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5S0u0BbiN2a"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "T9kfCw6LRaGl",
    "outputId": "149b0e54-57bf-4823-cf30-549dca50859e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:37:45,415 : INFO : collecting all words and their counts\n",
      "2019-08-07 16:37:45,416 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-08-07 16:37:45,881 : INFO : PROGRESS: at sentence #10000, processed 2294933 words, keeping 82417 word types\n",
      "2019-08-07 16:37:46,381 : INFO : PROGRESS: at sentence #20000, processed 4591079 words, keeping 124832 word types\n",
      "2019-08-07 16:37:46,867 : INFO : PROGRESS: at sentence #30000, processed 6884452 words, keeping 159825 word types\n",
      "2019-08-07 16:37:47,108 : INFO : collected 176258 word types from a corpus of 8035381 raw words and 35000 sentences\n",
      "2019-08-07 16:37:47,109 : INFO : Loading a fresh vocabulary\n",
      "2019-08-07 16:37:47,225 : INFO : min_count=10 retains 24661 unique words (13% of original 176258, drops 151597)\n",
      "2019-08-07 16:37:47,226 : INFO : min_count=10 leaves 7763119 word corpus (96% of original 8035381, drops 272262)\n",
      "2019-08-07 16:37:47,339 : INFO : deleting the raw counts dictionary of 176258 items\n",
      "2019-08-07 16:37:47,359 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2019-08-07 16:37:47,360 : INFO : downsampling leaves estimated 5721143 word corpus (73.7% of prior 7763119)\n",
      "2019-08-07 16:37:47,452 : INFO : estimated required memory for 24661 words and 300 dimensions: 71516900 bytes\n",
      "2019-08-07 16:37:47,454 : INFO : resetting layer weights\n",
      "2019-08-07 16:37:47,872 : INFO : training model with 4 workers on 24661 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
      "2019-08-07 16:37:48,901 : INFO : EPOCH 1 - PROGRESS: at 2.57% examples, 143931 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:49,966 : INFO : EPOCH 1 - PROGRESS: at 5.62% examples, 150965 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:51,001 : INFO : EPOCH 1 - PROGRESS: at 8.48% examples, 154504 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:52,022 : INFO : EPOCH 1 - PROGRESS: at 11.57% examples, 158617 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:53,058 : INFO : EPOCH 1 - PROGRESS: at 14.65% examples, 161935 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:54,156 : INFO : EPOCH 1 - PROGRESS: at 17.65% examples, 162544 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:55,258 : INFO : EPOCH 1 - PROGRESS: at 21.18% examples, 164843 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:56,268 : INFO : EPOCH 1 - PROGRESS: at 24.49% examples, 167562 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:57,331 : INFO : EPOCH 1 - PROGRESS: at 27.54% examples, 167112 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:58,356 : INFO : EPOCH 1 - PROGRESS: at 30.76% examples, 168702 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:37:59,443 : INFO : EPOCH 1 - PROGRESS: at 33.78% examples, 167832 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:00,486 : INFO : EPOCH 1 - PROGRESS: at 36.79% examples, 167264 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:01,487 : INFO : EPOCH 1 - PROGRESS: at 39.42% examples, 166282 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:02,543 : INFO : EPOCH 1 - PROGRESS: at 42.40% examples, 165744 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:03,575 : INFO : EPOCH 1 - PROGRESS: at 45.37% examples, 165537 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:04,596 : INFO : EPOCH 1 - PROGRESS: at 48.32% examples, 165537 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:05,641 : INFO : EPOCH 1 - PROGRESS: at 51.32% examples, 165288 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:06,709 : INFO : EPOCH 1 - PROGRESS: at 54.30% examples, 164873 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:07,786 : INFO : EPOCH 1 - PROGRESS: at 57.20% examples, 164371 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:08,800 : INFO : EPOCH 1 - PROGRESS: at 59.94% examples, 164099 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:09,821 : INFO : EPOCH 1 - PROGRESS: at 62.77% examples, 163785 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:10,832 : INFO : EPOCH 1 - PROGRESS: at 65.40% examples, 163257 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:11,867 : INFO : EPOCH 1 - PROGRESS: at 68.25% examples, 162961 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:12,881 : INFO : EPOCH 1 - PROGRESS: at 70.82% examples, 162238 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:38:13,889 : INFO : EPOCH 1 - PROGRESS: at 73.47% examples, 161880 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:14,931 : INFO : EPOCH 1 - PROGRESS: at 76.24% examples, 161281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:15,950 : INFO : EPOCH 1 - PROGRESS: at 78.73% examples, 160423 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:16,980 : INFO : EPOCH 1 - PROGRESS: at 81.44% examples, 160042 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:38:18,014 : INFO : EPOCH 1 - PROGRESS: at 84.09% examples, 159420 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:19,035 : INFO : EPOCH 1 - PROGRESS: at 86.69% examples, 159172 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:20,096 : INFO : EPOCH 1 - PROGRESS: at 89.29% examples, 158505 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:21,178 : INFO : EPOCH 1 - PROGRESS: at 92.09% examples, 158400 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:22,267 : INFO : EPOCH 1 - PROGRESS: at 95.12% examples, 158261 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:23,373 : INFO : EPOCH 1 - PROGRESS: at 98.13% examples, 158047 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:23,978 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-08-07 16:38:23,982 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-07 16:38:24,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-07 16:38:24,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-07 16:38:24,065 : INFO : EPOCH - 1 : training on 8035381 raw words (5720599 effective words) took 36.2s, 158078 effective words/s\n",
      "2019-08-07 16:38:25,167 : INFO : EPOCH 2 - PROGRESS: at 2.57% examples, 135573 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:26,233 : INFO : EPOCH 2 - PROGRESS: at 5.62% examples, 146377 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:27,252 : INFO : EPOCH 2 - PROGRESS: at 8.36% examples, 149889 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:28,277 : INFO : EPOCH 2 - PROGRESS: at 10.81% examples, 146687 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:29,322 : INFO : EPOCH 2 - PROGRESS: at 13.47% examples, 146703 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:30,352 : INFO : EPOCH 2 - PROGRESS: at 16.01% examples, 146968 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:31,381 : INFO : EPOCH 2 - PROGRESS: at 18.57% examples, 146472 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:38:32,418 : INFO : EPOCH 2 - PROGRESS: at 21.04% examples, 145076 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:33,512 : INFO : EPOCH 2 - PROGRESS: at 23.76% examples, 144562 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:34,536 : INFO : EPOCH 2 - PROGRESS: at 26.13% examples, 143686 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:35,538 : INFO : EPOCH 2 - PROGRESS: at 28.70% examples, 143359 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:36,574 : INFO : EPOCH 2 - PROGRESS: at 31.12% examples, 143122 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:37,657 : INFO : EPOCH 2 - PROGRESS: at 33.89% examples, 143492 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:38,681 : INFO : EPOCH 2 - PROGRESS: at 36.65% examples, 143949 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:39,692 : INFO : EPOCH 2 - PROGRESS: at 39.18% examples, 144054 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:40,734 : INFO : EPOCH 2 - PROGRESS: at 41.81% examples, 143856 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:41,823 : INFO : EPOCH 2 - PROGRESS: at 43.91% examples, 141754 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:42,877 : INFO : EPOCH 2 - PROGRESS: at 46.69% examples, 142378 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:43,951 : INFO : EPOCH 2 - PROGRESS: at 49.29% examples, 142093 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:45,078 : INFO : EPOCH 2 - PROGRESS: at 52.32% examples, 142504 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:46,084 : INFO : EPOCH 2 - PROGRESS: at 55.06% examples, 142985 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:38:47,228 : INFO : EPOCH 2 - PROGRESS: at 57.67% examples, 142596 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:48,255 : INFO : EPOCH 2 - PROGRESS: at 60.05% examples, 142321 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-07 16:38:49,303 : INFO : EPOCH 2 - PROGRESS: at 62.14% examples, 141118 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:50,397 : INFO : EPOCH 2 - PROGRESS: at 63.96% examples, 139237 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:51,482 : INFO : EPOCH 2 - PROGRESS: at 65.51% examples, 137035 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:52,501 : INFO : EPOCH 2 - PROGRESS: at 67.00% examples, 135104 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:38:53,545 : INFO : EPOCH 2 - PROGRESS: at 68.48% examples, 133163 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:54,638 : INFO : EPOCH 2 - PROGRESS: at 69.87% examples, 130923 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:55,672 : INFO : EPOCH 2 - PROGRESS: at 71.70% examples, 129962 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:56,736 : INFO : EPOCH 2 - PROGRESS: at 73.27% examples, 128524 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:57,891 : INFO : EPOCH 2 - PROGRESS: at 75.24% examples, 127414 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:58,954 : INFO : EPOCH 2 - PROGRESS: at 77.14% examples, 126523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:38:59,988 : INFO : EPOCH 2 - PROGRESS: at 78.60% examples, 125230 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:01,114 : INFO : EPOCH 2 - PROGRESS: at 80.14% examples, 123888 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:02,218 : INFO : EPOCH 2 - PROGRESS: at 82.16% examples, 123228 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:03,260 : INFO : EPOCH 2 - PROGRESS: at 83.96% examples, 122451 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:04,294 : INFO : EPOCH 2 - PROGRESS: at 85.89% examples, 122120 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:05,316 : INFO : EPOCH 2 - PROGRESS: at 88.88% examples, 123352 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:06,323 : INFO : EPOCH 2 - PROGRESS: at 91.71% examples, 124385 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:07,339 : INFO : EPOCH 2 - PROGRESS: at 94.98% examples, 125680 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:08,437 : INFO : EPOCH 2 - PROGRESS: at 98.10% examples, 126504 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:08,924 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-08-07 16:39:08,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-07 16:39:08,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-07 16:39:09,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-07 16:39:09,015 : INFO : EPOCH - 2 : training on 8035381 raw words (5722158 effective words) took 44.9s, 127333 effective words/s\n",
      "2019-08-07 16:39:10,030 : INFO : EPOCH 3 - PROGRESS: at 2.68% examples, 152829 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:11,058 : INFO : EPOCH 3 - PROGRESS: at 5.62% examples, 154735 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:12,061 : INFO : EPOCH 3 - PROGRESS: at 8.24% examples, 154012 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:13,070 : INFO : EPOCH 3 - PROGRESS: at 10.69% examples, 150232 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:14,121 : INFO : EPOCH 3 - PROGRESS: at 13.14% examples, 146597 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:15,178 : INFO : EPOCH 3 - PROGRESS: at 15.77% examples, 147426 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:16,183 : INFO : EPOCH 3 - PROGRESS: at 18.57% examples, 149284 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:17,200 : INFO : EPOCH 3 - PROGRESS: at 21.30% examples, 149596 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:18,229 : INFO : EPOCH 3 - PROGRESS: at 23.17% examples, 144281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:19,268 : INFO : EPOCH 3 - PROGRESS: at 24.62% examples, 137899 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:20,430 : INFO : EPOCH 3 - PROGRESS: at 26.48% examples, 133526 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:21,551 : INFO : EPOCH 3 - PROGRESS: at 28.60% examples, 130535 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:22,596 : INFO : EPOCH 3 - PROGRESS: at 30.30% examples, 128202 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:23,668 : INFO : EPOCH 3 - PROGRESS: at 31.84% examples, 124930 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:39:24,684 : INFO : EPOCH 3 - PROGRESS: at 33.40% examples, 122653 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:25,718 : INFO : EPOCH 3 - PROGRESS: at 35.32% examples, 121321 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:26,741 : INFO : EPOCH 3 - PROGRESS: at 37.15% examples, 120247 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:27,793 : INFO : EPOCH 3 - PROGRESS: at 40.30% examples, 123181 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:28,836 : INFO : EPOCH 3 - PROGRESS: at 43.40% examples, 125531 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:29,856 : INFO : EPOCH 3 - PROGRESS: at 46.59% examples, 128131 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:30,920 : INFO : EPOCH 3 - PROGRESS: at 49.68% examples, 129899 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:32,025 : INFO : EPOCH 3 - PROGRESS: at 53.13% examples, 132215 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:33,127 : INFO : EPOCH 3 - PROGRESS: at 56.15% examples, 133158 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:34,136 : INFO : EPOCH 3 - PROGRESS: at 57.91% examples, 131970 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:35,200 : INFO : EPOCH 3 - PROGRESS: at 59.41% examples, 130085 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:36,217 : INFO : EPOCH 3 - PROGRESS: at 61.14% examples, 128828 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:37,287 : INFO : EPOCH 3 - PROGRESS: at 63.03% examples, 127659 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:38,289 : INFO : EPOCH 3 - PROGRESS: at 65.16% examples, 127585 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:39,364 : INFO : EPOCH 3 - PROGRESS: at 68.25% examples, 128853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:40,403 : INFO : EPOCH 3 - PROGRESS: at 70.70% examples, 129040 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:41,507 : INFO : EPOCH 3 - PROGRESS: at 72.33% examples, 127464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:39:42,595 : INFO : EPOCH 3 - PROGRESS: at 74.07% examples, 126461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:43,694 : INFO : EPOCH 3 - PROGRESS: at 75.75% examples, 125043 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:44,705 : INFO : EPOCH 3 - PROGRESS: at 77.64% examples, 124463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:45,744 : INFO : EPOCH 3 - PROGRESS: at 79.20% examples, 123419 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:46,880 : INFO : EPOCH 3 - PROGRESS: at 81.19% examples, 122668 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:47,886 : INFO : EPOCH 3 - PROGRESS: at 82.93% examples, 122010 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:39:48,969 : INFO : EPOCH 3 - PROGRESS: at 85.89% examples, 122930 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:49,989 : INFO : EPOCH 3 - PROGRESS: at 88.88% examples, 124158 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:50,992 : INFO : EPOCH 3 - PROGRESS: at 91.88% examples, 125358 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:52,019 : INFO : EPOCH 3 - PROGRESS: at 94.98% examples, 126435 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:53,054 : INFO : EPOCH 3 - PROGRESS: at 98.10% examples, 127419 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:53,766 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-08-07 16:39:53,776 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-07 16:39:53,830 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-07 16:39:53,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-07 16:39:53,909 : INFO : EPOCH - 3 : training on 8035381 raw words (5721651 effective words) took 44.9s, 127455 effective words/s\n",
      "2019-08-07 16:39:55,140 : INFO : EPOCH 4 - PROGRESS: at 1.55% examples, 74889 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:39:56,301 : INFO : EPOCH 4 - PROGRESS: at 3.63% examples, 85670 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:57,480 : INFO : EPOCH 4 - PROGRESS: at 5.62% examples, 88627 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:39:58,655 : INFO : EPOCH 4 - PROGRESS: at 7.58% examples, 90273 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-07 16:39:59,787 : INFO : EPOCH 4 - PROGRESS: at 9.46% examples, 91855 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:00,793 : INFO : EPOCH 4 - PROGRESS: at 10.93% examples, 90608 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:01,796 : INFO : EPOCH 4 - PROGRESS: at 12.80% examples, 92391 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:02,835 : INFO : EPOCH 4 - PROGRESS: at 15.13% examples, 97301 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:03,855 : INFO : EPOCH 4 - PROGRESS: at 17.35% examples, 101325 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:04,898 : INFO : EPOCH 4 - PROGRESS: at 20.54% examples, 107662 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:05,914 : INFO : EPOCH 4 - PROGRESS: at 23.17% examples, 110803 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:06,930 : INFO : EPOCH 4 - PROGRESS: at 24.73% examples, 109173 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:07,934 : INFO : EPOCH 4 - PROGRESS: at 26.25% examples, 107731 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:08,967 : INFO : EPOCH 4 - PROGRESS: at 27.95% examples, 106422 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:10,029 : INFO : EPOCH 4 - PROGRESS: at 29.60% examples, 105428 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:11,042 : INFO : EPOCH 4 - PROGRESS: at 31.12% examples, 104463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:12,122 : INFO : EPOCH 4 - PROGRESS: at 32.92% examples, 103988 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:13,129 : INFO : EPOCH 4 - PROGRESS: at 34.68% examples, 103634 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:14,248 : INFO : EPOCH 4 - PROGRESS: at 36.41% examples, 102720 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:15,604 : INFO : EPOCH 4 - PROGRESS: at 37.88% examples, 100184 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:40:16,636 : INFO : EPOCH 4 - PROGRESS: at 39.18% examples, 99023 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:17,720 : INFO : EPOCH 4 - PROGRESS: at 40.77% examples, 98316 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:18,737 : INFO : EPOCH 4 - PROGRESS: at 42.52% examples, 98240 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:19,810 : INFO : EPOCH 4 - PROGRESS: at 44.24% examples, 97958 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:20,874 : INFO : EPOCH 4 - PROGRESS: at 45.83% examples, 97472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:21,922 : INFO : EPOCH 4 - PROGRESS: at 48.68% examples, 99601 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:22,954 : INFO : EPOCH 4 - PROGRESS: at 51.69% examples, 101862 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:23,973 : INFO : EPOCH 4 - PROGRESS: at 54.71% examples, 104024 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:25,171 : INFO : EPOCH 4 - PROGRESS: at 56.26% examples, 102953 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:26,220 : INFO : EPOCH 4 - PROGRESS: at 57.91% examples, 102627 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:27,245 : INFO : EPOCH 4 - PROGRESS: at 59.41% examples, 102200 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:28,393 : INFO : EPOCH 4 - PROGRESS: at 61.03% examples, 101442 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:29,394 : INFO : EPOCH 4 - PROGRESS: at 62.77% examples, 101335 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:30,398 : INFO : EPOCH 4 - PROGRESS: at 64.08% examples, 100653 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:31,480 : INFO : EPOCH 4 - PROGRESS: at 65.75% examples, 100352 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:40:32,635 : INFO : EPOCH 4 - PROGRESS: at 67.39% examples, 99723 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:33,680 : INFO : EPOCH 4 - PROGRESS: at 69.25% examples, 99740 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:34,692 : INFO : EPOCH 4 - PROGRESS: at 70.70% examples, 99334 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:35,848 : INFO : EPOCH 4 - PROGRESS: at 72.32% examples, 98766 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:36,869 : INFO : EPOCH 4 - PROGRESS: at 73.82% examples, 98539 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:37,882 : INFO : EPOCH 4 - PROGRESS: at 75.63% examples, 98466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:39,077 : INFO : EPOCH 4 - PROGRESS: at 77.27% examples, 97876 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-07 16:40:40,089 : INFO : EPOCH 4 - PROGRESS: at 78.84% examples, 97706 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:41,314 : INFO : EPOCH 4 - PROGRESS: at 80.64% examples, 97388 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:40:42,325 : INFO : EPOCH 4 - PROGRESS: at 82.38% examples, 97374 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:43,395 : INFO : EPOCH 4 - PROGRESS: at 84.06% examples, 97111 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:44,453 : INFO : EPOCH 4 - PROGRESS: at 85.75% examples, 97036 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:45,559 : INFO : EPOCH 4 - PROGRESS: at 87.25% examples, 96721 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:46,560 : INFO : EPOCH 4 - PROGRESS: at 88.88% examples, 96617 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:47,594 : INFO : EPOCH 4 - PROGRESS: at 90.41% examples, 96450 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:48,595 : INFO : EPOCH 4 - PROGRESS: at 93.43% examples, 97869 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:49,600 : INFO : EPOCH 4 - PROGRESS: at 96.52% examples, 99132 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:50,612 : INFO : EPOCH 4 - PROGRESS: at 99.24% examples, 100188 words/s, in_qsize 6, out_qsize 0\n",
      "2019-08-07 16:40:50,860 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-08-07 16:40:50,875 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-07 16:40:50,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-07 16:40:51,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-07 16:40:51,040 : INFO : EPOCH - 4 : training on 8035381 raw words (5720726 effective words) took 57.1s, 100148 effective words/s\n",
      "2019-08-07 16:40:52,332 : INFO : EPOCH 5 - PROGRESS: at 1.55% examples, 71090 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:53,335 : INFO : EPOCH 5 - PROGRESS: at 3.49% examples, 86083 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:54,557 : INFO : EPOCH 5 - PROGRESS: at 5.12% examples, 81834 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:55,561 : INFO : EPOCH 5 - PROGRESS: at 6.84% examples, 85318 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:56,575 : INFO : EPOCH 5 - PROGRESS: at 8.24% examples, 84745 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:57,626 : INFO : EPOCH 5 - PROGRESS: at 9.80% examples, 85020 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:58,642 : INFO : EPOCH 5 - PROGRESS: at 11.19% examples, 83776 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:40:59,664 : INFO : EPOCH 5 - PROGRESS: at 12.80% examples, 84398 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:00,706 : INFO : EPOCH 5 - PROGRESS: at 14.30% examples, 84651 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:01,722 : INFO : EPOCH 5 - PROGRESS: at 15.65% examples, 84412 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:02,897 : INFO : EPOCH 5 - PROGRESS: at 17.11% examples, 83751 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:03,926 : INFO : EPOCH 5 - PROGRESS: at 19.07% examples, 85228 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:05,044 : INFO : EPOCH 5 - PROGRESS: at 20.65% examples, 84940 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:06,111 : INFO : EPOCH 5 - PROGRESS: at 22.52% examples, 85920 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:07,234 : INFO : EPOCH 5 - PROGRESS: at 24.17% examples, 85558 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:08,261 : INFO : EPOCH 5 - PROGRESS: at 25.78% examples, 86098 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:41:09,267 : INFO : EPOCH 5 - PROGRESS: at 27.41% examples, 86338 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:41:10,412 : INFO : EPOCH 5 - PROGRESS: at 29.04% examples, 85902 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:11,484 : INFO : EPOCH 5 - PROGRESS: at 30.42% examples, 85504 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:12,685 : INFO : EPOCH 5 - PROGRESS: at 31.84% examples, 84565 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:13,765 : INFO : EPOCH 5 - PROGRESS: at 33.29% examples, 84244 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:14,782 : INFO : EPOCH 5 - PROGRESS: at 34.80% examples, 84170 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:15,815 : INFO : EPOCH 5 - PROGRESS: at 36.41% examples, 84314 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:16,853 : INFO : EPOCH 5 - PROGRESS: at 37.98% examples, 84452 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:17,863 : INFO : EPOCH 5 - PROGRESS: at 39.42% examples, 84415 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:19,053 : INFO : EPOCH 5 - PROGRESS: at 40.90% examples, 83807 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:20,145 : INFO : EPOCH 5 - PROGRESS: at 42.18% examples, 83079 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-07 16:41:21,152 : INFO : EPOCH 5 - PROGRESS: at 43.40% examples, 82630 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:22,200 : INFO : EPOCH 5 - PROGRESS: at 44.88% examples, 82533 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:23,300 : INFO : EPOCH 5 - PROGRESS: at 46.59% examples, 82768 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:24,469 : INFO : EPOCH 5 - PROGRESS: at 48.19% examples, 82627 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:25,573 : INFO : EPOCH 5 - PROGRESS: at 49.80% examples, 82601 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-07 16:41:26,597 : INFO : EPOCH 5 - PROGRESS: at 51.34% examples, 82597 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:27,668 : INFO : EPOCH 5 - PROGRESS: at 52.81% examples, 82486 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:28,698 : INFO : EPOCH 5 - PROGRESS: at 54.71% examples, 83025 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:29,957 : INFO : EPOCH 5 - PROGRESS: at 56.26% examples, 82675 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:30,974 : INFO : EPOCH 5 - PROGRESS: at 57.91% examples, 83015 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:32,107 : INFO : EPOCH 5 - PROGRESS: at 59.07% examples, 82427 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:33,145 : INFO : EPOCH 5 - PROGRESS: at 60.41% examples, 82227 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:34,285 : INFO : EPOCH 5 - PROGRESS: at 61.55% examples, 81514 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:35,368 : INFO : EPOCH 5 - PROGRESS: at 63.03% examples, 81412 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:36,442 : INFO : EPOCH 5 - PROGRESS: at 64.80% examples, 81798 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:37,514 : INFO : EPOCH 5 - PROGRESS: at 66.25% examples, 81712 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:38,526 : INFO : EPOCH 5 - PROGRESS: at 67.74% examples, 81752 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:39,547 : INFO : EPOCH 5 - PROGRESS: at 69.25% examples, 81757 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:40,562 : INFO : EPOCH 5 - PROGRESS: at 70.70% examples, 81782 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:41,566 : INFO : EPOCH 5 - PROGRESS: at 72.06% examples, 81683 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-07 16:41:42,670 : INFO : EPOCH 5 - PROGRESS: at 73.25% examples, 81298 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:43,752 : INFO : EPOCH 5 - PROGRESS: at 74.72% examples, 81215 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-07 16:41:44,809 : INFO : EPOCH 5 - PROGRESS: at 76.24% examples, 81165 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:45,834 : INFO : EPOCH 5 - PROGRESS: at 77.78% examples, 81182 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:46,841 : INFO : EPOCH 5 - PROGRESS: at 79.33% examples, 81349 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:47,995 : INFO : EPOCH 5 - PROGRESS: at 81.19% examples, 81537 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:49,025 : INFO : EPOCH 5 - PROGRESS: at 82.93% examples, 81778 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:50,089 : INFO : EPOCH 5 - PROGRESS: at 84.57% examples, 81850 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:51,134 : INFO : EPOCH 5 - PROGRESS: at 86.22% examples, 82069 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:52,151 : INFO : EPOCH 5 - PROGRESS: at 87.75% examples, 82194 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:53,175 : INFO : EPOCH 5 - PROGRESS: at 89.38% examples, 82308 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:54,200 : INFO : EPOCH 5 - PROGRESS: at 91.01% examples, 82525 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:55,308 : INFO : EPOCH 5 - PROGRESS: at 92.58% examples, 82510 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:56,386 : INFO : EPOCH 5 - PROGRESS: at 94.48% examples, 82757 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:57,441 : INFO : EPOCH 5 - PROGRESS: at 96.13% examples, 82820 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:58,463 : INFO : EPOCH 5 - PROGRESS: at 97.89% examples, 83008 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-07 16:41:59,487 : INFO : EPOCH 5 - PROGRESS: at 99.37% examples, 83091 words/s, in_qsize 5, out_qsize 0\n",
      "2019-08-07 16:41:59,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-08-07 16:41:59,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-07 16:41:59,791 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-07 16:41:59,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-07 16:41:59,871 : INFO : EPOCH - 5 : training on 8035381 raw words (5720571 effective words) took 68.8s, 83116 effective words/s\n",
      "2019-08-07 16:41:59,873 : INFO : training on a 40176905 raw words (28605705 effective words) took 252.0s, 113512 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 15s, sys: 4.93 s, total: 16min 20s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build word2vec model\n",
    "w2v_num_features = 300\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
    "                                   min_count=10, workers=4, iter=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUZHFsj8RaGo"
   },
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWfcUVixRaGr"
   },
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3GMe1dCfRaGw",
    "outputId": "1eb163f9-fb81-49f9-e568-7b491ce532fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mSFQ9H0RaGy"
   },
   "source": [
    "## Modeling with deep neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oz9gtU9RaGz"
   },
   "source": [
    "### Building Deep neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCI0uEBIRaGz"
   },
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(1))\n",
    "    dnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cQpTH6FRaG1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:43:49,921 : WARNING : From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:43:49,977 : WARNING : From /anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBzbo-YlRaG4"
   },
   "source": [
    "### Visualize sample deep architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "0fuhzYagRaG5",
    "outputId": "864631c3-fff5-4954-e4e2-68c8d5930af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 351,489\n",
      "Trainable params: 351,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "w2v_dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-COslmPRaG8"
   },
   "source": [
    "### Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "0kcisAskRaG8",
    "outputId": "366a74b4-e5ee-4837-8393-21b5f5b67df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 3500 samples\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 16:43:52,424 : WARNING : From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31500/31500 [==============================] - 2s 70us/sample - loss: 0.3267 - acc: 0.8609 - val_loss: 0.3180 - val_acc: 0.8686\n",
      "Epoch 2/10\n",
      "31500/31500 [==============================] - 2s 62us/sample - loss: 0.2973 - acc: 0.8759 - val_loss: 0.2991 - val_acc: 0.8829\n",
      "Epoch 3/10\n",
      "31500/31500 [==============================] - 2s 59us/sample - loss: 0.2894 - acc: 0.8804 - val_loss: 0.2916 - val_acc: 0.8794\n",
      "Epoch 4/10\n",
      "31500/31500 [==============================] - 2s 61us/sample - loss: 0.2803 - acc: 0.8847 - val_loss: 0.3033 - val_acc: 0.8774\n",
      "Epoch 5/10\n",
      "31500/31500 [==============================] - 2s 61us/sample - loss: 0.2725 - acc: 0.8866 - val_loss: 0.3012 - val_acc: 0.8760\n",
      "Epoch 6/10\n",
      "31500/31500 [==============================] - 2s 62us/sample - loss: 0.2666 - acc: 0.8888 - val_loss: 0.3072 - val_acc: 0.8834\n",
      "Epoch 7/10\n",
      "31500/31500 [==============================] - 2s 66us/sample - loss: 0.2591 - acc: 0.8916 - val_loss: 0.3050 - val_acc: 0.8823\n",
      "Epoch 8/10\n",
      "31500/31500 [==============================] - 2s 64us/sample - loss: 0.2504 - acc: 0.8968 - val_loss: 0.3054 - val_acc: 0.8754\n",
      "Epoch 9/10\n",
      "31500/31500 [==============================] - 2s 67us/sample - loss: 0.2417 - acc: 0.8995 - val_loss: 0.3205 - val_acc: 0.8694\n",
      "Epoch 10/10\n",
      "31500/31500 [==============================] - 2s 67us/sample - loss: 0.2346 - acc: 0.9020 - val_loss: 0.3168 - val_acc: 0.8726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179f13cf8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "7C1gcc4iRaG-",
    "outputId": "d49667e7-ebe0-4bad-8a99-96031869b921"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "2XHEZ-X9RaG_",
    "outputId": "8bffc4eb-dec4-4308-bfbf-7fef5706512e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.88      0.88      7490\n",
      "    positive       0.88      0.87      0.88      7510\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6628</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>984</td>\n",
       "      <td>6526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6628       862\n",
       "positive       984      6526"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Text Classification - Machine Learning and Basic DNN Models.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
