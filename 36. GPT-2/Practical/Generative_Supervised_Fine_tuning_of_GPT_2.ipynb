{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDl4Ns0WSKSm"
   },
   "source": [
    "# Generative Supervised Fine-tuning of GPT-2\n",
    "\n",
    "Now that we have our GPT-2 model all trained up - we need a way we can get it to generate what we want.\n",
    "\n",
    "In the following notebook, we're going to use an approach called \"Supervised Fine-tuning\" to achieve our goals today.\n",
    "\n",
    "In essence, we're going to use each example as a self-contained unit (with potential for something called \"packing\") and this is going to allow us to build \"labeled\" data.\n",
    "\n",
    "For this notebook, we're going to be flying quite high up in the levels of abstraction. Take extra care to look into the libraries we're using today!\n",
    "\n",
    "Let's start by grabbing our dependencies, as always:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83053,
     "status": "ok",
     "timestamp": 1754792263496,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "0-RFDpkt7-UJ",
    "outputId": "d3b74cd8-5f3f-4e2c-96ed-4417afe4ea0b"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate datasets trl bitsandbytes -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K7Xg9Y2WdhN"
   },
   "source": [
    "## Dataset Curation\n",
    "\n",
    "We're going to be fine-tuning our model on SQL generation today.\n",
    "\n",
    "First thing we'll need is a dataset to train on!\n",
    "\n",
    "We'll use [this](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset today!\n",
    "\n",
    "First up, let's load it and take a look at what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "072fc77e0a1c4350a44a9a378691665a",
      "d9f8310cb92d4085a5bf1c4a3d4f1f9c",
      "012551ad76e84ef29866b1ea499f4f41",
      "70f8c090a14f4bba9283e55bb3087da5",
      "df2eb5497c844ad88e6e2e3fe045ba2d",
      "5b1ec78715f04f42b20c33277774a09b",
      "26f2a48163ad4d3fac0506ca1cba02ed",
      "0eef7b5618e54c3f90ee5050ec5cdd70",
      "145247cd8c4742fe84ea1a382df7f91e",
      "0d08345b24e8494c8f7acab8b45fbf91",
      "198f23ad978747b99c90e876522dd4e0",
      "bade28bc762e428d878007fbdbfbf058",
      "78a640bb1ae74f83beaa1f692c3e766f",
      "295b921f27d9457fb265ba3c2816c8fd",
      "49c1385ba4ac45a9a61e582bae940b83",
      "d5dd345ac72c40e5b2eff4aa2a771d83",
      "2f22d2a1c993489fa1221ca987b8cda5",
      "af58a69a1fa64544af9e03767528a119",
      "aa3c687478f6429ab006b11121120290",
      "bd258bfc501f46deb5ea8d74c367fca7",
      "c27edb2e2a204324af18c56f21f527a2",
      "ddd9941d9d314b2ca4ee261dca377eee",
      "b78889c4896a430b87569957431f1895",
      "4277e5c49d104bb59c24cdfea1bf861e",
      "2fc79396d4d2420f833c6ee4bd745449",
      "36028aa9804d431286faf7fc050be9c8",
      "05f5cf14cac8424f99dd7c46e81dc9f4",
      "9637c4cd2a7247839bbbf563d9ed962e",
      "6bedb2c236bc4a7db6f2254f86fb2e2e",
      "29f30460ceff4b3194e520a326361d12",
      "80e0bcbb687f446da25c14dfdad67260",
      "9596bcfd8fc34c26b5206725fa22b46e",
      "26ee504b91ca4cc5a0b7c5e599187b9e"
     ]
    },
    "executionInfo": {
     "elapsed": 4730,
     "status": "ok",
     "timestamp": 1754792270651,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "-d-4QaiP_H4Q",
    "outputId": "52127375-1b69-415e-9a82-cb8438a44196"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sql_dataset = load_dataset(\"b-mc2/sql-create-context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754792272562,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "-XpDls7m_3z0",
    "outputId": "f6401dc6-fd88-4381-8304-b57caf5b828b"
   },
   "outputs": [],
   "source": [
    "sql_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754792274378,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "tFFbU1RaB5Tm",
    "outputId": "27882ed4-e109-43f0-c8bc-5038e1a1660f"
   },
   "outputs": [],
   "source": [
    "sql_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu8CHgxDXDN0"
   },
   "source": [
    "So, we've got ~78.5K rows of:\n",
    "\n",
    "- question - a natural language query about\n",
    "- context - the `CREATE TABLE` statement - which gives us important context about the table\n",
    "- answer - a SQL query that is aligned with both the question and the context.\n",
    "\n",
    "Let's split our data into `train`, `val`, and `test` datasets.\n",
    "\n",
    "We can use our `train` and `val` sets to train and evaluate our model during training - and our `test` set to ultimately benchmark the generations of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K5laAgh_6tt"
   },
   "outputs": [],
   "source": [
    "sql_dataset_train_test = sql_dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754792276975,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "ii9cIwA-BHoa",
    "outputId": "01a98254-0b40-4dc2-de7b-cc1ea5533675"
   },
   "outputs": [],
   "source": [
    "sql_dataset_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjAVoV3gA5oZ"
   },
   "outputs": [],
   "source": [
    "sql_dataset_val_test = sql_dataset_train_test[\"test\"].train_test_split(test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754792279457,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "1-UmbR51BrDi",
    "outputId": "7bb1ab64-2131-45fe-ea06-4b5cf6b12418"
   },
   "outputs": [],
   "source": [
    "sql_dataset_val_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srs1wmlmBOkf"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "split_sql_dataset = DatasetDict({\n",
    "    \"train\" : sql_dataset_train_test[\"train\"],\n",
    "    \"val\" : sql_dataset_val_test[\"train\"],\n",
    "    \"test\" : sql_dataset_val_test[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754792282345,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "poLNUmMlBhRJ",
    "outputId": "31223ddf-0889-4e51-b008-cc15527023b4"
   },
   "outputs": [],
   "source": [
    "split_sql_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmEMUwH0XqqR"
   },
   "source": [
    "### Creating a \"Prompt\"\n",
    "\n",
    "Now we need to create a prompt that's going to allow us to interact with our model when we desired the trained behaviour.\n",
    "\n",
    "Think of this as a pattern that aligns the model with our desired outputs.\n",
    "\n",
    "We need a single text prompt, as that is what the `SFTTrainer` we're going to use to fine-tune our model expects.\n",
    "\n",
    "The basic idea is that we're going to merge the `question`, `context`, and `answer` into a single block of text that shows the model our desired outputs.\n",
    "\n",
    "Let's look at what that block needs to look like:\n",
    "\n",
    "```\n",
    "{bos_token}### Instruction:\n",
    "{system_message}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{response}{eos_token}\n",
    "```\n",
    "\n",
    "Let's look at that from a completed prompt perspective to get a bit more information:\n",
    "\n",
    "```\n",
    "<|startoftext|>### Instruction:\n",
    "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "How many locations did the team play at on week 7?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_24123547_2 (location VARCHAR, week VARCHAR)\n",
    "\n",
    "### Response:SELECT COUNT(location) FROM table_24123547_2 WHERE week = 7<|endoftext|>\n",
    "```\n",
    "\n",
    "As you can see, our prompt contains completed examples of our task. We're going to show our model many of these examples over and over again to teach it to produce outputs that are aligned with our goals!\n",
    "\n",
    "First step, let's create a template we can use to call `.format()` on while constructing our prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbZamutqdd8c"
   },
   "source": [
    "###üèóÔ∏èActivity:\n",
    "\n",
    "Create the following templates:\n",
    "\n",
    "- `TEXT2SQL_TRAINING_PROMPT_TEMPLATE`\n",
    "- `TEXT2SQL_INFERENCE_PROMPT_TEMPLATE`\n",
    "\n",
    "> HINT: Remember that during inference we do not want to prepopulate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9UROm23C_9X"
   },
   "outputs": [],
   "source": [
    "TEXT2SQL_TRAINING_PROMPT_TEMPLATE = \"\"\"\\\n",
    "{bos_token}### Instruction:\n",
    "{system_message}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{response}{eos_token}\n",
    "\"\"\"\n",
    "\n",
    "TEXT2SQL_INFERENCE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "{bos_token}### Instruction:\n",
    "{system_message}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIgyltDPZCIX"
   },
   "source": [
    "Now let's create a function we can map over our dataset to create the full prompt text block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJgejJtVdzuV"
   },
   "source": [
    "###üèóÔ∏èActivity:\n",
    "\n",
    "Define a `SYSTEM_MESSAGE` to use with your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4XrahGud7Mn"
   },
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. You must output the SQL query that answers the question.\n",
    "                 \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL1deglfB2wP"
   },
   "outputs": [],
   "source": [
    "def create_sql_prompt(sample):\n",
    "  full_prompt = TEXT2SQL_TRAINING_PROMPT_TEMPLATE.format(\n",
    "      bos_token = \"<|startoftext|>\",\n",
    "      eos_token = \"<|endoftext|>\",\n",
    "      system_message = SYSTEM_MESSAGE,\n",
    "      input = sample[\"question\"],\n",
    "      context = sample[\"context\"],\n",
    "      response = sample[\"answer\"]\n",
    "  )\n",
    "\n",
    "  return {\"text\" : full_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbFq4KFxZKLo"
   },
   "source": [
    "I've created this helper-function to be able to see how our model is doing visibly, rather than only through metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXSLQkQ9GOZP"
   },
   "outputs": [],
   "source": [
    "def create_sql_prompt_and_response(sample):\n",
    "  full_prompt = TEXT2SQL_INFERENCE_PROMPT_TEMPLATE.format(\n",
    "      bos_token = \"<|startoftext|>\",\n",
    "      system_message = SYSTEM_MESSAGE,\n",
    "      input = sample[\"question\"],\n",
    "      context = sample[\"context\"]\n",
    "  )\n",
    "\n",
    "  ground_truth = sample[\"answer\"]\n",
    "\n",
    "  return {\"full_prompt\" : full_prompt, \"ground_truth\" : ground_truth}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9CWaa99ZXIl"
   },
   "source": [
    "Let's look at an example of a formatted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754792338832,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "wYVsO2v7Et8P",
    "outputId": "f1b336a9-98a1-4d5d-ad7e-6a09ab5bb08a"
   },
   "outputs": [],
   "source": [
    "create_sql_prompt(split_sql_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUcHKG2BZZov"
   },
   "source": [
    "Great!\n",
    "\n",
    "Now we can map this over our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "6a9e8cb50fdd4a7bbe74716ba4b8e4ec",
      "5bb2cb15900447ec94d72b66137646e7",
      "e8d8c9e84d4d4c10922b5ddc7fa7b190",
      "8e7e343a8f4a453aabc598a27ce44f6e",
      "d182ac63bc8046c0baa3daa76e29aa0a",
      "2a74dad1c4a34532bbb6f8590f0fd659",
      "5c2aeb404ea5457c8eea4e19c5706944",
      "5e71e1fe81c048f6bfed1a64a38b61a9",
      "8442eaab7ff04b05972f5214b4a9252f",
      "baf086c41cfd482384d62b8b21873092",
      "1a8087978bf4404ba0ed0ba69ec27eae",
      "11c1c63781a242a9b40ab4c25005f256",
      "e5caa6c2e6ab4281949a812bdb3157c6",
      "a750b2440ef845da9317608f1460e257",
      "bab241d8c7f84bb0bc05d5be68d1bcb1",
      "03d01f3fefb849cfbb17a1f20a6c531a",
      "8004c637a3e74a2e8b8cea0f190c07d7",
      "4a7a8b42d6a445f2b38be61534029589",
      "35097bf0b5c04b0ca381fe1003bfacb4",
      "a4c13ab19b4240399ac9854aee4f1ec3",
      "b0660a4c65504857a01efac0aa99a5b7",
      "7aaab5f8109848cb95c4ee4ef90c0dc0",
      "2863d96601044960b71a8335b866bb67",
      "2ed7eb8e41e345aca6582728134d8c00",
      "f3645f2ad2c94f0a823e938bce3869e9",
      "9fda1840cc324adb871d0098ec6e20b0",
      "a54d67409adf49c1b979478f43bb03c1",
      "0cfb4d6b318f4129b413647dc392fb9a",
      "a2b336a7ba5c4f4b8f10c7ce58a3b74a",
      "2b1128a0bd5f4989a45dfa2cb14f1265",
      "273849417e784853953c36d96b4f73a9",
      "b5321ce99e524b1a8d345b9bee666498",
      "c03bd7f6c74347919fe11637301cdf29"
     ]
    },
    "executionInfo": {
     "elapsed": 8193,
     "status": "ok",
     "timestamp": 1754792359253,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "mDaaSE0VReUC",
    "outputId": "b07cd55f-d627-4cff-975f-48d5d9d31b76"
   },
   "outputs": [],
   "source": [
    "split_sql_dataset = split_sql_dataset.map(create_sql_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvmncxWAZc9d"
   },
   "source": [
    "## Load the Model And Preproccessing\n",
    "\n",
    "Now for the moment we've all been waiting for...\n",
    "\n",
    "Loading our model!\n",
    "\n",
    "Let's use the `AutoModelForCausalLM` and `AutoTokenzier` classes from `transformers` to see just how easy this is.\n",
    "\n",
    "- [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/auto#transformers.AutoModelForCausalLM)\n",
    "- [`AutoTokenizer`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [GPT-2 Model Card](https://huggingface.co/gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "61494672df3b4ac19b3de7cebd01f8c8",
      "70a1f3ff5949408a85d4b0977f725cfc",
      "86709bce64f14eb5b92fff8a8c2745b7",
      "4d82727f0e8d48ee902a391ccc0e5597",
      "59bb54e8efff4c0c9ea750f78d771e8d",
      "8e56b21a1c15424d96b360256c43a1ca",
      "776ab5ee9fc54334ad0026289969cf79",
      "ac809864d81245e8bb9b9d6974f89375",
      "341733c70ad14bf4816f191cb73cd082",
      "53d0b606a60642c49a4d9622fb6d9f0e",
      "4a3a9fac11ad4b92873b3e0b2d579d8c",
      "5e74b8e6cdf54a1db59a80bc873be858",
      "6810bf9746494f9c8056118310a695cc",
      "b0b8067c2a784b9cba70731b2933f554",
      "78fd2ff8a011435cbe9b4d9c43390546",
      "f7152941f6f44d52beb57ea7ba6a1672",
      "749153d325c34fea918da6fabe9a17ad",
      "6400d6c008444f59b5e011315d1f94ae",
      "0461c2989603436cabb92df3aaea0665",
      "cdecbe3c7c8949ca8829ae48ca9d0575",
      "a44ca29cff9f46388b4957d5cd560f56",
      "9f4c591f697e41ac9cf893843ea82e44",
      "3b96866c73764b46a965bf881f0a2c43",
      "e4ba150a669b41009df99c5c243a2562",
      "de0faf231563456e8b650f116f63d4b8",
      "909c0f17ee2b4750b725959ea661d0d7",
      "ca1554e8fe494fe082bc5ab090a79345",
      "c1bc10d191fc4702b050f27666984343",
      "7ad72039fda44bf99425263f9dadd2bc",
      "ed9913eb88f14051a852eb7fdbf39225",
      "6ca8225373424b5fb56b7f53e9c8231e",
      "080185a6528d40afb1aada5ba1ba64cc",
      "70afed9135464244ae87813abda9ef64",
      "734bf216906f4ffcbe17e365fdf2e6f5",
      "dcb0a21208c645b19e459816b7e90e3e",
      "1abe9ecdc95f494ebacddcedc3e350ef",
      "4fc0e715ec38462b91856539957d5846",
      "e3e34197525d448ea19d636fd918849c",
      "0b9a833231d44599b1fae5c0e1ebe9c8",
      "6b9db8cb0c5149f7aeaec0d4a4806f22",
      "9176a884d15b4bc48ff9cfa8ed830e67",
      "72d294a9dcbc4c4b96b6e783e9264b6d",
      "6390ca64458b4a9893bfb9f6045bd27c",
      "27a1d787ecd9499e805611336ac1dff7",
      "2118d992bc5148b7bfcb363cb218278c",
      "5a2bdd20f50f41eba5550d93f2bc57c5",
      "6e2e634e72fc4a5ea36af7204a587797",
      "f7150fcdef05418e83d5b36b9c36f209",
      "377eb225a0004dc592136074a06191fd",
      "4b63ad7b62e24ca19ea9f0e3d2d2567a",
      "1590bbdde639469f93228431fd715b49",
      "05c6ee398e8f44338fe7b6413f4d9430",
      "49feadb3d40b4761a56e1ef6db92fd3a",
      "a80a8963690c4ff284f3c759c6942262",
      "89bbd4d9b9b24cda96f5e062d0760bb0",
      "135402c67f474af1aecd37f532d02c61",
      "fd043c4f31db47b7b96f4c3bb291dbeb",
      "fca488027f464caba052d27cd080259c",
      "ab6d1216eda44ea989461759689e062a",
      "d65800c9b532449aa29a9c99b9d71e99",
      "a744d9f912b74026a67178a4ebd4b043",
      "9f873b6713794cfb828c08679e5f9fd1",
      "a8b4f36407f5410cb797222092aaa330",
      "9e32ea5859fe40fbbad573cdb33ca1cb",
      "94a0145866fd46ff981ec3bc1c772c4a",
      "226143b87282420b9598f2d73ef39668",
      "35066fbce8794752b61aabf1169fe1aa",
      "20a78239f79d4046874fc2d7f8ba2eb3",
      "e76e61b40da744979886948c5f62d6bb",
      "2014d1f03ceb4ddab46e9e8d1e7f5730",
      "54b98ae6a43141be81f226a9ae181c68",
      "27e5f3b91fe246bdb224b5f9aa944585",
      "3e5cb9432732460bbb4c437ff12cdfe8",
      "04853dc8e11048e8b74350e4f11f7b6c",
      "55930acb49ea46ca9d9ce31207dd2495",
      "44fc060cfe7348338bc0698c15699131",
      "f4a5632a0d4d47438e64ef22ff6460b0"
     ]
    },
    "executionInfo": {
     "elapsed": 18802,
     "status": "ok",
     "timestamp": 1754792380555,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "OmVijWlv8IEs",
    "outputId": "807d7278-1e63-4b86-ba8e-abe40f9fb531"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "gpt2_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0twGVRK6ZscY"
   },
   "source": [
    "We need to make sure our tokenizer has a `pad_token` in order to be able to pad sequences so they're all the same length.\n",
    "\n",
    "We'll use a little trick here to set our padding token to our eos (end of sequence) token to make training go a little smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-m-LZ8JCbIl"
   },
   "outputs": [],
   "source": [
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wVqPpq1afVe"
   },
   "source": [
    "We also need to make sure we resize our model to be aligned with the token embeddings. If we didn't do this - we'd face a shape error while training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1754792385667,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "t72OiyKUPD_Z",
    "outputId": "25182f90-255c-48dc-b447-49d4f37e2153"
   },
   "outputs": [],
   "source": [
    "gpt2_base_model.resize_token_embeddings(len(gpt2_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swhcqh8GamBx"
   },
   "source": [
    "Now let's use the Hugging Face `pipeline` to see what generation looks like for our untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1754794011159,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "1VLInxcvCdTO",
    "outputId": "0e4cc1c1-ec13-45cd-e8cc-3afbc85093d9"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GenerationConfig\n",
    "\n",
    "generator = pipeline('text-generation', model=gpt2_base_model, tokenizer=gpt2_tokenizer)\n",
    "set_seed(42)\n",
    "\n",
    "def generate_sample(sample):\n",
    "  prompt_package = create_sql_prompt_and_response(sample)\n",
    "\n",
    "  generation_config = GenerationConfig(\n",
    "      max_new_tokens=50,\n",
    "      do_sample=True,\n",
    "      top_k=50,\n",
    "      temperature=1e-4,\n",
    "      eos_token_id=gpt2_base_model.config.eos_token_id,\n",
    "  )\n",
    "\n",
    "  print(\"Input processed : \",prompt_package[\"full_prompt\"])\n",
    "\n",
    "  generation = generator(prompt_package[\"full_prompt\"], generation_config=generation_config)\n",
    "  print(\"---------------\")\n",
    "  print(\"Model Response:\")\n",
    "  print(generation[0][\"generated_text\"].replace(prompt_package[\"full_prompt\"], \"\"))\n",
    "  print(\"+++++++++++++++\")\n",
    "  print(\"Ground Truth\")\n",
    "  print(prompt_package[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1599,
     "status": "ok",
     "timestamp": 1754792394586,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "zz7i-yC1HEE1",
    "outputId": "d8ebc1f7-c681-4b49-9d04-b03ecabca702"
   },
   "outputs": [],
   "source": [
    "generate_sample(split_sql_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPlJ992dawCa"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Now that we have our model set up, our tokenizer set up, we can finally begin training!\n",
    "\n",
    "We'll be using the `TrainingArguments` from Hugging Face's `transformers` library to help us keep track of our hyper-parameters. More information and documentation available:\n",
    "\n",
    "- [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "Let's look at our Trainer, and set some hyper-parameters:\n",
    "\n",
    "- `per_device_train_batch_size` - this is a batch size that accomodates distributed training\n",
    "- `gradient_accumulation_steps` - this is exactly the same as the previous notebook, it's a way to \"simulate\" a large batch size by collecting losses over multiple iterations - scaling them - and then combining them together.\n",
    "- `gradient_checkpointing` - I'll let the authors speak for themselves [here](https://github.com/cybertronai/gradient-checkpointing). In essence: This saves memory at the cost of computational time.\n",
    "- `max_grad_norm` - this is the value used for gradient clipping, which is a method of reducing vanishing gradient potential\n",
    "- `max_steps` - how many steps will we train for?\n",
    "- `learning_rate` - how fast should we learn?\n",
    "- `save_total_limit` - how many versions of the model will we save?\n",
    "- `logging_steps` - how often we should log\n",
    "- `output_dir` - where to save our checkpoints\n",
    "- `optim` - which optimizer to use, you'll notice we're using a full precision paged optimizer - this is a performative and stable optimizer - but it uses extra memory\n",
    "- `lr_scheduler_type` - we are once again using a cosine scheduler!\n",
    "- `evaluation_strategy` - we have an evaluation dataset, this defines when we should leverage it during training\n",
    "- `eval_steps` - how many steps we should evaluate for\n",
    "- `warmup_ration` - how many \"warmup\" steps we take to reach our full learning rate before we start decaying. this is a ration of our max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YepcLPn1F_WS"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#  per_device_train_batch_size=4,\n",
    "#  gradient_accumulation_steps=4,\n",
    "#  gradient_checkpointing =True,\n",
    "#  max_grad_norm= 0.3,\n",
    "#  ###num_train_epochs=2,\n",
    "#  max_steps=500,\n",
    "#  learning_rate=2e-4,\n",
    "#  save_total_limit=3,\n",
    "#  logging_steps=10,\n",
    "#  output_dir=\"sql_gpt2\",\n",
    "#  optim=\"paged_adamw_32bit\",\n",
    "#  lr_scheduler_type=\"cosine\",\n",
    "#  eval_strategy=\"steps\",\n",
    "#  eval_steps=50,\n",
    "#  warmup_ratio=0.05,\n",
    "#  dataset_text_field=\"text\",\n",
    "#  max_seq_length=1024\n",
    "# )\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"sql_gpt2\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",  # correct name\n",
    "    eval_steps=50,\n",
    "    warmup_ratio=0.05,\n",
    "    dataset_text_field=\"text\",  # only works here, not in TrainingArguments\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg61u-HqY8XU"
   },
   "source": [
    "###‚ùìQuestion\n",
    "\n",
    "Is this process using usupervised, or supervised learning?\n",
    "\n",
    "\\#\\#\\# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFtZLpxUdIYD"
   },
   "source": [
    "Now, for our `SFTTrainer` AKA \"Where the magic happens\".\n",
    "\n",
    "You can read all about the `SFTTrainer` here:\n",
    "\n",
    "- [`SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer#trl.SFTTrainer)\n",
    "\n",
    "This `SFTTrainer` is going to take our above training arguments, our data, our model and our tokenizer, and train it all for us!\n",
    "\n",
    "Notice that we're setting `max_seq_length` to the maximum context window of our model - this ensures we do not exceed our maximum context window, and will pad our examples up to the maximum context window!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYMFaSkH0a9T"
   },
   "outputs": [],
   "source": [
    "SFTTrainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7sYx3RWzrAG"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=gpt2_base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_sql_dataset[\"train\"],\n",
    "    eval_dataset=split_sql_dataset[\"val\"],\n",
    "    #tokenizer=gpt2_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "error",
     "timestamp": 1754792562056,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "TlYaurxRWKdF",
    "outputId": "3ce59625-08f6-4c56-86f4-602b4f6c1aba"
   },
   "outputs": [],
   "source": [
    "# config = SFTConfig(\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=1024,\n",
    "#     # add other relevant settings\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#  gpt2_base_model,\n",
    "#  args=config,\n",
    "#  train_dataset=split_sql_dataset[\"train\"],\n",
    "#  eval_dataset=split_sql_dataset[\"val\"],\n",
    "#  tokenizer=gpt2_tokenizer,\n",
    "#  args=training_args\n",
    "# )\n",
    "\n",
    "# from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# config = SFTConfig(\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     # add other relevant settings\n",
    "# )\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     args=config,\n",
    "#     train_dataset=split_sql_dataset[\"train\"],\n",
    "#     eval_dataset=split_sql_dataset[\"val\"],\n",
    "#     tokenizer=tokenizer,              # if needed\n",
    "#     data_collator=data_collator,      # if needed\n",
    "#     # other settings as needed\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI9zuIdxaKv3"
   },
   "source": [
    "###‚ùìQuestion\n",
    "\n",
    "What do we use to determine loss in fine-tuning GPT-2?\n",
    "\n",
    "> HINT: [This](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) can help you if you get stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUyK7GWEdeh0"
   },
   "source": [
    "Finally, we can call our `.train()` method and watch it go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "executionInfo": {
     "elapsed": 500395,
     "status": "ok",
     "timestamp": 1754793847858,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "Hu1tn6ElNDgE",
    "outputId": "80ae06b8-1469-4c6b-e057-0a98b51f53e0"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeEcjAxhdio2"
   },
   "source": [
    "Let's save our fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSpddsskQak4"
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzsQ-kG1dlZB"
   },
   "source": [
    "## Testing our Model\n",
    "\n",
    "Now that we have a fine-tuned model, let's see how it did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3KFqSIkQfk-"
   },
   "outputs": [],
   "source": [
    "ft_gpt2_model = AutoModelForCausalLM.from_pretrained(\"sql_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1754793857805,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "L4UWKrNtQSGg",
    "outputId": "ca73b1cf-d2aa-4417-c257-9d38d3cdb962"
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=ft_gpt2_model, tokenizer=gpt2_tokenizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1754793871995,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "Wqfk3opE3YMO",
    "outputId": "d23f2281-482a-479b-e1a2-424c4a0f3030"
   },
   "outputs": [],
   "source": [
    "split_sql_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1754794019380,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "XPECHdiaQqQh",
    "outputId": "8a6e66c2-b2d0-4be3-cd53-b9130f538ac3"
   },
   "outputs": [],
   "source": [
    "generate_sample(split_sql_dataset[\"test\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnoSWZXaeOWD"
   },
   "source": [
    "That is *significantly* better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sfSkxJUe-v4"
   },
   "source": [
    "#### ###‚ùìQuestion\n",
    "\n",
    "How might you evaluate your generated SQL? Please provide 3 different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ynl0HNAYkTJ"
   },
   "outputs": [],
   "source": [
    "https://huggingface.co/evaluate-metric/spaces?p=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1Csypbkp8wms6NnbosLdcv4ngB3V-djWo",
     "timestamp": 1716016188044
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
