{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1usxOzWwLW9FDQxPetWi62PdRvx_JnMpJ","timestamp":1716011173974},{"file_id":"1FgUKk5TNcwTe2J4GJdTF04nVKFXKFlP5","timestamp":1705015141107}],"collapsed_sections":["eo_QP1ITFfX2"],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Unsupervised Pre-Training of GPT-Style Model\n","\n","In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n","\n","The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n","\n","All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n","\n","> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."],"metadata":{"id":"UWiGVj6njoDn"}},{"cell_type":"markdown","source":["## Data Selection\n","\n","For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n","\n","You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n","\n","> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n","\n","Let's start by grabbing our source repository for the day!"],"metadata":{"id":"eHi04aEnkKEZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMRsEQZy6tgc","executionInfo":{"status":"ok","timestamp":1754790920213,"user_tz":-240,"elapsed":859,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"9ee43b30-e468-4960-e28b-1fc63b06ee33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nanoGPT'...\n","remote: Enumerating objects: 686, done.\u001b[K\n","remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n","Receiving objects: 100% (686/686), 974.07 KiB | 7.01 MiB/s, done.\n","Resolving deltas: 100% (380/380), done.\n"]}],"source":["!git clone https://github.com/karpathy/nanoGPT.git"]},{"cell_type":"markdown","source":["Next, we'll need to grab some dependencies.\n","\n","`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."],"metadata":{"id":"6l4CqoEDl7ks"}},{"cell_type":"code","source":["!pip install tiktoken requests cohere openai -qU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_gepPv1Qdj_","executionInfo":{"status":"ok","timestamp":1754790928535,"user_tz":-240,"elapsed":8312,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"a3dc8011-16d4-4a28-fa48-e645f3a6dd61"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.5/295.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.3/786.3 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["First things first - let's download our dataset!\n","\n","We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set."],"metadata":{"id":"70hSjXmZmCt3"}},{"cell_type":"code","source":["import os\n","import requests\n","import tiktoken\n","import numpy as np\n","\n","current_path = \"/data/shakespeare\"\n","data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","\n","if not os.path.exists(current_path):\n","    os.makedirs(current_path)\n","\n","# download the tiny shakespeare dataset\n","input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n","if not os.path.exists(input_file_path):\n","\n","    with open(input_file_path, 'w') as f:\n","        f.write(requests.get(data_url).text)\n","\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","\n","n = len(data)\n","train_data = data[:int(n*0.9)]\n","val_data = data[int(n*0.9):]"],"metadata":{"id":"T7qRWArUNiZ5","executionInfo":{"status":"ok","timestamp":1754790928990,"user_tz":-240,"elapsed":453,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."],"metadata":{"id":"wU9BG2CymU-a"}},{"cell_type":"code","source":["!pip install tokenizers -qU"],"metadata":{"id":"gFnrwKpQPsYh","executionInfo":{"status":"ok","timestamp":1754790932997,"user_tz":-240,"elapsed":3993,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n","\n","Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n","\n","\n","\n"],"metadata":{"id":"rmWXE5ctma9Z"}},{"cell_type":"markdown","source":["### What is BPE?\n","\n","First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n","\n","The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n","\n","Let's take the following text and break it apart into its word components.\n","\n","\n","```\n","After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n","```\n","\n","A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."],"metadata":{"id":"GLecDiHbogvX"}},{"cell_type":"code","source":["input_text = \"\"\"\n","After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n","\"\"\"\n","\n","naive_word_list = input_text.split()"],"metadata":{"id":"m34NDAGCpiz6","executionInfo":{"status":"ok","timestamp":1754790933023,"user_tz":-240,"elapsed":11,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Now we can count our words and get their frequency."],"metadata":{"id":"hR8k-2bopqjy"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","vocab_and_frequencies = defaultdict(int)\n","\n","for word in naive_word_list:\n","  vocab_and_frequencies[\" \".join(list(word))] += 1\n","\n","sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_201bSQpvqD","executionInfo":{"status":"ok","timestamp":1754790933039,"user_tz":-240,"elapsed":7,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"71a961c5-39b2-484b-8695-ef8b8b04bf1e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."],"metadata":{"id":"NckufSxxp-w5"}},{"cell_type":"code","source":["from typing import Dict, Tuple, List, Set\n","\n","def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n","  vocab = set()\n","\n","  for word in current_vocab.keys():\n","    for subword in word.split():\n","      vocab.add(subword)\n","\n","  return len(vocab)"],"metadata":{"id":"BNcjzjDvvKjp","executionInfo":{"status":"ok","timestamp":1754790933050,"user_tz":-240,"elapsed":10,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["find_vocabulary_size(vocab_and_frequencies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pf3kCf-WvdBL","executionInfo":{"status":"ok","timestamp":1754790933071,"user_tz":-240,"elapsed":20,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"66c33d0b-0817-4831-c3e5-38173bd62b55"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["As we can see, there are 36 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."],"metadata":{"id":"VoMq7GhKqf7p"}},{"cell_type":"markdown","source":["Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."],"metadata":{"id":"OGxrHYmftDTr"}},{"cell_type":"code","source":["def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  pairs = {}\n","\n","  for word, frequency in current_vocab.items():\n","    symbols = word.split()\n","\n","    for i in range(len(symbols) - 1):\n","      pair = (symbols[i], symbols[i + 1])\n","      current_frequency = pairs.get(pair, 0)\n","      pairs[pair] = current_frequency + frequency\n","\n","  return pairs"],"metadata":{"id":"sTwvfTAErQN7","executionInfo":{"status":"ok","timestamp":1754790933090,"user_tz":-240,"elapsed":19,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"],"metadata":{"id":"FudOaKmYv9-y","executionInfo":{"status":"ok","timestamp":1754790933091,"user_tz":-240,"elapsed":1,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGIJfkk7wFYw","executionInfo":{"status":"ok","timestamp":1754790933217,"user_tz":-240,"elapsed":13,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"11f7dd5e-86a9-4f5e-99ce-8910d27e2f07"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('t', 'h'), 11),\n"," (('i', 'n'), 10),\n"," (('r', 'e'), 8),\n"," (('h', 'e'), 8),\n"," (('a', 't'), 7)]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Now that we have the frequent pairs - we can merge those pairs into a single token.\n","\n","Let's see how this process looks in code."],"metadata":{"id":"OqORqdzwsZ6s"}},{"cell_type":"code","source":["import re\n","\n","def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  vocab_out = {}\n","\n","  pattern = re.escape(' '.join(most_common_pair))\n","  replacement = ''.join(most_common_pair)\n","\n","  for word_in in current_vocab:\n","      word_out = re.sub(pattern, replacement, word_in)\n","      vocab_out[word_out] = current_vocab[word_in]\n","\n","  return vocab_out"],"metadata":{"id":"L7ohHm2kshoY","executionInfo":{"status":"ok","timestamp":1754790933234,"user_tz":-240,"elapsed":15,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["new_vocab_and_frequencies = merge_vocab(\n","    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n","    vocab_and_frequencies\n",")"],"metadata":{"id":"Ab760KKuwzZ6","executionInfo":{"status":"ok","timestamp":1754790933235,"user_tz":-240,"elapsed":1,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0XtvLbpxbSx","executionInfo":{"status":"ok","timestamp":1754790933262,"user_tz":-240,"elapsed":16,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"5c482552-b250-4b42-d18b-5ffa223788ba"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["After one merge, we can see that `t h` has been converted to `th`!\n","\n","Let's see how that impacted our vocabulary."],"metadata":{"id":"9DPkBzj2u-me"}},{"cell_type":"code","source":["find_vocabulary_size(new_vocab_and_frequencies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bO_xegCtxjQf","executionInfo":{"status":"ok","timestamp":1754790933277,"user_tz":-240,"elapsed":15,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"728ec192-9cd1-498c-ff04-16cf3cf121a4"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n","\n","In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"],"metadata":{"id":"o3M13D60xzZi"}},{"cell_type":"markdown","source":["## Training Our Tokenizer\n","\n","Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n","\n","Let's walk through the steps we'll take:\n","\n","1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n","\n","  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n","  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n","\n","2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n","\n","  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n","\n","3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n","\n","  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n","  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"],"metadata":{"id":"BePYCbHly02H"}},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from tokenizers.normalizers import NFD, Sequence\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import ByteLevel\n","\n","tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n","tokenizer.normalizer = Sequence([NFD()])\n","tokenizer.pre_tokenizer = ByteLevel()\n","tokenizer.decoder = ByteLevelDecoder()"],"metadata":{"id":"OrztE09OPosB","executionInfo":{"status":"ok","timestamp":1754790933292,"user_tz":-240,"elapsed":14,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n","\n","Let's use the following:\n","\n","- `\"<s>\"`    : bos_token - beginning of sequence token\n","- `\"</s>\"`   : eos_token - end of sequence token\n","- `\"<pad>\"`  : padding_token - token used to pad sequences\n","- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n","- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n","\n","We're also going to set a target vocabulary of 50,000 tokens."],"metadata":{"id":"dDqkNNdM1KsD"}},{"cell_type":"code","source":["trainer = BpeTrainer(\n","    vocab_size=50000,\n","    show_progress=True,\n","    special_tokens=[\n","      \"<s>\",\n","      \"<pad>\",\n","      \"</s>\",\n","      \"<unk>\",\n","      \"<mask>\"\n","    ]\n",")"],"metadata":{"id":"x9iQVhN3P3RN","executionInfo":{"status":"ok","timestamp":1754790933309,"user_tz":-240,"elapsed":17,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["Nothing left to do but point it at our data-source and let it train!\n","\n","We'll use the `.train()` method to accomplish this task.\n","\n","> NOTE: Pay attention to the desired inputs of the `.train()` method.\n","\n","- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"],"metadata":{"id":"yQ8X9vZe2Fyw"}},{"cell_type":"code","source":["tokenizer.train(files=[input_file_path], trainer=trainer)"],"metadata":{"id":"LinLHotSP7gv","executionInfo":{"status":"ok","timestamp":1754790933831,"user_tz":-240,"elapsed":522,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"],"metadata":{"id":"V2JNYiqB2qKV"}},{"cell_type":"code","source":["save_path = '/content/tokenizer'\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","tokenizer.model.save(save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jk6QjDGHQy2K","executionInfo":{"status":"ok","timestamp":1754790933851,"user_tz":-240,"elapsed":8,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"887a7539-ff2c-4658-c8ae-74068c0f53e1"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["!pip install transformers -qU"],"metadata":{"id":"cOOlbggdRFrN","executionInfo":{"status":"ok","timestamp":1754790937481,"user_tz":-240,"elapsed":3623,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"],"metadata":{"id":"us1vofdhQ45C","executionInfo":{"status":"ok","timestamp":1754790949960,"user_tz":-240,"elapsed":12464,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["Let's see how it tokenizes our inputs!"],"metadata":{"id":"0-Bnq7lV2xWo"}},{"cell_type":"code","source":["input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""],"metadata":{"id":"dnYnFa3fTRLf","executionInfo":{"status":"ok","timestamp":1754790949962,"user_tz":-240,"elapsed":1,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["tokenized_sentence = tokenizer.tokenize(input_sentence)\n","tokenized_sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSHY5VufRbBj","executionInfo":{"status":"ok","timestamp":1754790949972,"user_tz":-240,"elapsed":10,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"e0c97d8c-09b1-4f8a-9422-910335f6936f"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hark',\n"," ',',\n"," 'Ġmy',\n"," 'Ġname',\n"," 'Ġbe',\n"," 'ĠRomeo',\n"," '!',\n"," 'ĠI',\n"," 'Ġam',\n"," 'Ġbut',\n"," 'Ġa',\n"," 'Ġbeautiful',\n"," 'Ġsummer',\n"," \"'s\",\n"," 'Ġday',\n"," '!']"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","encoded_tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZrWzQQlTU41","executionInfo":{"status":"ok","timestamp":1754790949977,"user_tz":-240,"elapsed":5,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"af815d6a-b36f-44cd-e353-071d8fbc4c47"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n","decoded_tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oS6lE-NLRnzk","executionInfo":{"status":"ok","timestamp":1754790949993,"user_tz":-240,"elapsed":13,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"30190402-7fb6-492c-d6e6-072f4e6a4e93"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hark, my name be Romeo! I am but a beautiful summer's day!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Tokenizing Dataset\n","\n","Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n","\n","We'll simply encode our training and validation data - and then save them in binary files for later!\n","\n","> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."],"metadata":{"id":"ji3sF-rA21YH"}},{"cell_type":"code","source":["train_ids = tokenizer.encode(train_data)\n","val_ids = tokenizer.encode(val_data)\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"calHML6JPnCU","executionInfo":{"status":"ok","timestamp":1754790952113,"user_tz":-240,"elapsed":2120,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"1917b9b7-5129-42fe-f25f-a09711ec67f1"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["train has 291,284 tokens\n","val has 34,223 tokens\n"]}]},{"cell_type":"code","source":["# export to bin files\n","data_path = \"/data/shakespeare/\"\n","\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n","val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"],"metadata":{"id":"nKJ1KqiiPkRh","executionInfo":{"status":"ok","timestamp":1754790952137,"user_tz":-240,"elapsed":14,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Let's look at our first 100 training tokens to see what format they are in!"],"metadata":{"id":"DFbbvIi7xsgr"}},{"cell_type":"code","source":["train_ids[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9z7ia8AxqEn","executionInfo":{"status":"ok","timestamp":1754790952148,"user_tz":-240,"elapsed":10,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"a9fc6200-2dec-4259-ced2-0730562c2590"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  21,  388,  876,   13,   68, 6804,  373,  153, 2501,  622, 2092,\n","          9,  496,  136,  433,   11,   68,   68,   16,   89,   13,   68,\n","         34, 7882,    9,  433,   11,   68,   68,   21,  388,  876,   13,\n","         68,   40,   73,  252,  227, 3778, 1304,  103,  781,  351,  103,\n","       7504,   15,   68,   68,   16,   89,   13,   68,   33,   97, 5790,\n","         11, 3778,   11,   68,   68,   21,  388,  876,   13,   68,   21,\n","        388,    9,  104,  330, 3317, 1177,  145, 3563, 1766,  103,   80,\n","       1006,   11,   68,   68,   16,   89,   13,   68, 7797,  330,  486,\n","          9,  153,  330,  486,   11,   68,   68,   21,  388,  876,   13,\n","         68], dtype=uint16)"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["###🏗️Activity:\n","\n","Write Python code that will return the first 100 tokens as text.\n","\n","> HINT: An example of this code was used above!"],"metadata":{"id":"aDbAZt4Lx12n"}},{"cell_type":"code","source":["### YOUR CODE HERE"],"metadata":{"id":"za9OzSJDx-dQ","executionInfo":{"status":"ok","timestamp":1754790952156,"user_tz":-240,"elapsed":1,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Training The Model\n","\n","Now that we have our tokenized dataset, let's get to training our model!\n","\n","We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n","\n","First, let's literally jump into the `nanoGPT` repository we cloned earlier."],"metadata":{"id":"c0I3VrRC3XIO"}},{"cell_type":"code","source":["%cd nanoGPT"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUU2jaalUdqm","executionInfo":{"status":"ok","timestamp":1754790952172,"user_tz":-240,"elapsed":10,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"01364a8e-786c-44f7-d043-5d6ccb83f7cc"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanoGPT\n"]}]},{"cell_type":"markdown","source":["We'll do some critical imports."],"metadata":{"id":"13p1e8sa3k0V"}},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","\n","# from the local repo\n","from model import GPTConfig, GPT"],"metadata":{"id":"weNR37BwUYNg","executionInfo":{"status":"ok","timestamp":1754790952201,"user_tz":-240,"elapsed":24,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["### Hyper-Parameters\n","\n","We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."],"metadata":{"id":"kY_vWZG-3uM-"}},{"cell_type":"markdown","source":["#### I/O\n","\n","- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"],"metadata":{"id":"OykCjVQK5EX-"}},{"cell_type":"code","source":["out_dir = 'out'"],"metadata":{"id":"viM3qlWt5PVS","executionInfo":{"status":"ok","timestamp":1754790952202,"user_tz":-240,"elapsed":1,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["#### Initialization\n","\n","Since we're training from scratch, we'll use `init_from = 'scratch'`."],"metadata":{"id":"A5iwwrNL5H4C"}},{"cell_type":"code","source":["init_from = 'scratch'"],"metadata":{"id":"OK1z2m3C312T","executionInfo":{"status":"ok","timestamp":1754790952216,"user_tz":-240,"elapsed":13,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["#### Eval and Logging\n","\n","- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n","- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n","- `eval_iters` - this is how *many* iterations we want to evaluate for.\n","- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n","- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."],"metadata":{"id":"2YlolKOj4_dE"}},{"cell_type":"code","source":["eval_interval = 250\n","eval_iters = 200\n","log_interval = 10\n","eval_only = False\n","always_save_checkpoint = True"],"metadata":{"id":"MbFN5Ltq4_mo","executionInfo":{"status":"ok","timestamp":1754790952220,"user_tz":-240,"elapsed":3,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["#### Dataset\n","\n","We can set our dataset here - we'll use the one we created earlier!"],"metadata":{"id":"a488zaF_4zQk"}},{"cell_type":"code","source":["dataset = 'shakespeare'"],"metadata":{"id":"_QC7vWXC40Hp","executionInfo":{"status":"ok","timestamp":1754790952223,"user_tz":-240,"elapsed":3,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["#### Typical Hyper-Parameters\n","\n","- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n","- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n","- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."],"metadata":{"id":"XP9rBgGc426Q"}},{"cell_type":"code","source":["gradient_accumulation_steps = 1\n","batch_size = 16\n","block_size = 512"],"metadata":{"id":"EM_ybLPP43Pd","executionInfo":{"status":"ok","timestamp":1754790952231,"user_tz":-240,"elapsed":0,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["#### Model Architecture\n","\n","- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n","- `n_head` - this is the number of attention heads in each decoder layer!\n","- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook.\n","- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n","- `bias` - wether or not to use bias inside the LayerNorm/Linear layers.\n","\n","> NOTE: You need to ensure your `n_embd` is cleanly divided by your `n_head`. That is to say:\n",">\n","> `n_embd % n_head == 0`."],"metadata":{"id":"UZ-8bDIY45GS"}},{"cell_type":"code","source":["n_layer = 6\n","n_head = 6\n","n_embd = 516\n","dropout = 0.2\n","bias = False"],"metadata":{"id":"gMyyDBxB6k4H","executionInfo":{"status":"ok","timestamp":1754790952232,"user_tz":-240,"elapsed":0,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["#####❓Question:\n","\n","How many attention heads (total) will our final network have?\n","\n"],"metadata":{"id":"FiTyY5Cotwig"}},{"cell_type":"markdown","source":["#### Optimizer Hyper-Parameters\n","\n","Basic Optimizer Hyper-Parameters:\n","\n","- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n","- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n","\n","Learning Rate Decay Settings:\n","\n","- `decay_lr` - set decay flag\n","- `weight_Decay` - how much to decay lr by\n","- `lr_decay_iters` - should be set to ~max_iters.\n","- `min_lr` - the minimum lr, should be ~ lr / 10\n","\n","Clipping and Warmup:\n","\n","- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n","- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n","\n","> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."],"metadata":{"id":"3NWDTaAz7gwh"}},{"cell_type":"code","source":["# adamw optimizer\n","learning_rate = 1e-3\n","max_iters = 5_0\n","beta1 = 0.9\n","beta2 = 0.99\n","\n","# lr decay settings\n","decay_lr = True\n","weight_decay = 1e-1\n","lr_decay_iters = 5_0\n","min_lr = 1e-4\n","\n","# clipping and warmup\n","grad_clip = 1.0\n","warmup_iters = 10"],"metadata":{"id":"qe-669jwUptI","executionInfo":{"status":"ok","timestamp":1754790952236,"user_tz":-240,"elapsed":3,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."],"metadata":{"id":"ucldc4mz9yeT"}},{"cell_type":"code","source":["backend = 'nccl'\n","device = 'cuda'\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n","compile = True\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","# -----------------------------------------------------------------------------\n","master_process = True\n","seed_offset = 0\n","ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","os.makedirs(out_dir, exist_ok=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHiGlMOp8Nux","executionInfo":{"status":"ok","timestamp":1754790952285,"user_tz":-240,"elapsed":48,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"e2a3b9ab-f1f9-46cc-8dbc-0d07cacfb3b0"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["tokens per iteration will be: 8,192\n"]}]},{"cell_type":"markdown","source":["### Torch Settings\n","\n","We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n","\n","Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."],"metadata":{"id":"eKmdfbye-BNf"}},{"cell_type":"code","source":["torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'cuda' if 'cuda' in device else 'cpu'\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"],"metadata":{"id":"yh34QGD6VARU","executionInfo":{"status":"ok","timestamp":1754790952335,"user_tz":-240,"elapsed":49,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["### Dataloader\n","\n","This block will:\n","\n","1. Set the data path\n","2. Load the dataset we tokenized earlier from the `.bin` we saved\n","3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."],"metadata":{"id":"gKeNwYaZ-Zoc"}},{"cell_type":"code","source":["data_dir = os.path.join('/data', dataset)\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y"],"metadata":{"id":"tOjaPyJpVEgx","executionInfo":{"status":"ok","timestamp":1754790952347,"user_tz":-240,"elapsed":11,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["Let's look at what an example of our batches would look like.\n","\n","To remind ourselves:\n","\n","- `train_data` - has ~2.9 million entries\n","- `block_size` - is 512\n","- `batch_size` - is 16"],"metadata":{"id":"1Z7vMU34yRbq"}},{"cell_type":"code","source":["ix = torch.randint(len(train_data) - block_size, (batch_size,))\n","x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n","y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"],"metadata":{"id":"9_-Y5RZ-yX2a","executionInfo":{"status":"ok","timestamp":1754790952355,"user_tz":-240,"elapsed":7,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["print(f\"Our randomly selected indices were: {ix}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JDxXph4yh2g","executionInfo":{"status":"ok","timestamp":1754790952364,"user_tz":-240,"elapsed":8,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"0952d471-1f60-44eb-d363-1dff0dd62506"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n","        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n"]}]},{"cell_type":"code","source":["print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAzHJH-kzK5E","executionInfo":{"status":"ok","timestamp":1754790952378,"user_tz":-240,"elapsed":13,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"a07193c2-4b29-4569-8369-e921ad614918"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["The first 10 elements of `x` at the first randomly selected index is:\n","tensor([   68,    16,    81,  2358, 19949,   116,   172,  1280,     9,    68])\n"]}]},{"cell_type":"code","source":["print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbaF6v8ezkWn","executionInfo":{"status":"ok","timestamp":1754790952380,"user_tz":-240,"elapsed":2,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"4d482b1e-ad36-49fc-dd04-8c5636a86649"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["The first 10 elements of `y` at the first randomly selected index is:\n","tensor([   16,    81,  2358, 19949,   116,   172,  1280,     9,    68,    16])\n"]}]},{"cell_type":"markdown","source":["#####❓Question:\n","\n","Both `x` and `y` are lists of tokens - as is expected - but what relationship to you notice between `x` and `y`?\n","\n","> HINT: What is our auto-regressive language model trying to predict?"],"metadata":{"id":"zOwOEI0Pzntj"}},{"cell_type":"markdown","source":["So the first component selects a random index from our training data (accounting for our block size)"],"metadata":{"id":"N62oDfdWy0XJ"}},{"cell_type":"markdown","source":["### Simple Initialization of Model\n","\n","Here we init our number of iterations as 0, and our best val loss as a very high number."],"metadata":{"id":"EbDlW-68_atH"}},{"cell_type":"code","source":["iter_num = 0\n","best_val_loss = 1e9"],"metadata":{"id":"6hsepdVBVzQU","executionInfo":{"status":"ok","timestamp":1754790952389,"user_tz":-240,"elapsed":2,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["Obtain our vocab size from our trained tokenizer."],"metadata":{"id":"A4Uj9qBI_vXc"}},{"cell_type":"code","source":["meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = tokenizer.vocab_size\n","meta_vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m53DcCdFV0_a","executionInfo":{"status":"ok","timestamp":1754790952397,"user_tz":-240,"elapsed":8,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"ef2a3f55-a2bb-4393-8445-8375803ebc05"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20099"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["Create our model args dict."],"metadata":{"id":"V7bcNelYARmD"}},{"cell_type":"code","source":["model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout)"],"metadata":{"id":"JfIWEbanV7ZS","executionInfo":{"status":"ok","timestamp":1754790952407,"user_tz":-240,"elapsed":9,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["Instantiate our model with the provided `model_args`.\n","\n","These are derived from the hyper-parameters we set above."],"metadata":{"id":"2WWcbkiCAUI2"}},{"cell_type":"code","source":["if init_from == 'scratch':\n","    print(\"Initializing a new model from scratch\")\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Xly4iA0V-vF","executionInfo":{"status":"ok","timestamp":1754790953115,"user_tz":-240,"elapsed":707,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"5d40eec2-a08e-4652-d60b-71f14819e485"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing a new model from scratch\n","number of parameters: 29.55M\n"]}]},{"cell_type":"markdown","source":["There we go! If you used the default values - you should have a model with 29.55M parameters!\n","\n","Let's set our block_size to the correct size as determined in our configuration steps."],"metadata":{"id":"BpViOsxLAl6p"}},{"cell_type":"code","source":["if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size"],"metadata":{"id":"TrEawNxdWRhm","executionInfo":{"status":"ok","timestamp":1754790953121,"user_tz":-240,"elapsed":7,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["Now we can look at our model in all its glory!"],"metadata":{"id":"eRgguPLKAuZ5"}},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaE3KSTnAtJs","executionInfo":{"status":"ok","timestamp":1754790953264,"user_tz":-240,"elapsed":142,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"8ae114e4-7ac8-4cb9-b383-fd2e79926616"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(20099, 516)\n","    (wpe): Embedding(512, 516)\n","    (drop): Dropout(p=0.2, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln_1): LayerNorm()\n","        (attn): CausalSelfAttention(\n","          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n","          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n","          (attn_dropout): Dropout(p=0.2, inplace=False)\n","          (resid_dropout): Dropout(p=0.2, inplace=False)\n","        )\n","        (ln_2): LayerNorm()\n","        (mlp): MLP(\n","          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm()\n","  )\n","  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",")"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["###🏗️Activity:\n","\n","Label the following components of the transformer architecture with the module names of our newly device-cast model!\n","\n","The first layer is provided as an example.\n","\n","- Layer Normalization: `ln_f`\n","- Linear Projection for Output:\n","- Attention Mechanism:\n","- Positional Encoding:\n","- Embeddings:\n","- Feed Forward Network:\n","\n"],"metadata":{"id":"oHVw3KF3uvIA"}},{"cell_type":"markdown","source":["We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."],"metadata":{"id":"LzoEY6gcBOSp"}},{"cell_type":"code","source":["scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"],"metadata":{"id":"BNUThRt4WT5H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754790953280,"user_tz":-240,"elapsed":8,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"8b2780f5-1f91-4288-80e2-a30bdd755932"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1217904957.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"]}]},{"cell_type":"markdown","source":["Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."],"metadata":{"id":"6Zs5Hcf9BBUD"}},{"cell_type":"code","source":["optimizer = model.configure_optimizers(\n","    weight_decay,\n","    learning_rate,\n","    (beta1, beta2),\n","    device_type\n",")\n","\n","checkpoint = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YesGeUnoWViL","executionInfo":{"status":"ok","timestamp":1754790953286,"user_tz":-240,"elapsed":6,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"4e99e6b2-d79f-4335-c082-4e1f3b686a1d"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["num decayed parameter tensors: 26, with 29,805,708 parameters\n","num non-decayed parameter tensors: 13, with 6,708 parameters\n","using fused AdamW: True\n"]}]},{"cell_type":"markdown","source":["Now we can compile our model!\n","\n","If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n","\n","Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."],"metadata":{"id":"ZF5YWJoKB4og"}},{"cell_type":"code","source":["if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0FNU0T0WXdI","executionInfo":{"status":"ok","timestamp":1754790955378,"user_tz":-240,"elapsed":2091,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"7137511e-4f86-42fa-8e17-445a25c05b32"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["compiling the model... (takes a ~minute)\n"]}]},{"cell_type":"markdown","source":["We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n","\n","You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."],"metadata":{"id":"p6lRcVsZCXRO"}},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"lUB5zVLVWbhM","executionInfo":{"status":"ok","timestamp":1754790955380,"user_tz":-240,"elapsed":0,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["### Creating our LR Scheduler\n","\n","Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n","\n","We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n","\n","![img](https://i.imgur.com/KoFEl0b.png)\n","\n","There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"],"metadata":{"id":"fLsOpaACDDkF"}},{"cell_type":"code","source":["def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n","    return min_lr + coeff * (learning_rate - min_lr)"],"metadata":{"id":"7-mNpWBSWdHh","executionInfo":{"status":"ok","timestamp":1754790955381,"user_tz":-240,"elapsed":0,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["###❓Question:\n","\n","What advantages does a learning-rate scheduler have over a static learning rate?\n","\n","Feel free to consult and cite any resources you find!"],"metadata":{"id":"UV0qN0fDwdOF"}},{"cell_type":"markdown","source":["We need to set some specific values in our env to allow training in Colab."],"metadata":{"id":"cqFePCZmE1Lq"}},{"cell_type":"code","source":["!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7nDL6s4YT6E","executionInfo":{"status":"ok","timestamp":1754790960288,"user_tz":-240,"elapsed":4897,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"ad7500c5-f736-4ca2-eecd-66bf93d5eae1"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n"]}]},{"cell_type":"markdown","source":["## The Training Loop\n","\n","Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"],"metadata":{"id":"Nhqmxeo0Eg0Z"}},{"cell_type":"code","source":["max_iters = 1000\n","always_save_checkpoint = True\n","out_dir = \"/content/out\"\n","os.makedirs(out_dir, exist_ok=True)\n"],"metadata":{"id":"2FigLzjAvVvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, Y = get_batch('train')\n","t0 = time.time()\n","local_iter_num = 0\n","raw_model = model\n","running_mfu = -1.0 # model flops utilization\n","best_val_loss = float('inf')\n","\n","while True:\n","    # determine and set the learning rate for this iteration\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    # evaluate the loss on train/val sets and write checkpoints\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                print(\"inside checkpoint\")\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    # forward backward update, with optional gradient accumulation to simulate larger batch size\n","    # and using the GradScaler if data type is float16\n","    for micro_step in range(gradient_accumulation_steps):\n","        with ctx:\n","            logits, loss = model(X, Y)\n","            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n","        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n","        X, Y = get_batch('train')\n","        # backward pass, with gradient scaling if training in fp16\n","        scaler.scale(loss).backward()\n","    # clip the gradient\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    # step the optimizer and scaler if training in fp16\n","    scaler.step(optimizer)\n","    scaler.update()\n","    # flush the gradients as soon as we can, no need for this memory anymore\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # timing and logging\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        # get loss as float. note: this is a CPU-GPU sync point\n","        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5: # let the training loop settle a bit\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","    # termination conditions\n","    if iter_num > max_iters:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHbyEapRWmpc","executionInfo":{"status":"ok","timestamp":1754791765434,"user_tz":-240,"elapsed":20789,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"8b278cda-d72c-42f9-ac89-76cec572eb2d"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["step 500: train loss 4.4750, val loss 5.0898\n","inside checkpoint\n","saving checkpoint to /content/out\n","iter 500: loss 4.5732, time 3363.68ms, mfu -100.00%\n","iter 510: loss 4.5239, time 20.53ms, mfu 25.11%\n","iter 520: loss 4.4821, time 20.56ms, mfu 25.11%\n","iter 530: loss 4.5340, time 20.58ms, mfu 25.10%\n","iter 540: loss 4.4946, time 20.62ms, mfu 25.09%\n","iter 550: loss 4.5435, time 20.50ms, mfu 25.10%\n","iter 560: loss 4.3862, time 20.59ms, mfu 25.09%\n","iter 570: loss 4.5043, time 20.57ms, mfu 25.09%\n","iter 580: loss 4.4000, time 20.52ms, mfu 25.09%\n","iter 590: loss 4.4004, time 20.51ms, mfu 25.09%\n","iter 600: loss 4.4958, time 20.54ms, mfu 25.09%\n","iter 610: loss 4.4228, time 20.52ms, mfu 25.10%\n","iter 620: loss 4.4289, time 20.52ms, mfu 25.10%\n","iter 630: loss 4.4545, time 20.54ms, mfu 25.10%\n","iter 640: loss 4.4479, time 20.54ms, mfu 25.10%\n","iter 650: loss 4.4258, time 20.49ms, mfu 25.10%\n","iter 660: loss 4.3501, time 20.51ms, mfu 25.11%\n","iter 670: loss 4.3797, time 20.56ms, mfu 25.10%\n","iter 680: loss 4.2734, time 20.42ms, mfu 25.12%\n","iter 690: loss 4.4258, time 20.57ms, mfu 25.11%\n","iter 700: loss 4.4806, time 20.56ms, mfu 25.11%\n","iter 710: loss 4.3887, time 20.51ms, mfu 25.11%\n","iter 720: loss 4.2813, time 20.55ms, mfu 25.11%\n","iter 730: loss 4.4011, time 20.56ms, mfu 25.10%\n","iter 740: loss 4.1859, time 20.51ms, mfu 25.10%\n","step 750: train loss 4.2332, val loss 5.0086\n","inside checkpoint\n","saving checkpoint to /content/out\n","iter 750: loss 4.2091, time 3600.01ms, mfu 22.61%\n","iter 760: loss 4.3288, time 20.57ms, mfu 22.85%\n","iter 770: loss 4.2115, time 20.44ms, mfu 23.09%\n","iter 780: loss 4.2367, time 20.55ms, mfu 23.29%\n","iter 790: loss 4.2326, time 20.54ms, mfu 23.47%\n","iter 800: loss 4.3364, time 20.53ms, mfu 23.63%\n","iter 810: loss 4.2746, time 19.70ms, mfu 23.89%\n","iter 820: loss 4.3192, time 20.48ms, mfu 24.01%\n","iter 830: loss 4.2458, time 20.52ms, mfu 24.12%\n","iter 840: loss 4.1472, time 20.54ms, mfu 24.22%\n","iter 850: loss 4.2600, time 20.96ms, mfu 24.26%\n","iter 860: loss 4.3108, time 20.52ms, mfu 24.34%\n","iter 870: loss 4.1959, time 20.52ms, mfu 24.42%\n","iter 880: loss 4.0777, time 19.99ms, mfu 24.56%\n","iter 890: loss 4.0891, time 20.51ms, mfu 24.62%\n","iter 900: loss 4.1397, time 20.64ms, mfu 24.65%\n","iter 910: loss 4.1145, time 20.48ms, mfu 24.70%\n","iter 920: loss 4.1904, time 20.47ms, mfu 24.75%\n","iter 930: loss 4.0521, time 20.56ms, mfu 24.78%\n","iter 940: loss 4.1834, time 20.49ms, mfu 24.82%\n","iter 950: loss 4.2998, time 20.53ms, mfu 24.85%\n","iter 960: loss 4.2345, time 20.44ms, mfu 24.89%\n","iter 970: loss 4.0567, time 20.52ms, mfu 24.91%\n","iter 980: loss 4.1957, time 20.55ms, mfu 24.93%\n","iter 990: loss 4.0160, time 20.50ms, mfu 24.95%\n","step 1000: train loss 4.0325, val loss 4.9728\n","inside checkpoint\n","saving checkpoint to /content/out\n","iter 1000: loss 4.2454, time 3631.41ms, mfu 22.47%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LEFB64wUtJFz","executionInfo":{"status":"ok","timestamp":1754791731084,"user_tz":-240,"elapsed":38,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmznY_Mau95B","executionInfo":{"status":"ok","timestamp":1754791668488,"user_tz":-240,"elapsed":100,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"7d7cd424-291b-4530-9b22-d8e12b2fa998"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanoGPT\n"]}]},{"cell_type":"markdown","source":["## Generating Outputs with our New Model\n","\n","Now we can leverage the `sample.py` file to generate outputs from our model!"],"metadata":{"id":"L2J5JlRxFJOM"}},{"cell_type":"markdown","source":["### Generation Set Up and Model Loading"],"metadata":{"id":"eo_QP1ITFfX2"}},{"cell_type":"code","source":["import os\n","import pickle\n","from contextlib import nullcontext\n","import torch\n","import tiktoken\n","from model import GPTConfig, GPT\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'out' # ignored if init_from is not 'resume'\n","start = \"\\n my love the war\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"],"metadata":{"id":"-vftqU9LheEK","executionInfo":{"status":"ok","timestamp":1754792042039,"user_tz":-240,"elapsed":10,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQRB3j7iiNkl","executionInfo":{"status":"ok","timestamp":1754792043951,"user_tz":-240,"elapsed":942,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"3d89d8bb-8e5b-4f89-d988-9059960ed8bb"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 29.55M\n"]}]},{"cell_type":"code","source":["model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)"],"metadata":{"id":"N1YAy8DriVZZ","executionInfo":{"status":"ok","timestamp":1754792044729,"user_tz":-240,"elapsed":25,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["enc = tokenizer\n","encode = lambda s: enc.encode(s)\n","decode = lambda l: enc.decode(l)"],"metadata":{"id":"KoB-5ZuLicAT","executionInfo":{"status":"ok","timestamp":1754792045857,"user_tz":-240,"elapsed":2,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}}},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":["### Generation!"],"metadata":{"id":"mkTQ9wo7FjYU"}},{"cell_type":"code","source":["# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n","\n","# run generation\n","with torch.no_grad():\n","    with ctx:\n","        for k in range(num_samples):\n","            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","            print(decode(y[0].tolist()))\n","            print('---------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmTcaHCjii5l","executionInfo":{"status":"ok","timestamp":1754792069913,"user_tz":-240,"elapsed":22676,"user":{"displayName":"Mohamed Noordeen Alaudeen","userId":"13097854355562551879"}},"outputId":"a63a8596-139c-45af-95ab-229b620811ed"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," my love the war:\n","\n","\n","\n","\n","\n","The people:\n","Nis,\n","To the other are\n","LUS:\n","And is he OF this king.\n","Mirst II:\n","\n","You are\n","\n","And you's\n","GLTER:\n","POMUEEN:\n","That had,\n","\n","\n","\n","CTER:\n","\n","CYTER:\n","\n","\n","His lord, I say the own than a world,'s good, that not, in it; they do speak,\n","\n","M should he do say for me!\n","W do, my man,\n","\n","\n","\n","\n","\n","I 'isKINGHAM:\n","\n","\n","That 'ouf my lord, I am do not me,\n","Tith will will on the BOLINGBROKE:\n","Her, an cause:\n","\n","\n","Bot my lord, be.\n","\n","P III:\n","CSAB MARGARET:\n","So is too are I am have was not.\n","To is a good did\n","And a I know my good other were you am a own,--\n","How from my queen, but I will,\n","\n","SostXENES:\n","But may in his lady,\n","\n","He,\n","Let to a part,\n","\n","Sne look my mother.\n","GUS: and not thy good heart,\n","\n","ANENKINGHAM:\n","\n","\n","As thou say.\n","Mid I know a will not my king,\n","\n","\n","\n","\n","And such that not, if his father:\n","I cannot and I?\n","Ff no,\n","Than a brother, sir, the man,\n","To be me;\n","I is the dead.\n","HnONTES:\n","\n","If, here,\n","\n","\n","\n","What,\n","\n","Why, the poor more much\n","The more, you, take the house.\n","\n","We.\n","\n","Ntis die,\n","The- wife!\n","\n","\n","\n","H'ith by a true,\n","O, but, let a own good\n","And I'll may pray your lips is not now home.\n","\n","May, for a poor, my VI:\n","And be both, I am, who,\n","To thy stay.\n","And I have my love of thee, O,\n","\n","\n","To this man,\n","By myELLA:\n","And as a city!\n","I would come.\n","BroTER:\n","But\n","---------------\n","\n"," my love the war,\n","NUEEN HENRY OFIO:\n","NEN lord,\n","\n","\n","\n","\n","\n","I am the majesty.\n","\n","And this like be\n","\n","\n","Thisou'll to.\n","\n","To brother;\n","That'll\n","I am we I have\n","\n","\n","Your brother,\n","'o were I had,\n","\n","\n","you was the lord.\n","To to not and I noble lord,\n","\n","\n","Rs be'shich you will so day,\n","\n","\n","\n","As you is as the cause;\n","And stay, we tell\n","\n","To I's art all you,\n","\n","That love thus,\n","With an noble\n","\n","The\n","Then, gentle lord,\n","MUEEN:\n","\n","\n","\n","They have be and a king!\n","Who,\n","\n","M,\n","Or and be are not to my grace, but I do I that I would Edward?\n","I do I is their power, and I'll not, I art thee!\n","Her, that not- sun, the father, I?\n","More,\n","\n","What,\n","To have know the king;\n","Wf for my eyes:\n","For but a king is them and all the heart is the\n","Which,\n","YICIUS:\n","The day,\n","When I am the hand, where in you should Warwick,\n","Will it?\n","\n","\n","\n","\n","And we will.\n","\n","\n","CORKINGHAM:\n","And not and have thine,\n","\n","Fs he is the first,\n","And\n","CEN man\n","I'll now.\n","\n","The king,\n","Fitf come, by his heart's good\n","\n","UORDUKE VINCENTIO:\n","\n","\n","I's my love; I am make yourIO:\n","To how the soul, sir, and all him.\n","\n","And sir, or what love, and be to, I being leave his prince,\n","As I am how the lord,\n","JYpherd:\n","\n","\n","\n","And father, by it not a husband, but her his well,\n","I, I am not, and my lord, I must dead.\n","\n","Cou shall die,\n","Y' the very more,\n","Have,\n","\n","If it, for you, for for he must friends,\n","\n","GLO.\n","\n","And have your think\n","VOL EDWARD:\n","\n","---------------\n","\n"," my love the war,\n","Be,\n","The mother;\n","\n","So is do sweet highness'hich is a own head, he have God, if on me;\n","What, and then?\n","If I have my king, and what most blood.\n","\n","We'll to me, and I have thou.\n","Ahere will as be fair?\n","ha RICHARDTER:\n","First more.\n","And me.\n","An crown,\n","to, O,\n","\n","\n","\n","I would a other.\n","And I do not,\n","\n","Than thou\n","Romeo,\n","To is thou look,\n","Ltwere you to thy wife, no well too;\n","Your lord, you not then, she,\n","\n","The lord's in the BOLINGBROKE:\n","And, to the business, and she in the sun\n","Cond\n","Be'll I have have not that thou do you, we shall I have I'll what is mynight;\n","Mf his own in me in seen\n","\n","\n","GLO:\n","\n","So for my lord, I's be be heart's with a king, and by him,\n","\n","I have them.\n","For that are so I'll but all,\n","W must's a VINCENTIO:\n","\n","OULI III:\n","Sith me,\n","\n","And it were it not,\n","CUEEN:\n","I'll no face;\n","\n","For I a city of thee with me!\n","And at hef it are my lord, to most man, but a man is myENIUS:\n","\n","\n","\n","PORSAB:\n","\n","\n","\n","And thou noble own.\n","G: and see the VINCENTIO:\n","And that nothing, what have thyINIUS:\n","That bring for he have be by my honour, Ihich in the father, as make for you is the body, my good hour,\n","\n","But when her hand, not him, my that have himself, what do what,\n","IYet is it,\n","First out,\n","Jre the time and be the in thy more's.\n","Fre I can I will.\n","That,\n","Way is a time!\n","\n","And some man, I would before the world.\n","Where's the earth,\n","\n","To not and this, and I done\n","To If we is the name:\n","\n","not Iith so will a man,\n","And be be tell the\n","---------------\n","\n"," my love the war,\n","\n","\n","\n","POMUEEN:\n","\n","Fou is we have by the king:\n","WICET:\n","O,\n","\n","\n","As I'll be.\n","\n","Igainst my grace,\n","\n","COM be do my son,\n","PUTUS:\n","C- first?\n","And they is your shall my a crown and they shall have a soldiers,\n","Rre he as the grace, then, with the life,\n","J RICHARDUS:\n","And done,\n","No, as it not,\n","W our queen, as a face,\n","How,\n","To too;\n","PUS:\n","And had all his king,\n","There, good do my head,\n","\n","But I come and my night, but or love,\n","\n","\n","Hay, I love'd,\n","\n","\n","\n","\n","Rven ' must we am doth me in see,\n","\n","Of him of our friends,\n","We had?\n","KING RICHARDET:\n","Peace, for a\n","Fou will have so\n","ERI RICHARDET:\n","\n","\n","Tirst OF AUMERLE:\n","\n","\n","\n","Wond own be?\n","ROM York'd of my the man,\n","Fon\n","\n","\n","Than it, I have his other.\n","Earry, I is, they will may I'll tell me.\n","\n","Sould see the name,\n","\n","\n","\n","MUKEUCES:\n","How'll nothing your body,\n","Then,\n","\n","\n","\n","Will thou; go, God?\n","For I will most general,\n","With do there is a\n","Of it,\n","The lord!\n","\n","To they are not?\n","\n","Pf not,\n","With that I be not, by her.\n","The fair are you did.\n","AhLO:\n","SondET:\n","\n","Or;\n","That where,\n","My other?\n","\n","UYill sweet's\n","MON thee,\n","Shall thou I the eyes, but an husband;\n","DntYou is you my royal shall pardon this brother, if I have you that heart,\n","\n","MAR VINCENTIO:\n","Of but we good, and not,\n","I,\n","\n","For thy lord,\n","And all it thus.\n","No, he cannot mine,\n","SUC,\n","Which had his love, now to I will which be night, and be to I do too\n","---------------\n","\n"," my love the war;\n","What art he,\n","\n","And you of, make your matter.\n","The sight, and son:\n","O, 'own:\n","A\n","\n","Though this crown, ever,\n","it, I\n","Why head's not,\n","\n","Well,\n","Hf I do this man.\n","I will can that is more,\n","Y II:\n","To say my head?\n","But this make,\n","Dhere do by the queen- king, if thy royal than nothing;\n","MecondELLA:\n","For be have I have have not with the honour; do me!\n","Fith you, you bear the heart.\n","RAR deed,\n","\n","\n","UYome say; as the a part.\n","That you them; if you\n","By he was it will are to the good have he with like it'd,\n","'ith make I'll our one where an mother, good bear it in no more,\n","\n","\n","\n","\n","\n","\n","\n","PONTES: how him,\n","O is all not them,\n","\n","For the other and I do\n","Gnvost:\n","It,\n","I should her.\n","But be for a lord, my woman, by it.\n","And not.\n","you,\n","I are I must I am make me;\n","\n","\n","Which would '' the good king,\n","\n","SOR VINCENTIO:\n","\n","ANer wilt and 'et, whose be he do now and be my stay for them,\n","\n","Sis my brother's sweet.\n","POM him, I is too,\n","Sird.\n","Thf your king!\n","Mhose will some,\n","\n","To be us,\n","\n","If a death, one shall\n","\n","BoldELLA:\n","\n","And my thousand,\n","And his head,\n","\n","Your lord.\n","\n","\n","That,\n","Hith that did not I,\n","O, ere it\n","\n","Sere.\n","Be will take the mother,\n","To an mother in what a\n","My husband and all.\n","In am that,\n","\n","\n","My hands to the life,\n","Thf her-house,\n","Ff by not to peace, myou to do his time, and a YORK:\n","H may were in the Lady:\n","So you is them:\n","Fou, I,\n","MYnET:\n","We, thet be am,\n","\n","---------------\n","\n"," my love the war.\n","\n","\n","ThouAMILLO:\n","\n","Ast how much for out\n","It,\n","\n","be,\n","Your other, tell his world,\n","As mine?\n","And for myt be with of no shall not, but I have not be not,\n","Be good, as he shall you,\n","\n","Iith anVOLIO:\n","\n","Fs thee.\n","\n","That with great news:\n","To,\n","\n","And here,\n","Be, your life,\n","Wnd be and the.\n","Say, when you\n","Cho wilt and- company, he not that kill the LAURENCE:\n","I'll he have he are the eyes with the head, if me.\n","\n","\n","\n","PUEENUKELO:\n","\n","The friends, how find I will thou art on not we'll most go and give thyELLA:\n","And keep the good good to myt my great truth,\n","\n","I will a first,\n","DUKE:\n","I will gone him,\n","And me like come.\n","Sirstvost:\n","\n","\n","And be how.\n","\n","\n","\n","\n","\n","\n","\n","Who:\n","G,\n","\n","Of he cannot hast be your crown,\n","\n","\n","Or have me and done my city of the own son.\n","If my lord!\n","DUKE VINCENTIO:\n","I do you have,\n","A:\n","\n","SLONTES:\n","S ANNE:\n","\n","I know your man.\n","\n","I I am I;\n","Oake a king.\n","RTER:\n","BUCUCet I beseech mine,\n","\n","\n","ROM thine.\n","As do shef a go.\n","\n","KINGelENIUS:\n","And call me,\n","Iould make Rome.\n","\n","CYet you do he shall it, to themorrow:\n","\n","\n","Come, your lord:\n","\n","And have to the lord,\n","\n","Alas,\n","KING:\n","\n","Gnto have.\n","Mre I doth do hast to be our sons,,\n","\n","And love.\n","\n","\n","POMEO:\n","\n","ThSAB VINCENTIO:\n","Ser theENIUS:\n","\n","\n","\n","\n","\n","Thou VINCENTIO:\n","To meet him,\n","Hither,\n","I must not we'll he might this hence,\n","Of do,\n","\n","To he are say\n","To then, she, for\n","---------------\n","\n"," my love the war.\n","Away!\n","\n","PLO,\n","CAor we know the brother'st are be by your lord;\n","LAt you, nor friends:\n","\n","And to my lord!\n","\n","\n","Sn ELIZABETH:\n","\n","Lord:\n","Sould be that is not you will have true, they are you would be it his in me,\n","\n","\n","I will not, myt be most Duke for be made,\n","\n","Fet thyou had I- AUMERLE:\n","\n","Snd now, I had up, let it in it, or know,\n","\n","To I'll I'll be be both but been,\n","\n","O's stand for the love he,\n","I am the soul.\n","\n","Of sir,\n","As thy grace,\n","I,\n","\n","CTo mean with theWICK:\n","\n","Cith speak shall be I know thy lord!\n","\n","\n","\n","LINA:\n","What will else.\n","That and be\n","\n","\n","NOMEO:\n","\n","Why, and my you, bear me, I love\n","\n","SAP lord, there.\n","I am find your part.\n","\n","J, and theday, and the way for her for you myENIUS:\n","it,\n","And good I would not be not do\n","\n","RONTES:\n","\n","BRLO:\n","pELLA:\n","To no VINCENTIO:\n","To his never;\n","If their death, I should a lord,\n","SERC\n","HPET:\n","To not in death;\n","Wan me;\n","And,\n","O, my lord,\n","' my heads from thy king,\n","\n","And\n","For our lord,\n","\n","HEN hast at the other a head, and thou shall have his good king of death,\n","I am my head, thou never\n","Ho,\n","Of old is no father, to the king,\n","\n","Pray my king, he\n","Weer I do any say; a world,\n","To a that most head:\n","WYENIUS:\n","\n","Come,\n","I am this love.\n","I'll to death with him.\n","\n","To,\n","The present;\n","The husband,\n","And thou thou honour.\n","\n","That do\n","\n","One to your husband:\n","Where, you, you good is be in the land:\n","Wecere but I'll will that Edward\n","---------------\n","\n"," my love the war.\n","\n","WYre the Lord,\n","\n","Nurse:\n","Oitherf,\n","Of they would make this hands\n","Wo he is my lord,\n","Necou is that not;\n","\n","Adnd his\n","\n","Shall made his\n","NirstUCESTER:\n","And, and good have my lord,\n","I have have't, by him\n","\n","With'll heard him, the mother, I will I am a hand.\n","\n","\n","\n","\n","And a lord,\n","\n","And and him\n","And\n","I'll see my lord,\n","\n","I am\n","\n","\n","H my day, every'll so should is, and I do it.\n","And not we'll I is both?\n","WondONTES:\n","het once, I did,\n","Nr is for that I am go,\n","What, at to that know you,\n","\n","That'll eyes:\n","With my\n","The never all it so, I do in you say\n","As I will had\n","Oirst III:\n","And I'll be so.\n","Thou am this,\n","My am not not,\n","And that have,\n","\n","With all these OFELLA:\n","POM his heart.\n","\n","As her?\n","And my lord is a world;\n","\n","SUEENTER:\n","Er follow, for my people:\n","\n","He,\n","To the lords, my lord,\n","\n","Sor did his own--\n","HUCHWICK:\n","Of thou how have do some mother:\n","That thou is your thousand,\n","\n","Fnd I have so, I did, his world, and ever love.\n","You would I know you,\n","COM her day,\n","\n","\n","\n","\n","\n","To thy name, pardon\n","The hour, this, you would is his earth,\n","If a city:\n","HULIou would the king,\n","\n","\n","And a LAURENCE:\n","\n","\n","\n","They,\n","Now,\n","And be so,\n","\n","\n","Of you I am words- life,\n","Which, no more.\n","With take my lord,\n","So 'ec many to your thou have he'll been shall the brother,\n","\n","\n","\n","\n","\n","P am the prince, and be not he;\n","The brother; if.\n","P EDWARD:\n","\n","\n","And thee\n","Why\n","---------------\n","\n"," my love the war!\n","\n","This lord?\n","What,\n","\n","I do for this up;\n","\n","And I will I'll, but,\n","Of one,\n","In the lord.\n","\n","T the VI:\n","For all been thou, to myself some less his so, all;\n","As 'ouom a\n","\n","In I, and the head!\n","\n","The\n","If his lord, and heaven!\n","That his son:\n","I, she too?\n","\n","WARET:\n","To a be the husband.\n","And know still, sir, and\n","\n","\n","FirstENIUS:\n","So in the king, O, I cannot\n","\n","I am all that can call,\n","May, that in a state,\n","Mho,\n","To hear the wind,\n","\n","\n","\n","If I have, to the wife,\n","Will 'ou am a place, and him in the head of my lord, to what speak, thou'll let you would so hath own BOLINGBROKE:\n","No?\n","Mf thy lord,\n","\n","\n","LOLI RICHARDTER:\n","\n","NAMIL ELIZABETH:\n","Of his what would well, I do not it, to her, come,\n","PRIAR should be had yet shall mine, sir, the stay, there,\n","And do they have you is,\n","\n","\n","Rs him;\n","JULITER:\n","\n","\n","\n","I do to make, and thee,\n","NOR YORK:\n","And he is I is it of the breast,\n","\n","\n","And and made and speak,\n","That an rest, sir, I will be the lord, my rest,\n","FroTER:\n","\n","I art the rest, your deed,\n","To when we do now,\n","Well,\n","Tf you?\n","ES' may;\n","As as to the husband,\n","And,\n","As one on the son,\n","O, and did by it had I have but in hisRY?\n","Sot ourmorrow,\n","I'll I'll thy lord, I are like me,\n","Sad he'll, but a husband!\n","And and but I am thou; the Citizen:\n","Hho, I do not I I did to such my father, I\n","O,\n","Nhich will now, boy,\n","And do in with the lord, and- VI:\n","F\n","---------------\n","\n"," my love the war in\n","\n","\n","\n","\n","\n","FRIEEN,\n","\n","\n","FCo am will be,\n","MEN lord:\n","Is the head, your mother, and not\n","\n","And thou not not, and I would our world, I will follow\n","To as the queen,\n","\n","\n","MThe highness, and tell how do,\n","\n","\n","LAR EDWARDUCES:\n","\n","JOMAR lord,\n","Ss a VI:\n","\n","\n","O;\n","DORTER:\n","\n","AUMNIA: or,\n","The art his\n","\n","I will by she;\n","\n","UUCHUS:\n","R RICHARDIOLAN:\n","And no more\n","And, it is not be as your noble man, my ' should give'ho, and see's have me, but your lord, if you is thy lord, I say, and I'll man,\n","The king, my lord!\n","I'll,\n","Now am we know you were which do this, and bear, what and I may not the cause in can I'll more.\n","And thou is not thus?\n","For bear he had,\n","Hre his be the head'd,\n","O,\n","I'll such as bear you shall I will take the brother,\n","\n","What shall do his and\n","To a lord!\n","\n","Jor yet?\n","That hope,t\n","\n","SORD:\n","\n","The\n","Rr,\n","\n","And there.\n","Jond first thing, If me.\n","\n","\n","Wer you is not of the day of the lord, but I are have die:\n","If my queen,\n","\n","ANr well:\n","\n","But is in the honour\n","\n","MARESS:\n","CAs I thy do,\n","And I do me from the lord,\n","You, but if she I as a grace;\n","And I, and him\n","S EDWARD:\n","\n","\n","Why,\n","We is it,\n","CErom yourELLA:\n","\n","\n","\n","By all, for the enemy\n","\n","\n","\n","And thee; stay, and if the Duke;\n","Mnd let thee I'll a lord then is a name;\n","As and we, for be now;\n","As not's a be have heart!\n","O,\n","And\n","And I am an soul:\n","And good, and as my YORK:\n","A\n","---------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9Sc4Kgo7wHEL"},"execution_count":null,"outputs":[]}]}