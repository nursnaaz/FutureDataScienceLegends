{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPccNL4MDlq7"
   },
   "source": [
    "### ⚠ IMPORTANT ⚠\n",
    "\n",
    "Please ensure your Colab runtime is set to the following:\n",
    "\n",
    "A100 GPU\n",
    "\n",
    "Evaluation and instruction-tuning a LLM is a resource intensive process - please make sure you're using the appropriate instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6TK3hY9Xs5Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKSx2zaqB-QF"
   },
   "source": [
    "# Model Evaluation: A Primer\n",
    "\n",
    "Now that we've spent some time creating models with:\n",
    "\n",
    "- Unsupervised pre-training -> nanoGPT -> Shapereare\n",
    "- Supervised fine-tuning -> gpt2 -> b-mc2/sql-create-context -> Understand and write sql query\n",
    "- Some instruction-tuning -> Mistral.v01 -> mosaicml/instruct-v3 -> Thought how to respond to the instruction\n",
    "\n",
    "We're ready to begin to think about how we can evaluate these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acilXetkCTep"
   },
   "source": [
    "## Baseline Evaluation\n",
    "\n",
    "In order to properly understand how our model is improving - we need to first start with a baseline evaluation of our model's performance.\n",
    "\n",
    "Let's start with Mistral AI's `Mistral-7B` model.\n",
    "\n",
    "We're going to load and compare everything in 4-bit quantization today in order to ensure we can fit the model on our Google Colab instance.\n",
    "\n",
    "Let's start by setting up and loading our model to prepare it for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V-U6VaGDQJ_"
   },
   "source": [
    "### Load Mistral AI's Mistral-7B in 4-bit Quantization\n",
    "\n",
    "Let's grab our dependencies, and load our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85948,
     "status": "ok",
     "timestamp": 1754794248551,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "hLLkiP7pB42M",
    "outputId": "beccc84d-ab25-402e-f9fd-dce1bb43e91b"
   },
   "outputs": [],
   "source": [
    "!pip install -qU bitsandbytes datasets accelerate loralib peft transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12251,
     "status": "ok",
     "timestamp": 1754794291646,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "nPbEpaYQDZ45"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HkWkD64DcIX"
   },
   "source": [
    "Conforming to previous notebooks - let's set up our quantization config for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754794293637,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "8wQcWs3uDae0"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2niGU5gDD3E4"
   },
   "source": [
    "Now we have our quantization settings confirmed - let's load up our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "35e63563a05544308a311ea676632ca8",
      "d55d4a15c02c4263907db1fa2508990b",
      "15d6afb3d08e46b4abbbfaa671563656",
      "5290d4147e2e48cf86dac8c173403bb2",
      "b2e518a17e3942e0b11c8a13ddcbfffe",
      "aca15a64df084b999b512c58b142cf2b",
      "479975a2e7a04f7993ab3ea4b5190ddf",
      "7dc2f41652644eb3b4b8c225e16dfe1c",
      "fc315565ec5145b9bc58437c2ee84c20",
      "8068ba6e01964419a1f07584b89aec91",
      "e668e0434b654c738557474bb3c172dc",
      "8ddfbe7bdb73471a9f4df60c32ab2175",
      "53b64d9e9e1b41619e2784130c722244",
      "7acc85dea0924143abad1834f2cd2fd9",
      "fd6291ff611044ca936ede8aa6f4f4f4",
      "8539b46fc9bd4a3da7e1d743b698c080",
      "e0184235710b43ff84da0bc29337e1c4",
      "94ce0969c72441e89388ca7533561ee9",
      "2e8bbc30791d492c8a882414937b7f49",
      "a6add0721ddd47a481d0d842d9206571"
     ]
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1754794350473,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "JvP3p2IB5M5h",
    "outputId": "f2a198e8-dc96-4148-c576-4be415e184e0"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "8248335cf207463888c62553e8f0d0fa",
      "c8141dfae2f84dbf9daa94ccd82d6908",
      "cdf8737eb6dc4835842773b19cf9a530",
      "6c6cfe0ce44e40e2a859870c7537d800",
      "638132c0450846508563cae91309b2d9",
      "3b2d24b5120f42f8bafe5a074e63b673",
      "251c699ecf8f478db01c61c72fe5e10d",
      "65e54c4f30274ac3b48e3ea63aac9fbb",
      "4a1861c770fd46efb5063a21aa2e82b2",
      "0ca0c5c1e6f4445b86add68648c10249",
      "324c45f181aa4189aa75fe34fe0546cc",
      "1d372133dfa24a44a15687c411773295",
      "5513d69c4c214453bd7bd8cf41ffd54c",
      "b4d7e65dfd0a4a8aa0ba581d7d532c15",
      "dee3a289cfee4959995b0441418616bd",
      "d190b1c21d7b45b3af1f178d0b9b79b0",
      "436d971cbc434a7da8c205872849833f",
      "5a82120fab084bb4a41bb118c8ba7d8d",
      "5693d9778e90491d9818135649738d24",
      "c74e1297e2744d549beb9a885e693bbd",
      "3fafbc356aa84201a33887da284029c1",
      "3437ed321d8c43e791b339022c95d7ad",
      "5a6e40fe504044dc91bfe1ea643b1f97",
      "5db3b6fd2dfb42f89c8652243e125a0f",
      "7a162f799d4e40b590531fa26a54afcf",
      "6253d2e16f704f5e888c7bd56f966499",
      "00ff32742e9d4a34af8d670492ed5278",
      "39762841b2a74b079abaebc00daa4d95",
      "7d54d12bf9b64433aed49cbc96dbc1b4",
      "1ba11fd4780945848d0caab825b86a77",
      "3fc166a53698492fbcbf25d16638fbdb",
      "1a5781d1f3e14e42b643f7a53328e3ed",
      "62d0331f49174e388b4ea05b152d0f4c",
      "8ac70fe847b644e1b9596fa3113e8f0a",
      "76edfc5f06134a01b61d24a8bc77d3e5",
      "d971c61233f4439c97ba697b16864799",
      "420900d329984ab9a238bd0cf448aba5",
      "b149bd8b1a9e493ca32b4e91a788609e",
      "b72e38c05a924e9b9a2d371ef13e9348",
      "ea3889fca5814a3fbbe9c180836ad219",
      "f145cf78fe05452d98f2637934bbe70e",
      "b3865fa7fbee4e789b167665ba36be5e",
      "149bf3cfaef344c29fbd72138a4e88f8",
      "c15d919bee34498d9fcbea7202f084a9",
      "75bc9f17cc87465096e93e5bb9fc62b7",
      "a2d2fa47b3f347ec9eaae3e25fe3db44",
      "4858d4ea2f6b409f918f4f192e39fa9f",
      "26b03322364a4e9fb0b05f87044236c8",
      "7a9d95c4d21c4449bbf742a7b9371052",
      "82bf0df90bf14391b7fb3c4d10e68d55",
      "e51e0ea22d7f40c4a4e1f7b9033e4a94",
      "7d29819fb6934be795c12d4c71ee18a9",
      "72edb74dd07f47e58f2e817925bc4095",
      "4fb65db640c140b4bb90da90e0b3defd",
      "e246e5ebbab143bfa09c878fa7937c3e",
      "b5fd476b19374b0287283fd0f2250409",
      "9ba795bec7b64a8999b1b3e9675d1af6",
      "bce45d3f0fbb418aacf8c2d58ae42999",
      "f9906d550de048499108141695eb963d",
      "83492b97706d4474917642640e4e0c48",
      "d6c1e836732341a3a2fff3437c73416a",
      "e633041d901e4a48a59fcd83a674f8b0",
      "079d73efde5a4411a2e0b21a37d6608a",
      "be8c4a45f70b488eb82c0b00933610f6",
      "5d2abeb422d24667aa66fe02c3e05b92",
      "80f4d51c433343f28e1609aa2af2f2ce",
      "2cdf143f3c9843e99f652d89dd313ae6",
      "a82f9ffeb3514a67832a4a786f6a2330",
      "bed901907ac846058a352cff740855a1",
      "914ceed2fed549e19fbd45c4c6af8e54",
      "49466938d1ee426dbb2eef788d4142bb",
      "59eb80650f8c476b9b6ebc562200a0e2",
      "dcacaa7c309a44d296a8ee7ad02e5753",
      "cfbabb6c3f3e4c258ca821dd894b7786",
      "8c69a777a9c246c0b75aa2e1e718156e",
      "eefccd01ff804ecdb5aa226de1f59618",
      "35efc69ed6c547328a940375f2d8b649",
      "de9f1c3f40284f58b5006091b7b22ed9",
      "c2122c38630740ba99a347b94b94cf7e",
      "f9ae15bc2e514d1aa6913e458544cddc",
      "529dab47325249a78312da524a69dea5",
      "57e7ec5e20f44306bcfbb360f56af47c",
      "2421348ac04e4b998839a8bdd1a96731",
      "906fa417520d45d58a89bb4677e3497f",
      "6ab4e6d53dfe44868370c0d5c87b40e2",
      "23c28adcdd3f44f08caa0302a44001fb",
      "7591c3248b904a41b7840773b2a9d31a",
      "c1c37a3df34b455cb9d20e7ea6ac50c3",
      "918f3f4580c34c219184fafd80b94b6c",
      "0668a258333a4f21b68f65e8d58954bc",
      "422a9e7161484b9d8bc7f2b57e6a0876",
      "260bb46b8811464d91e28339b90d2380",
      "158ede0889b942f68de96c8add13e41e",
      "2971d5b6d133457fad12962531c04b59",
      "de696c22eac946c2a6f601e70962e2cf",
      "a607ffce8e1045ac87de5431c68e6acb",
      "e3cdd74525e1453b949bbbea3b2d5af2",
      "e0d788f95d8d4e6dbac881ca7b800789",
      "a129f761fd584ad7be5bd61fc1b4bc80",
      "da232a69252f40fca4b98788b2d137af",
      "e26ad5d7da804e58ac7e2352003151b6",
      "b35624bd57984656b2f5fa34b0fa021c",
      "7a45601b331c44b089c37c85764a4fdd",
      "64b975f667fd45a29a66d38a56071536",
      "84050f1f8eac498e9a24fedbc1d64b37",
      "984d9cf97bc64317a3a276bf07779ed0",
      "7b54619078c5410984b39f0dacd4bd0a",
      "a1f1a5162e214f2d8b84c5ca95534419",
      "ad62ca5cc1304934acb366b8b9ff18e7",
      "072336b19c584ef7bfcbd75d4176afcb",
      "9d8841878dbd42838107cbcb7ea6270f",
      "74c992065b9f4c23ab05c081c5b6282a",
      "e801daf87b8e4309aa9ad504df0e9642",
      "b9c110cdb6cd413399b34f31315e753e",
      "8b64ac113b7b431bbebede8ac8351ab1",
      "5147f6ce597a42b086ac7dedb8f893e8",
      "5fb358f9684f4f54b70360129708f9cc",
      "7d73a07fb1c743a785df5442020bf479",
      "fe4444412b1f4f11826191ac536a2285",
      "c2623e322c7347678c59c099c4c91481",
      "609eb294bb7a44898cfd856fa431bf30"
     ]
    },
    "executionInfo": {
     "elapsed": 99872,
     "status": "ok",
     "timestamp": 1754794566186,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "e41ggQCpDwYe",
    "outputId": "97eb8792-ad5a-4c11-c83e-6c995e867bfb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2ItP0tAe44d"
   },
   "outputs": [],
   "source": [
    "#With Quantization\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1754794622097,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "9gduMBJPfLS0",
    "outputId": "cc42e4ca-739d-4129-81f0-23e893855002"
   },
   "outputs": [],
   "source": [
    "#Without  Quantization\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz2eVTU4fcrR"
   },
   "outputs": [],
   "source": [
    "#Without Quantization GPU memory\n",
    "#29073/1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWuDq8y2fgH_"
   },
   "outputs": [],
   "source": [
    "#With Quantization GPU memory\n",
    "#5441/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQcR0wr1cqRd"
   },
   "outputs": [],
   "source": [
    "# With Quantization\n",
    "# CPU times: user 37.1 s, sys: 29.1 s, total: 1min 6s\n",
    "# Wall time: 1min 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWGDnI5yEP7G"
   },
   "source": [
    "####❓ Question:\n",
    "\n",
    "Taking a look at the [model card](https://huggingface.co/mistralai/Mistral-7B-v0.1) (and the linked resources on the card) is this an instruct-tuned model or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqfBj0tcHSa3"
   },
   "source": [
    "### Collect and Load the Eleuther AI Evaluation Harness\n",
    "\n",
    "Now that we have our baseline model loaded - we need to evaluate it.\n",
    "\n",
    "For that, we'll use a tool called [Eleuther AI's LM evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness). This is a specialized tool for running benchmarks on various language tasks.\n",
    "\n",
    "Let's start by grabbing and installing it!\n",
    "\n",
    "Why Eleuther AI's Evaluation Harness? Well - it's what powers the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21603,
     "status": "ok",
     "timestamp": 1754794674058,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "CZMp4-_rIrit",
    "outputId": "b39abe27-e660-4572-956f-72693cb73cc5"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "%cd lm-evaluation-harness\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96Jw_Z_zMOgd"
   },
   "source": [
    "Now, we can cast our model to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1616,
     "status": "ok",
     "timestamp": 1754794680870,
     "user": {
      "displayName": "Mohamed Noordeen Alaudeen",
      "userId": "13097854355562551879"
     },
     "user_tz": -240
    },
    "id": "NUGozWKFIvrL",
    "outputId": "2ac6b5a6-c9df-48d4-acad-25a66175e327"
   },
   "outputs": [],
   "source": [
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "eval_model = HFLM(model, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mkr352OMTG-"
   },
   "source": [
    "We'll set up our tasks so we can leverage them at evaluation time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNuXARDAMXOv"
   },
   "source": [
    "Next, we can evaluate our base model!\n",
    "\n",
    ">NOTE: This step will take ~30-40min. to run in full on the A100 - so ensure you set aside time to run it fully if you desire!\n",
    "\n",
    "We're going to leverage two benchmarks today:\n",
    "\n",
    "- [HellaSwag](https://rowanzellers.com/hellaswag/)\n",
    "- [ARC Easy](https://leaderboard.allenai.org/arc_easy/submissions/get-started)\n",
    "- A subset of the [MMLU benchmark](https://paperswithcode.com/dataset/mmlu), focusing only on the `machine_learning` task.\n",
    "\n",
    "These are lightweight benchmarks used to \"score\" models against eachother on the OpenLM leaderboard.\n",
    "\n",
    "We'll consider a simple average of their scores as the \"overall\" score of the baseline model.\n",
    "\n",
    "You could easily extend the number of tasks considered if you wanted to more exactly emulate the Open LLM Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY4cmJuqKjgf"
   },
   "outputs": [],
   "source": [
    "results = lm_eval.simple_evaluate(\n",
    "    model=eval_model,\n",
    "    tasks=[\"hellaswag\", \"arc_easy\"],\n",
    "    num_fewshot=0,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Conu8kZsQ6Es",
    "outputId": "43ee551a-d34c-48ac-9ad0-bc6d46930f26"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "4d5ff2b550794cf3a35fbe8a9c474c7e",
      "6e509f22fcec420a90b4eb8237bb7c77",
      "82256bc1cca6428f859710d5871f0dc5",
      "95d32ca20bad4d04aeecfc6e17d9ce16",
      "588e319107284bc7b7a59506576c7036",
      "254f99debb814c50addc808695c9cbf2",
      "3094215fc934420a921177f3c49ee50e",
      "726f5c6020ce46d3a151bc59ab296ff6",
      "5bc4907761044dc0b984aebbaa7e3738",
      "91533e217b734a53a0b3af1cffc95e3a",
      "d1fb1573d5f4487ab2506fdaadd1f730",
      "e1cf9dfd366c496fa4e3ad23637837dd",
      "8b4db95613b34ee1afbf8a3b66dc74fe",
      "55795f772bc94542b162de7f34a5dcf6",
      "e73783d15c3d43ccac8a9d98b87d9bf7",
      "4def00d80e984a2099c8b97db8ecb1b7",
      "c74e842817ed4c2f9c48205233c5c5d2",
      "f64da27d171548d18ccfec5e8b90f227",
      "a487e41551a14155a56165ea90006d8d",
      "feb45ae88ef241dcb7affce871d9a6e6",
      "ca10f11732d94b668e0e4a5f997014f6",
      "d73714576fdd4792ab92ca1895454052",
      "54801506f72b44429cb5f73176a62417",
      "1820d608302b4152bbb3adec001f6d22",
      "3d9bdb136f824ee494506f6da69a5bb8",
      "8a7ca6b5b3da4dafad05058f8e11ab30",
      "4072b42913844815a3d1942edd84f279",
      "7e1ba305bfd440dfaf6511930ff87753",
      "a236bde40bcf453d9b409fc1c0ae1604",
      "75a3b19954d14e49a3c7e00e20754a8c",
      "791f26c5f49043efb2cdff96bd2a77a2",
      "71675c34a30b4b2fbfffbbf903dc9b0f",
      "3ccc56a7bc5a4930bf67ccad57b0a290"
     ]
    },
    "id": "jWiHpPLSS4V_",
    "outputId": "cb6e4cae-2f88-42b3-d64c-2209b76464f2"
   },
   "outputs": [],
   "source": [
    "fs_mmlu_results = lm_eval.simple_evaluate(\n",
    "    model=eval_model,\n",
    "    tasks=[\"mmlu_flan_n_shot_loglikelihood_machine_learning\"],\n",
    "    num_fewshot=5,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Y1NwvNZlToFF",
    "outputId": "57a30482-73f5-43bc-a8af-ad0b90b1e104"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(fs_mmlu_results[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_-JY2xdvtuR"
   },
   "source": [
    "### Zero-Shot MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwbJvwz1dT-w",
    "outputId": "a55fee82-eef4-4e51-a1eb-d5fb316558a5"
   },
   "outputs": [],
   "source": [
    "zs_mmlu_results = lm_eval.simple_evaluate(\n",
    "    model=eval_model,\n",
    "    tasks=[\"mmlu_flan_n_shot_loglikelihood_machine_learning\"],\n",
    "    num_fewshot=0,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-nJQZcxvdWHF",
    "outputId": "41a9ab97-1497-47e6-98a8-a448db476921"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(zs_mmlu_results[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLnF58BJvpQe"
   },
   "source": [
    "### Chain of Thought\n",
    "\n",
    "Now let's try a Chain of Thought example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74VhIi79TlD_",
    "outputId": "8af405f7-2314-43eb-bbfd-ab40e91f3699"
   },
   "outputs": [],
   "source": [
    "cot_mmlu_results = lm_eval.simple_evaluate(\n",
    "    model=eval_model,\n",
    "    tasks=[\"mmlu_flan_cot_zeroshot_machine_learning\"],\n",
    "    num_fewshot=0,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "hDD7Av-BTqy6",
    "outputId": "7e51f6a8-3c56-4513-f0ce-aa5ce2d49860"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(cot_mmlu_results[\"results\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
