{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sDOTIJUf09Y",
        "outputId": "4b94328e-3b66-4bdf-ee2f-e9f3428a181b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy 1-Hour Complete Tutorial\n",
            "========================================\n",
            "✅ spaCy model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# spaCy Complete Tutorial - 1 Hour Crash Course\n",
        "# ===============================================\n",
        "\n",
        "# Installation (run this in terminal first):\n",
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "# python -m spacy download en_core_web_md  # Optional: for word vectors\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"spaCy 1-Hour Complete Tutorial\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"✅ spaCy model loaded successfully!\")\n",
        "except OSError:\n",
        "    print(\"❌ Please install the English model: python -m spacy download en_core_web_sm\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 1: BASICS & DOCUMENT PROCESSING (8 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n1. BASICS & DOCUMENT PROCESSING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Creating spaCy documents\n",
        "text1 = \"Hello world! This is a simple example.\"\n",
        "doc1 = nlp(text1)\n",
        "\n",
        "text2 = \"\"\"Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\n",
        "Tim Cook is the CEO. The company was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976. \"\"\"\n",
        "doc2 = nlp(text2)\n",
        "\n",
        "text3 = \"\"\"He was running over the top of the hill and sits on a road for eating his breakfast \"\"\"\n",
        "doc3 = nlp(text3)\n",
        "\n",
        "print(f\"Original text: {text1}\")\n",
        "print(f\"spaCy Doc object: {doc1}\")\n",
        "print(f\"Type: {type(doc1)}\")\n",
        "\n",
        "# Document properties\n",
        "print(f\"\\nDocument properties:\")\n",
        "print(f\"Length: {len(doc2)} tokens\")\n",
        "print(f\"Text: {doc2.text[:100]}...\")\n",
        "print(f\"Language: {doc2.lang_}\")\n",
        "\n",
        "# Iterating through tokens\n",
        "print(f\"\\nFirst 10 tokens:\")\n",
        "for i, token in enumerate(doc2):\n",
        "    print(f\"  {i+1:2d}: '{token.text}' (pos: {token.pos_}, lemma: {token.lemma_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZcBc7znf4Xk",
        "outputId": "f2cd6d46-b8a7-47bb-cf9a-50f9aedd9ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. BASICS & DOCUMENT PROCESSING\n",
            "-----------------------------------\n",
            "Original text: Hello world! This is a simple example.\n",
            "spaCy Doc object: Hello world! This is a simple example.\n",
            "Type: <class 'spacy.tokens.doc.Doc'>\n",
            "\n",
            "Document properties:\n",
            "Length: 38 tokens\n",
            "Text: Apple Inc. is an American multinational technology company headquartered in Cupertino, California. \n",
            "...\n",
            "Language: en\n",
            "\n",
            "First 10 tokens:\n",
            "   1: 'Apple' (pos: PROPN, lemma: Apple)\n",
            "   2: 'Inc.' (pos: PROPN, lemma: Inc.)\n",
            "   3: 'is' (pos: AUX, lemma: be)\n",
            "   4: 'an' (pos: DET, lemma: an)\n",
            "   5: 'American' (pos: ADJ, lemma: american)\n",
            "   6: 'multinational' (pos: ADJ, lemma: multinational)\n",
            "   7: 'technology' (pos: NOUN, lemma: technology)\n",
            "   8: 'company' (pos: NOUN, lemma: company)\n",
            "   9: 'headquartered' (pos: VERB, lemma: headquarter)\n",
            "  10: 'in' (pos: ADP, lemma: in)\n",
            "  11: 'Cupertino' (pos: PROPN, lemma: Cupertino)\n",
            "  12: ',' (pos: PUNCT, lemma: ,)\n",
            "  13: 'California' (pos: PROPN, lemma: California)\n",
            "  14: '.' (pos: PUNCT, lemma: .)\n",
            "  15: '\n",
            "' (pos: SPACE, lemma: \n",
            ")\n",
            "  16: 'Tim' (pos: PROPN, lemma: Tim)\n",
            "  17: 'Cook' (pos: PROPN, lemma: Cook)\n",
            "  18: 'is' (pos: AUX, lemma: be)\n",
            "  19: 'the' (pos: DET, lemma: the)\n",
            "  20: 'CEO' (pos: PROPN, lemma: CEO)\n",
            "  21: '.' (pos: PUNCT, lemma: .)\n",
            "  22: 'The' (pos: DET, lemma: the)\n",
            "  23: 'company' (pos: NOUN, lemma: company)\n",
            "  24: 'was' (pos: AUX, lemma: be)\n",
            "  25: 'founded' (pos: VERB, lemma: found)\n",
            "  26: 'by' (pos: ADP, lemma: by)\n",
            "  27: 'Steve' (pos: PROPN, lemma: Steve)\n",
            "  28: 'Jobs' (pos: PROPN, lemma: Jobs)\n",
            "  29: ',' (pos: PUNCT, lemma: ,)\n",
            "  30: 'Steve' (pos: PROPN, lemma: Steve)\n",
            "  31: 'Wozniak' (pos: PROPN, lemma: Wozniak)\n",
            "  32: ',' (pos: PUNCT, lemma: ,)\n",
            "  33: 'and' (pos: CCONJ, lemma: and)\n",
            "  34: 'Ronald' (pos: PROPN, lemma: Ronald)\n",
            "  35: 'Wayne' (pos: PROPN, lemma: Wayne)\n",
            "  36: 'in' (pos: ADP, lemma: in)\n",
            "  37: '1976' (pos: NUM, lemma: 1976)\n",
            "  38: '.' (pos: PUNCT, lemma: .)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token in enumerate(doc3):\n",
        "  print(i, token, \" - \",token.lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVDTlfHKhzqA",
        "outputId": "2615ecc2-fa3c-4539-98bf-557d68e124f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 He  -  he\n",
            "1 was  -  be\n",
            "2 running  -  run\n",
            "3 over  -  over\n",
            "4 the  -  the\n",
            "5 top  -  top\n",
            "6 of  -  of\n",
            "7 the  -  the\n",
            "8 hill  -  hill\n",
            "9 and  -  and\n",
            "10 sits  -  sit\n",
            "11 on  -  on\n",
            "12 a  -  a\n",
            "13 road  -  road\n",
            "14 for  -  for\n",
            "15 eating  -  eat\n",
            "16 his  -  his\n",
            "17 breakfast  -  breakfast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: TOKENIZATION & TOKEN ATTRIBUTES (10 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n2. TOKENIZATION & TOKEN ATTRIBUTES\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog! It's running at 25.5 mph.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"\\nDetailed Token Analysis:\")\n",
        "print(f\"{'Token':<12} {'POS':<8} {'Tag':<6} {'Explanation on Tag': <15} {'Lemma':<30} {'Shape':<8} {'Alpha':<5} {'Stop':<4}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<8} {token.tag_:<6} {spacy.explain(token.tag_)} {token.lemma_:<12} \"\n",
        "        f\"{token.shape_:<8} {token.is_alpha:<5} {token.is_stop}\")\n",
        "\n",
        "\n",
        "\n",
        "# Token properties explanation\n",
        "print(f\"\\nToken Properties Explained:\")\n",
        "properties = [\n",
        "    (\"text\", \"Raw token text\"),\n",
        "    (\"pos_\", \"Part-of-speech tag (simplified)\"),\n",
        "    (\"tag_\", \"Detailed part-of-speech tag\"),\n",
        "    (\"lemma_\", \"Base form of the token\"),\n",
        "    (\"shape_\", \"Word shape (Xxxx, dddd, etc.)\"),\n",
        "    (\"is_alpha\", \"Is the token alphabetic?\"),\n",
        "    (\"is_stop\", \"Is the token a stop word?\"),\n",
        "    (\"is_punct\", \"Is the token punctuation?\"),\n",
        "    (\"is_digit\", \"Is the token a digit?\"),\n",
        "    (\"like_num\", \"Does the token resemble a number?\")\n",
        "]\n",
        "\n",
        "for prop, desc in properties:\n",
        "    print(f\"  {prop:<12}: {desc}\")\n",
        "\n",
        "# Advanced token filtering\n",
        "print(f\"\\nFiltered Tokens:\")\n",
        "# Non-stop, alphabetic words\n",
        "meaningful_tokens = [token.lemma_.lower() for token in doc\n",
        "                    if not token.is_stop and token.is_alpha and len(token.text) > 2]\n",
        "print(f\"Meaningful words: {meaningful_tokens}\")\n",
        "\n",
        "# Numbers and quantities\n",
        "numbers = [token.text for token in doc if token.like_num]\n",
        "print(f\"Numbers found: {numbers}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpYJxP1fk6X0",
        "outputId": "392584c0-c491-4e63-a580-2388a5fd0fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "2. TOKENIZATION & TOKEN ATTRIBUTES\n",
            "-----------------------------------\n",
            "Text: The quick brown fox jumps over the lazy dog! It's running at 25.5 mph.\n",
            "\n",
            "Detailed Token Analysis:\n",
            "Token        POS      Tag    Explanation on Tag Lemma                          Shape    Alpha Stop\n",
            "----------------------------------------------------------------------\n",
            "The          DET      DT     determiner the          Xxx      1     True\n",
            "quick        ADJ      JJ     adjective (English), other noun-modifier (Chinese) quick        xxxx     1     False\n",
            "brown        ADJ      JJ     adjective (English), other noun-modifier (Chinese) brown        xxxx     1     False\n",
            "fox          NOUN     NN     noun, singular or mass fox          xxx      1     False\n",
            "jumps        VERB     VBZ    verb, 3rd person singular present jump         xxxx     1     False\n",
            "over         ADP      IN     conjunction, subordinating or preposition over         xxxx     1     True\n",
            "the          DET      DT     determiner the          xxx      1     True\n",
            "lazy         ADJ      JJ     adjective (English), other noun-modifier (Chinese) lazy         xxxx     1     False\n",
            "dog          NOUN     NN     noun, singular or mass dog          xxx      1     False\n",
            "!            PUNCT    .      punctuation mark, sentence closer !            !        0     False\n",
            "It           PRON     PRP    pronoun, personal it           Xx       1     True\n",
            "'s           AUX      VBZ    verb, 3rd person singular present be           'x       0     True\n",
            "running      VERB     VBG    verb, gerund or present participle run          xxxx     1     False\n",
            "at           ADP      IN     conjunction, subordinating or preposition at           xx       1     True\n",
            "25.5         NUM      CD     cardinal number 25.5         dd.d     0     False\n",
            "mph          NOUN     NN     noun, singular or mass mph          xxx      1     False\n",
            ".            PUNCT    .      punctuation mark, sentence closer .            .        0     False\n",
            "\n",
            "Token Properties Explained:\n",
            "  text        : Raw token text\n",
            "  pos_        : Part-of-speech tag (simplified)\n",
            "  tag_        : Detailed part-of-speech tag\n",
            "  lemma_      : Base form of the token\n",
            "  shape_      : Word shape (Xxxx, dddd, etc.)\n",
            "  is_alpha    : Is the token alphabetic?\n",
            "  is_stop     : Is the token a stop word?\n",
            "  is_punct    : Is the token punctuation?\n",
            "  is_digit    : Is the token a digit?\n",
            "  like_num    : Does the token resemble a number?\n",
            "\n",
            "Filtered Tokens:\n",
            "Meaningful words: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'run', 'mph']\n",
            "Numbers found: ['25.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: PART-OF-SPEECH TAGGING & DEPENDENCY PARSING (12 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n3. POS TAGGING & DEPENDENCY PARSING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "sentence = \"The beautiful sunset painted the sky with vibrant colors yesterday evening.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(f\"Sentence: {sentence}\")\n",
        "\n",
        "# Part-of-speech tagging\n",
        "print(f\"\\nPart-of-Speech Analysis:\")\n",
        "print(f\"{'Token':<12} {'POS':<8} {'Tag':<6} {'Description':<20}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "pos_descriptions = {\n",
        "    'DET': 'Determiner', 'ADJ': 'Adjective', 'NOUN': 'Noun', 'VERB': 'Verb',\n",
        "    'ADP': 'Adposition', 'ADV': 'Adverb', 'PROPN': 'Proper Noun'\n",
        "}\n",
        "\n",
        "for token in doc:\n",
        "    desc = pos_descriptions.get(token.pos_, token.pos_)\n",
        "    print(f\"{token.text:<12} {token.pos_:<8} {token.tag_:<6} {desc:<20}\")\n",
        "\n",
        "# Dependency parsing\n",
        "print(f\"\\nDependency Relations:\")\n",
        "print(f\"{'Token':<12} {'Relation':<12} {'Head':<12} {'Children'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for token in doc:\n",
        "    children = [child.text for child in token.children]\n",
        "    children_str = \", \".join(children) if children else \"None\"\n",
        "    print(f\"{token.text:<12} {token.dep_:<12} {token.head.text:<12} {children_str}\")\n",
        "\n",
        "# Visualizing dependencies (text-based)\n",
        "print(f\"\\nDependency Tree (simplified):\")\n",
        "def print_dependencies(token, depth=0):\n",
        "    print(\"  \" * depth + f\"├─ {token.text} ({token.dep_})\")\n",
        "    for child in token.children:\n",
        "        print_dependencies(child, depth + 1)\n",
        "\n",
        "# Find the root token\n",
        "root = [token for token in doc if token.dep_ == \"ROOT\"][0]\n",
        "print_dependencies(root)\n",
        "\n",
        "# Extract specific grammatical patterns\n",
        "print(f\"\\nExtracted Patterns:\")\n",
        "subjects = [token.text for token in doc if token.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
        "objects = [token.text for token in doc if token.dep_ in (\"dobj\", \"pobj\")]\n",
        "verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
        "\n",
        "print(f\"Subjects: {subjects}\")\n",
        "print(f\"Objects: {objects}\")\n",
        "print(f\"Verbs: {verbs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfGP-tBHlvuh",
        "outputId": "f5b607b9-f5d3-40ce-a0f2-13928208340c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "3. POS TAGGING & DEPENDENCY PARSING\n",
            "-----------------------------------\n",
            "Sentence: The beautiful sunset painted the sky with vibrant colors yesterday evening.\n",
            "\n",
            "Part-of-Speech Analysis:\n",
            "Token        POS      Tag    Description         \n",
            "--------------------------------------------------\n",
            "The          DET      DT     Determiner          \n",
            "beautiful    ADJ      JJ     Adjective           \n",
            "sunset       NOUN     NN     Noun                \n",
            "painted      VERB     VBD    Verb                \n",
            "the          DET      DT     Determiner          \n",
            "sky          NOUN     NN     Noun                \n",
            "with         ADP      IN     Adposition          \n",
            "vibrant      ADJ      JJ     Adjective           \n",
            "colors       NOUN     NNS    Noun                \n",
            "yesterday    NOUN     NN     Noun                \n",
            "evening      NOUN     NN     Noun                \n",
            ".            PUNCT    .      PUNCT               \n",
            "\n",
            "Dependency Relations:\n",
            "Token        Relation     Head         Children\n",
            "------------------------------------------------------------\n",
            "The          det          sunset       None\n",
            "beautiful    amod         sunset       None\n",
            "sunset       nsubj        painted      The, beautiful\n",
            "painted      ROOT         painted      sunset, sky, with, evening, .\n",
            "the          det          sky          None\n",
            "sky          dobj         painted      the\n",
            "with         prep         painted      colors\n",
            "vibrant      amod         colors       None\n",
            "colors       pobj         with         vibrant\n",
            "yesterday    compound     evening      None\n",
            "evening      npadvmod     painted      yesterday\n",
            ".            punct        painted      None\n",
            "\n",
            "Dependency Tree (simplified):\n",
            "├─ painted (ROOT)\n",
            "  ├─ sunset (nsubj)\n",
            "    ├─ The (det)\n",
            "    ├─ beautiful (amod)\n",
            "  ├─ sky (dobj)\n",
            "    ├─ the (det)\n",
            "  ├─ with (prep)\n",
            "    ├─ colors (pobj)\n",
            "      ├─ vibrant (amod)\n",
            "  ├─ evening (npadvmod)\n",
            "    ├─ yesterday (compound)\n",
            "  ├─ . (punct)\n",
            "\n",
            "Extracted Patterns:\n",
            "Subjects: ['sunset']\n",
            "Objects: ['sky', 'colors']\n",
            "Verbs: ['paint']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: NAMED ENTITY RECOGNITION (10 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n4. NAMED ENTITY RECOGNITION (NER)\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "text = \"\"\"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\n",
        "The company is now worth over $2 trillion dollars. Tim Cook became CEO in 2011.\n",
        "Microsoft, Google, and Amazon are major competitors. The headquarters is located at\n",
        "One Apple Park Way, and they employ over 100,000 people worldwide.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"Text: {text[:100]}...\")\n",
        "\n",
        "# Extract all named entities\n",
        "print(f\"\\nNamed Entities Found:\")\n",
        "print(f\"{'Entity':<20} {'Label':<12} {'Description':<25}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "entity_descriptions = {\n",
        "    'PERSON': 'People, including fictional',\n",
        "    'ORG': 'Companies, agencies, institutions',\n",
        "    'GPE': 'Countries, cities, states',\n",
        "    'MONEY': 'Monetary values',\n",
        "    'DATE': 'Absolute or relative dates',\n",
        "    'CARDINAL': 'Numerals that do not fall under other types',\n",
        "    'ORDINAL': 'First, second, etc.',\n",
        "    'FACILITY': 'Buildings, airports, highways, bridges',\n",
        "    'LOC': 'Non-GPE locations'\n",
        "}\n",
        "\n",
        "for ent in doc.ents:\n",
        "    desc = entity_descriptions.get(ent.label_, ent.label_)\n",
        "    print(f\"{ent.text:<20} {ent.label_:<12} {desc:<25}\")\n",
        "\n",
        "# Group entities by type\n",
        "entities_by_type = {}\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ not in entities_by_type:\n",
        "        entities_by_type[ent.label_] = []\n",
        "    entities_by_type[ent.label_].append(ent.text)\n",
        "\n",
        "print(f\"\\nEntities Grouped by Type:\")\n",
        "for label, entities in entities_by_type.items():\n",
        "    print(f\"  {label}: {entities}\")\n",
        "\n",
        "# Custom entity patterns\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Pattern for email addresses (simplified)\n",
        "email_pattern = [{\"LIKE_EMAIL\": True}]\n",
        "matcher.add(\"EMAIL\", [email_pattern])\n",
        "\n",
        "# Pattern for phone numbers (simplified)\n",
        "phone_pattern = [{\"SHAPE\": \"ddd-ddd-dddd\"}]\n",
        "matcher.add(\"PHONE\", [phone_pattern])\n",
        "\n",
        "test_text = \"Contact us at john.doe@company.com or call 555-123-4567 for support.\"\n",
        "test_doc = nlp(test_text)\n",
        "matches = matcher(test_doc)\n",
        "\n",
        "print(f\"\\nCustom Pattern Matching:\")\n",
        "print(f\"Text: {test_text}\")\n",
        "for match_id, start, end in matches:\n",
        "    label = nlp.vocab.strings[match_id]\n",
        "    span = test_doc[start:end]\n",
        "    print(f\"  Found {label}: {span.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZEkXy5tovrz",
        "outputId": "8cb7e737-3968-42c4-c0b8-21c5748abc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "4. NAMED ENTITY RECOGNITION (NER)\n",
            "-----------------------------------\n",
            "Text: Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976. \n",
            "The company is now worth ove...\n",
            "\n",
            "Named Entities Found:\n",
            "Entity               Label        Description              \n",
            "------------------------------------------------------------\n",
            "Apple Inc.           ORG          Companies, agencies, institutions\n",
            "Steve Jobs           PERSON       People, including fictional\n",
            "Cupertino            GPE          Countries, cities, states\n",
            "California           GPE          Countries, cities, states\n",
            "1976                 DATE         Absolute or relative dates\n",
            "over $2 trillion dollars MONEY        Monetary values          \n",
            "Tim Cook             PERSON       People, including fictional\n",
            "2011                 DATE         Absolute or relative dates\n",
            "Microsoft            ORG          Companies, agencies, institutions\n",
            "Google               ORG          Companies, agencies, institutions\n",
            "Amazon               ORG          Companies, agencies, institutions\n",
            "One                  CARDINAL     Numerals that do not fall under other types\n",
            "over 100,000         CARDINAL     Numerals that do not fall under other types\n",
            "\n",
            "Entities Grouped by Type:\n",
            "  ORG: ['Apple Inc.', 'Microsoft', 'Google', 'Amazon']\n",
            "  PERSON: ['Steve Jobs', 'Tim Cook']\n",
            "  GPE: ['Cupertino', 'California']\n",
            "  DATE: ['1976', '2011']\n",
            "  MONEY: ['over $2 trillion dollars']\n",
            "  CARDINAL: ['One', 'over 100,000']\n",
            "\n",
            "Custom Pattern Matching:\n",
            "Text: Contact us at john.doe@company.com or call 555-123-4567 for support.\n",
            "  Found EMAIL: john.doe@company.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: SENTENCE SEGMENTATION & TEXT PROCESSING (8 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n5. SENTENCE SEGMENTATION & TEXT PROCESSING\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "text = \"\"\"Natural language processing is fascinating! It involves multiple steps.\n",
        "First, we tokenize the text. Then, we analyze grammar. Finally, we extract meaning.\n",
        "Dr. Smith published a paper on this topic in 2020. The research was groundbreaking.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Sentence segmentation\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"\\nSentences detected: {len(list(doc.sents))}\")\n",
        "\n",
        "for i, sent in enumerate(doc.sents, 1):\n",
        "    print(f\"  {i}: {sent.text.strip()}\")\n",
        "\n",
        "# Sentence-level analysis\n",
        "print(f\"\\nSentence Analysis:\")\n",
        "for i, sent in enumerate(doc.sents, 1):\n",
        "    sent_doc = nlp(sent.text)\n",
        "    entities = [ent.text for ent in sent_doc.ents]\n",
        "    nouns = [token.text for token in sent_doc if token.pos_ == \"NOUN\"]\n",
        "    verbs = [token.lemma_ for token in sent_doc if token.pos_ == \"VERB\"]\n",
        "\n",
        "    print(f\"\\nSentence {i}: {sent.text[:50]}...\")\n",
        "    print(f\"  Entities: {entities if entities else 'None'}\")\n",
        "    print(f\"  Nouns: {nouns}\")\n",
        "    print(f\"  Verbs: {verbs}\")\n",
        "\n",
        "# Text statistics\n",
        "print(f\"\\nText Statistics:\")\n",
        "total_tokens = len(doc)\n",
        "sentences = list(doc.sents)\n",
        "avg_sent_length = total_tokens / len(sentences) if sentences else 0\n",
        "\n",
        "print(f\"  Total tokens: {total_tokens}\")\n",
        "print(f\"  Total sentences: {len(sentences)}\")\n",
        "print(f\"  Average sentence length: {avg_sent_length:.1f} tokens\")\n",
        "print(f\"  Unique lemmas: {len(set(token.lemma_ for token in doc))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZsv-JTWpHd8",
        "outputId": "259d5c62-5f84-4283-c2f0-a70345d345c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "5. SENTENCE SEGMENTATION & TEXT PROCESSING\n",
            "-----------------------------------\n",
            "Original text: Natural language processing is fascinating! It involves multiple steps. \n",
            "First, we tokenize the text. Then, we analyze grammar. Finally, we extract meaning. \n",
            "Dr. Smith published a paper on this topic in 2020. The research was groundbreaking.\n",
            "\n",
            "Sentences detected: 7\n",
            "  1: Natural language processing is fascinating!\n",
            "  2: It involves multiple steps.\n",
            "  3: First, we tokenize the text.\n",
            "  4: Then, we analyze grammar.\n",
            "  5: Finally, we extract meaning.\n",
            "  6: Dr. Smith published a paper on this topic in 2020.\n",
            "  7: The research was groundbreaking.\n",
            "\n",
            "Sentence Analysis:\n",
            "\n",
            "Sentence 1: Natural language processing is fascinating!...\n",
            "  Entities: None\n",
            "  Nouns: ['language', 'processing']\n",
            "  Verbs: []\n",
            "\n",
            "Sentence 2: It involves multiple steps. \n",
            "...\n",
            "  Entities: None\n",
            "  Nouns: ['steps']\n",
            "  Verbs: ['involve']\n",
            "\n",
            "Sentence 3: First, we tokenize the text....\n",
            "  Entities: ['First']\n",
            "  Nouns: ['text']\n",
            "  Verbs: ['tokenize']\n",
            "\n",
            "Sentence 4: Then, we analyze grammar....\n",
            "  Entities: None\n",
            "  Nouns: ['grammar']\n",
            "  Verbs: ['analyze']\n",
            "\n",
            "Sentence 5: Finally, we extract meaning. \n",
            "...\n",
            "  Entities: None\n",
            "  Nouns: ['meaning']\n",
            "  Verbs: ['extract']\n",
            "\n",
            "Sentence 6: Dr. Smith published a paper on this topic in 2020....\n",
            "  Entities: ['Smith', '2020']\n",
            "  Nouns: ['paper', 'topic']\n",
            "  Verbs: ['publish']\n",
            "\n",
            "Sentence 7: The research was groundbreaking....\n",
            "  Entities: None\n",
            "  Nouns: ['research']\n",
            "  Verbs: ['groundbreake']\n",
            "\n",
            "Text Statistics:\n",
            "  Total tokens: 48\n",
            "  Total sentences: 7\n",
            "  Average sentence length: 6.9 tokens\n",
            "  Unique lemmas: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDL5krNOqONf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}