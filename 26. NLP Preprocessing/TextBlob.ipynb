{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PTHLbNjy_AS",
        "outputId": "68817f15-b61a-420d-c1d2-c6b2048cd6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob 1-Hour Complete Tutorial\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "print(\"TextBlob 1-Hour Complete Tutorial\")\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 1: BASICS & TEXT CREATION (10 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n1. CREATING TEXTBLOB OBJECTS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Creating TextBlob objects\n",
        "text1 = \"Hello world! This is a simple example.\"\n",
        "blob1 = TextBlob(text1)\n",
        "\n",
        "text2 = \"\"\"Natural language processing is amazing!\n",
        "TextBlob makes it very easy to analyze text.\n",
        "We can perform sentiment analysis, extract nouns, and much more.\"\"\"\n",
        "blob2 = TextBlob(text2)\n",
        "\n",
        "print(f\"Original text: {text1}\")\n",
        "print(f\"TextBlob object: {blob1}\")\n",
        "print(f\"Type: {type(blob1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duPIBqNF3X_g",
        "outputId": "c96da3d0-1513-403c-a40e-60577591b73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. CREATING TEXTBLOB OBJECTS\n",
            "------------------------------\n",
            "Original text: Hello world! This is a simple example.\n",
            "TextBlob object: Hello world! This is a simple example.\n",
            "Type: <class 'textblob.blob.TextBlob'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP3RLrvo4jmc",
        "outputId": "87120d03-96d1-4805-d46e-3cea2718ca41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ellzw1Ja5hn5",
        "outputId": "54500a18-d296-4a44-a8ef-224c0652100a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: BASIC TEXT OPERATIONS (10 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n2. BASIC TEXT OPERATIONS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog. This is another sentence for testing!\"\n",
        "blob = TextBlob(sample_text)\n",
        "\n",
        "# Accessing words\n",
        "print(\"Words:\", blob.words)\n",
        "print(\"Number of words:\", len(blob.words))\n",
        "\n",
        "# Accessing sentences\n",
        "print(\"\\nSentences:\")\n",
        "for i, sentence in enumerate(blob.sentences, 1):\n",
        "    print(f\"  {i}: {sentence}\")\n",
        "\n",
        "# Noun phrases\n",
        "print(\"\\nNoun phrases:\", blob.noun_phrases)\n",
        "\n",
        "# Word frequency\n",
        "print(\"\\nWord frequencies:\")\n",
        "word_counts = blob.word_counts\n",
        "for word, count in list(word_counts.items())[:5]:  # Show first 5\n",
        "    print(f\"  '{word}': {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpZ1TR-q3oZ0",
        "outputId": "609971ae-d786-4292-cc8b-9570ca4f3b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "2. BASIC TEXT OPERATIONS\n",
            "------------------------------\n",
            "Words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'This', 'is', 'another', 'sentence', 'for', 'testing']\n",
            "Number of words: 15\n",
            "\n",
            "Sentences:\n",
            "  1: The quick brown fox jumps over the lazy dog.\n",
            "  2: This is another sentence for testing!\n",
            "\n",
            "Noun phrases: ['quick brown fox jumps', 'lazy dog']\n",
            "\n",
            "Word frequencies:\n",
            "  'the': 2\n",
            "  'quick': 1\n",
            "  'brown': 1\n",
            "  'fox': 1\n",
            "  'jumps': 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# SECTION 3: SENTIMENT ANALYSIS (10 minutes)\n",
        "# =============================================================================\n",
        "#https://github.com/sloria/TextBlob/blob/dev/src/textblob/en/en-sentiment.xml\n",
        "\n",
        "\n",
        "print(\"\\n\\n3. SENTIMENT ANALYSIS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Sample texts with different sentiments\n",
        "texts = [\n",
        "    \"I love this product! It's absolutely amazing and wonderful!\",\n",
        "    \"This movie is terrible. I hate it so much. Worst experience ever.\",\n",
        "    \"The weather is okay today. Nothing special about it.\",\n",
        "    \"Python is a great programming language for data science.\",\n",
        "    \"I'm feeling sad and disappointed about the results.\",\n",
        "    \"The class is very boring\",\n",
        "    \"The feedback is very good \"\n",
        "]\n",
        "\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(\"Polarity: -1 (negative) to +1 (positive)\")\n",
        "print(\"Subjectivity: 0 (objective) to 1 (subjective)\")\n",
        "print()\n",
        "\n",
        "for text in texts:\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    # Classify sentiment\n",
        "    if sentiment.polarity > 0.1:\n",
        "        mood = \"Positive\"\n",
        "    elif sentiment.polarity < -0.1:\n",
        "        mood = \"Negative\"\n",
        "    else:\n",
        "        mood = \"Neutral\"\n",
        "\n",
        "    print(f\"Text: {text[:50]}...\")\n",
        "    print(f\"  Polarity: {sentiment.polarity:.3f} ({mood})\")\n",
        "    print(f\"  Subjectivity: {sentiment.subjectivity:.3f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytWWAA9K38Wy",
        "outputId": "7e6d1c69-dfea-4c65-9b51-f6e1de15c551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "3. SENTIMENT ANALYSIS\n",
            "------------------------------\n",
            "Sentiment Analysis Results:\n",
            "Polarity: -1 (negative) to +1 (positive)\n",
            "Subjectivity: 0 (objective) to 1 (subjective)\n",
            "\n",
            "Text: I love this product! It's absolutely amazing and w...\n",
            "  Polarity: 0.742 (Positive)\n",
            "  Subjectivity: 0.833\n",
            "\n",
            "Text: This movie is terrible. I hate it so much. Worst e...\n",
            "  Polarity: -0.933 (Negative)\n",
            "  Subjectivity: 0.967\n",
            "\n",
            "Text: The weather is okay today. Nothing special about i...\n",
            "  Polarity: 0.429 (Positive)\n",
            "  Subjectivity: 0.536\n",
            "\n",
            "Text: Python is a great programming language for data sc...\n",
            "  Polarity: 0.800 (Positive)\n",
            "  Subjectivity: 0.750\n",
            "\n",
            "Text: I'm feeling sad and disappointed about the results...\n",
            "  Polarity: -0.625 (Negative)\n",
            "  Subjectivity: 0.875\n",
            "\n",
            "Text: The class is very boring...\n",
            "  Polarity: -1.000 (Negative)\n",
            "  Subjectivity: 1.000\n",
            "\n",
            "Text: The feedback is very good ...\n",
            "  Polarity: 0.910 (Positive)\n",
            "  Subjectivity: 0.780\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ8dA3rG-KrK",
        "outputId": "1ddd11e1-69bb-42c9-b534-7b490304e052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: PART-OF-SPEECH TAGGING (8 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n4. PART-OF-SPEECH TAGGING\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "text = \"The beautiful sunset painted the sky with vibrant colors yesterday.\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(\"\\nPart-of-Speech Tags:\")\n",
        "\n",
        "# Common POS tags explanation\n",
        "pos_meanings = {\n",
        "    'DT': 'Determiner', 'JJ': 'Adjective', 'NN': 'Noun', 'NNS': 'Plural Noun',\n",
        "    'VBD': 'Past Tense Verb', 'VBN': 'Past Participle', 'IN': 'Preposition',\n",
        "    'CC': 'Conjunction', 'RB': 'Adverb', 'PRP': 'Pronoun'\n",
        "}\n",
        "\n",
        "for word, tag in blob.tags:\n",
        "    meaning = pos_meanings.get(tag, 'Other')\n",
        "    print(f\"  {word:<12} -> {tag:<4} ({meaning})\")\n",
        "\n",
        "# Extract specific parts of speech\n",
        "nouns = [word for word, tag in blob.tags if tag.startswith('NN')]\n",
        "adjectives = [word for word, tag in blob.tags if tag.startswith('JJ')]\n",
        "verbs = [word for word, tag in blob.tags if tag.startswith('VB')]\n",
        "\n",
        "print(f\"\\nExtracted parts:\")\n",
        "print(f\"  Nouns: {nouns}\")\n",
        "print(f\"  Adjectives: {adjectives}\")\n",
        "print(f\"  Verbs: {verbs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZK2gSmt6jvU",
        "outputId": "54d46aff-c3cd-46f0-ba20-4654b231ee27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. PART-OF-SPEECH TAGGING\n",
            "------------------------------\n",
            "Text: The beautiful sunset painted the sky with vibrant colors yesterday.\n",
            "\n",
            "Part-of-Speech Tags:\n",
            "  The          -> DT   (Determiner)\n",
            "  beautiful    -> JJ   (Adjective)\n",
            "  sunset       -> NN   (Noun)\n",
            "  painted      -> VBD  (Past Tense Verb)\n",
            "  the          -> DT   (Determiner)\n",
            "  sky          -> NN   (Noun)\n",
            "  with         -> IN   (Preposition)\n",
            "  vibrant      -> JJ   (Adjective)\n",
            "  colors       -> NNS  (Plural Noun)\n",
            "  yesterday    -> NN   (Noun)\n",
            "\n",
            "Extracted parts:\n",
            "  Nouns: ['sunset', 'sky', 'colors', 'yesterday']\n",
            "  Adjectives: ['beautiful', 'vibrant']\n",
            "  Verbs: ['painted']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eck-bcD-gDa",
        "outputId": "dad773cd-6ed1-4b75-c632-2a1a2cb0434d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: TEXT NORMALIZATION & TRANSFORMATION (8 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n5. TEXT NORMALIZATION & TRANSFORMATION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "text = \"The CATS are running quickly through the gardens\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(f\"Original: {text}\")\n",
        "\n",
        "# Case transformations\n",
        "print(f\"Upper: {blob.upper()}\")\n",
        "print(f\"Lower: {blob.lower()}\")\n",
        "print(f\"Title: {blob.title()}\")\n",
        "\n",
        "# Word transformations\n",
        "words = blob.words\n",
        "print(f\"\\nOriginal words: {list(words)}\")\n",
        "\n",
        "# Singularization and Pluralization\n",
        "print(\"\\nSingular/Plural transformations:\")\n",
        "test_words = ['cats', 'running', 'gardens', 'child', 'mouse']\n",
        "for word in test_words:\n",
        "    w = TextBlob(word).words[0]\n",
        "    print(f\"  {word:<8} -> singular: {w.singularize():<8} | plural: {w.pluralize()}\")\n",
        "\n",
        "# Lemmatization (getting root form)\n",
        "print(\"\\nLemmatization:\")\n",
        "lemma_words = ['running', 'ran', 'better', 'dancing', 'wolves']\n",
        "for word in lemma_words:\n",
        "    w = TextBlob(word).words[0]\n",
        "    print(f\"  {word:<8} -> lemma: {w.lemmatize()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HjIBSVE-HP2",
        "outputId": "5de2e583-e60b-4020-cdc4-a23a2fd71f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "5. TEXT NORMALIZATION & TRANSFORMATION\n",
            "------------------------------\n",
            "Original: The CATS are running quickly through the gardens\n",
            "Upper: THE CATS ARE RUNNING QUICKLY THROUGH THE GARDENS\n",
            "Lower: the cats are running quickly through the gardens\n",
            "Title: The Cats Are Running Quickly Through The Gardens\n",
            "\n",
            "Original words: ['The', 'CATS', 'are', 'running', 'quickly', 'through', 'the', 'gardens']\n",
            "\n",
            "Singular/Plural transformations:\n",
            "  cats     -> singular: cat      | plural: catss\n",
            "  running  -> singular: running  | plural: runnings\n",
            "  gardens  -> singular: garden   | plural: gardenss\n",
            "  child    -> singular: child    | plural: children\n",
            "  mouse    -> singular: mouse    | plural: mice\n",
            "\n",
            "Lemmatization:\n",
            "  running  -> lemma: running\n",
            "  ran      -> lemma: ran\n",
            "  better   -> lemma: better\n",
            "  dancing  -> lemma: dancing\n",
            "  wolves   -> lemma: wolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 6: SPELLING CORRECTION (5 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n6. SPELLING CORRECTION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Text with spelling errors\n",
        "incorrect_text = \"I havv goood speling but somtimes I maek mistaks\"\n",
        "blob = TextBlob(incorrect_text)\n",
        "\n",
        "print(f\"Original: {incorrect_text}\")\n",
        "print(f\"Corrected: {blob.correct()}\")\n",
        "\n",
        "# Individual word correction\n",
        "wrong_words = ['speling', 'mistaks', 'recieve', 'definately', 'occured']\n",
        "print(\"\\nIndividual word corrections:\")\n",
        "for word in wrong_words:\n",
        "    corrected = TextBlob(word).correct()\n",
        "    print(f\"  {word:<12} -> {corrected}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D21jXPUI-brp",
        "outputId": "4e455026-49f5-46a5-f513-e2500ca23d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "6. SPELLING CORRECTION\n",
            "------------------------------\n",
            "Original: I havv goood speling but somtimes I maek mistaks\n",
            "Corrected: I have good spelling but sometimes I make mistake\n",
            "\n",
            "Individual word corrections:\n",
            "  speling      -> spelling\n",
            "  mistaks      -> mistake\n",
            "  recieve      -> receive\n",
            "  definately   -> definitely\n",
            "  occured      -> occurred\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 7: N-GRAMS & LANGUAGE DETECTION (5 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n7. N-GRAMS & LANGUAGE DETECTION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "text = \"Natural language processing with Python is powerful and useful\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# N-grams\n",
        "print(\"Bigrams (2-grams):\")\n",
        "for bigram in blob.ngrams(n=2):\n",
        "    print(f\"  {' '.join(bigram)}\")\n",
        "\n",
        "print(\"\\nTrigrams (3-grams):\")\n",
        "for trigram in blob.ngrams(n=3)[:5]:  # Show first 5\n",
        "    print(f\"  {' '.join(trigram)}\")\n",
        "\n",
        "# Language detection\n",
        "print(\"\\nLanguage Detection:\")\n",
        "texts_diff_langs = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Bonjour, comment allez-vous?\",\n",
        "    \"Hola, ¿cómo estás?\",\n",
        "    \"Hallo, wie geht es dir?\",\n",
        "    \"Привет, как дела?\"\n",
        "]\n",
        "\n",
        "for text in texts_diff_langs:\n",
        "    try:\n",
        "        detected_lang = TextBlob(text).detect_language()\n",
        "        print(f\"  '{text}' -> {detected_lang}\")\n",
        "    except:\n",
        "        print(f\"  '{text}' -> Could not detect\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyM_dWV2_Myn",
        "outputId": "7864d6b9-2fd6-46b1-9e7d-41aa2d5a6106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "7. N-GRAMS & LANGUAGE DETECTION\n",
            "------------------------------\n",
            "Bigrams (2-grams):\n",
            "  Natural language\n",
            "  language processing\n",
            "  processing with\n",
            "  with Python\n",
            "  Python is\n",
            "  is powerful\n",
            "  powerful and\n",
            "  and useful\n",
            "\n",
            "Trigrams (3-grams):\n",
            "  Natural language processing\n",
            "  language processing with\n",
            "  processing with Python\n",
            "  with Python is\n",
            "  Python is powerful\n",
            "\n",
            "Language Detection:\n",
            "  'Hello, how are you?' -> Could not detect\n",
            "  'Bonjour, comment allez-vous?' -> Could not detect\n",
            "  'Hola, ¿cómo estás?' -> Could not detect\n",
            "  'Hallo, wie geht es dir?' -> Could not detect\n",
            "  'Привет, как дела?' -> Could not detect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 8: PRACTICAL EXAMPLES & USE CASES (9 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n8. PRACTICAL EXAMPLES\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 1: Analyzing customer reviews\n",
        "print(\"Example 1: Customer Review Analysis\")\n",
        "reviews = [\n",
        "    \"This product is absolutely fantastic! Best purchase ever!\",\n",
        "    \"Terrible quality. Broke after one day. Don't buy this.\",\n",
        "    \"It's okay, nothing special but does the job.\",\n",
        "    \"Amazing customer service and great product quality!\",\n",
        "    \"Worst experience ever. Very disappointed.\"\n",
        "]\n",
        "\n",
        "review_analysis = []\n",
        "for i, review in enumerate(reviews, 1):\n",
        "    blob = TextBlob(review)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "\n",
        "    if sentiment > 0.1:\n",
        "        rating = \"⭐⭐⭐⭐⭐\" if sentiment > 0.5 else \"⭐⭐⭐⭐\"\n",
        "    elif sentiment < -0.1:\n",
        "        rating = \"⭐⭐\" if sentiment > -0.5 else \"⭐\"\n",
        "    else:\n",
        "        rating = \"⭐⭐⭐\"\n",
        "\n",
        "    review_analysis.append({\n",
        "        'review': review[:40] + \"...\",\n",
        "        'sentiment': sentiment,\n",
        "        'rating': rating\n",
        "    })\n",
        "\n",
        "print(\"\\nReview Analysis Results:\")\n",
        "for analysis in review_analysis:\n",
        "    print(f\"  {analysis['rating']} ({analysis['sentiment']:+.2f}) - {analysis['review']}\")\n",
        "\n",
        "# Example 2: Text preprocessing pipeline\n",
        "print(\"\\n\\nExample 2: Text Preprocessing Pipeline\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Step 1: Basic cleaning\n",
        "    cleaned = blob.lower()\n",
        "\n",
        "    # Step 2: Spelling correction\n",
        "    corrected = cleaned.correct()\n",
        "\n",
        "    # Step 3: Extract meaningful words (nouns and adjectives)\n",
        "    meaningful_words = [word for word, tag in corrected.tags\n",
        "                       if tag.startswith(('NN', 'JJ', 'VB'))]\n",
        "\n",
        "    # Step 4: Lemmatization\n",
        "    lemmatized = [TextBlob(word).words[0].lemmatize() for word in meaningful_words]\n",
        "\n",
        "    return {\n",
        "        'original': text,\n",
        "        'cleaned': str(cleaned),\n",
        "        'corrected': str(corrected),\n",
        "        'meaningful_words': meaningful_words,\n",
        "        'lemmatized': lemmatized,\n",
        "        'sentiment': corrected.sentiment.polarity\n",
        "    }\n",
        "\n",
        "# Test the pipeline\n",
        "sample_texts = [\n",
        "    \"The cats are running quickly through the beautiful gardens!\",\n",
        "    \"I absolutley love this amazng product. It's fantasic!\",\n",
        "    \"This experiance was terribel and I'm very disapointed.\"\n",
        "]\n",
        "\n",
        "print(\"Text Preprocessing Results:\")\n",
        "for text in sample_texts:\n",
        "    result = preprocess_text(text)\n",
        "    print(f\"\\nOriginal: {result['original']}\")\n",
        "    print(f\"Processed: {' '.join(result['lemmatized'])}\")\n",
        "    print(f\"Sentiment: {result['sentiment']:+.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-Bdshzl_P8z",
        "outputId": "730eb987-d6c1-41c0-c6c4-6fb116d34a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "8. PRACTICAL EXAMPLES\n",
            "------------------------------\n",
            "Example 1: Customer Review Analysis\n",
            "\n",
            "Review Analysis Results:\n",
            "  ⭐⭐⭐⭐⭐ (+0.75) - This product is absolutely fantastic! Be...\n",
            "  ⭐ (-1.00) - Terrible quality. Broke after one day. D...\n",
            "  ⭐⭐⭐⭐ (+0.43) - It's okay, nothing special but does the ...\n",
            "  ⭐⭐⭐⭐⭐ (+0.80) - Amazing customer service and great produ...\n",
            "  ⭐ (-0.99) - Worst experience ever. Very disappointed...\n",
            "\n",
            "\n",
            "Example 2: Text Preprocessing Pipeline\n",
            "Text Preprocessing Results:\n",
            "\n",
            "Original: The cats are running quickly through the beautiful gardens!\n",
            "Processed: cat are running beautiful garden\n",
            "Sentiment: +0.667\n",
            "\n",
            "Original: I absolutley love this amazng product. It's fantasic!\n",
            "Processed: i love amazing product 's fantastic\n",
            "Sentiment: +0.533\n",
            "\n",
            "Original: This experiance was terribel and I'm very disapointed.\n",
            "Processed: experience wa terrible i 'm disappointed\n",
            "Sentiment: -0.988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 9: INTEGRATION WITH PANDAS (5 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n9. INTEGRATION WITH PANDAS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create sample dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I love this new smartphone! Great battery life.\",\n",
        "        \"The camera quality is poor. Very disappointed.\",\n",
        "        \"Good value for money. Decent performance overall.\",\n",
        "        \"Excellent build quality and fast processing speed.\",\n",
        "        \"Battery dies too quickly. Not worth the price.\"\n",
        "    ],\n",
        "    'product': ['Phone A', 'Phone B', 'Phone C', 'Phone D', 'Phone E']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply TextBlob operations to pandas DataFrame\n",
        "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
        "df['word_count'] = df['text'].apply(lambda x: len(TextBlob(x).words))\n",
        "df['noun_phrases'] = df['text'].apply(lambda x: list(TextBlob(x).noun_phrases))\n",
        "\n",
        "print(\"DataFrame with TextBlob Analysis:\")\n",
        "print(df[['product', 'sentiment', 'word_count']].round(3))\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"Average sentiment: {df['sentiment'].mean():.3f}\")\n",
        "print(f\"Most positive review: {df.loc[df['sentiment'].idxmax(), 'product']}\")\n",
        "print(f\"Most negative review: {df.loc[df['sentiment'].idxmin(), 'product']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--59aAsX_Y3L",
        "outputId": "7fc9b7c0-3f9c-47e6-8f76-f7c9e32e8e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "9. INTEGRATION WITH PANDAS\n",
            "------------------------------\n",
            "DataFrame with TextBlob Analysis:\n",
            "   product  sentiment  word_count\n",
            "0  Phone A      0.490           8\n",
            "1  Phone B     -0.688           7\n",
            "2  Phone C      0.289           7\n",
            "3  Phone D      0.600           7\n",
            "4  Phone E     -0.150           8\n",
            "\n",
            "Summary Statistics:\n",
            "Average sentiment: 0.108\n",
            "Most positive review: Phone D\n",
            "Most negative review: Phone B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 10: TIPS & BEST PRACTICES (5 minutes)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n10. TIPS & BEST PRACTICES\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "tips = [\n",
        "    \"1. Performance: TextBlob is slower than spaCy for large datasets\",\n",
        "    \"2. Accuracy: Built-in sentiment analysis works well for general text but may need training for domain-specific content\",\n",
        "    \"3. Language: Works best with English; limited support for other languages\",\n",
        "    \"4. Spelling correction: Can be slow and may not always be accurate\",\n",
        "    \"5. Memory: Process large texts in chunks to avoid memory issues\",\n",
        "    \"6. Preprocessing: Always clean your text before analysis\",\n",
        "    \"7. Validation: Always validate sentiment results with sample data\",\n",
        "    \"8. Alternatives: Consider NLTK, spaCy, or transformers for production use\"\n",
        "]\n",
        "\n",
        "for tip in tips:\n",
        "    print(f\"  {tip}\")\n",
        "\n",
        "# Final example: Complete text analysis function\n",
        "def complete_text_analysis(text):\n",
        "    \"\"\"Comprehensive text analysis using TextBlob\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    analysis = {\n",
        "        'text': text,\n",
        "        'word_count': len(blob.words),\n",
        "        'sentence_count': len(blob.sentences),\n",
        "        'sentiment_polarity': blob.sentiment.polarity,\n",
        "        'sentiment_subjectivity': blob.sentiment.subjectivity,\n",
        "        'noun_phrases': list(blob.noun_phrases),\n",
        "        #'most_common_words': blob.word_counts.most_common(3),\n",
        "        'language': 'en'  # Default, as detection can be unreliable\n",
        "    }\n",
        "\n",
        "    # Sentiment classification\n",
        "    if analysis['sentiment_polarity'] > 0.1:\n",
        "        analysis['sentiment_label'] = 'Positive'\n",
        "    elif analysis['sentiment_polarity'] < -0.1:\n",
        "        analysis['sentiment_label'] = 'Negative'\n",
        "    else:\n",
        "        analysis['sentiment_label'] = 'Neutral'\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Test the complete analysis\n",
        "test_text = \"\"\"\n",
        "TextBlob is a Python library for processing textual data. It provides a simple API\n",
        "for diving into common natural language processing tasks such as part-of-speech tagging,\n",
        "noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "The library is built on top of NLTK and pattern, making it easy to use for beginners.\n",
        "\"\"\"\n",
        "\n",
        "final_analysis = complete_text_analysis(test_text)\n",
        "\n",
        "print(f\"\\n\\nCOMPLETE TEXT ANALYSIS EXAMPLE:\")\n",
        "print(f\"Text length: {len(final_analysis['text'])} characters\")\n",
        "print(f\"Words: {final_analysis['word_count']}\")\n",
        "print(f\"Sentences: {final_analysis['sentence_count']}\")\n",
        "print(f\"Sentiment: {final_analysis['sentiment_label']} ({final_analysis['sentiment_polarity']:+.3f})\")\n",
        "print(f\"Subjectivity: {final_analysis['sentiment_subjectivity']:.3f}\")\n",
        "print(f\"Key phrases: {final_analysis['noun_phrases'][:3]}\")\n",
        "#print(f\"Common words: {final_analysis['most_common_words']}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TEXTBLOB TUTORIAL COMPLETED! 🎉\")\n",
        "print(\"You've learned all the essential TextBlob features.\")\n",
        "print(\"Practice with your own text data to master these concepts!\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeEs4WiS_eHe",
        "outputId": "05170012-d76d-4842-d85d-52f9a25b77b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "10. TIPS & BEST PRACTICES\n",
            "------------------------------\n",
            "  1. Performance: TextBlob is slower than spaCy for large datasets\n",
            "  2. Accuracy: Built-in sentiment analysis works well for general text but may need training for domain-specific content\n",
            "  3. Language: Works best with English; limited support for other languages\n",
            "  4. Spelling correction: Can be slow and may not always be accurate\n",
            "  5. Memory: Process large texts in chunks to avoid memory issues\n",
            "  6. Preprocessing: Always clean your text before analysis\n",
            "  7. Validation: Always validate sentiment results with sample data\n",
            "  8. Alternatives: Consider NLTK, spaCy, or transformers for production use\n",
            "\n",
            "\n",
            "COMPLETE TEXT ANALYSIS EXAMPLE:\n",
            "Text length: 344 characters\n",
            "Words: 52\n",
            "Sentences: 3\n",
            "Sentiment: Positive (+0.176)\n",
            "Subjectivity: 0.513\n",
            "Key phrases: ['textblob', 'python', 'processing textual data']\n",
            "\n",
            "==================================================\n",
            "TEXTBLOB TUTORIAL COMPLETED! 🎉\n",
            "You've learned all the essential TextBlob features.\n",
            "Practice with your own text data to master these concepts!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N353ez8N_icx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}