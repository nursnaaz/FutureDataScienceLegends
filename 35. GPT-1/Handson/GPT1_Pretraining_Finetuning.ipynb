{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QvMyq7Z_UMoU"
      },
      "outputs": [],
      "source": [
        "#GPT 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IYbeqX_3VgNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Steps to build the GPT model in local\n",
        "\n",
        "1. Create the Training Data (10 lines of text)\n",
        "2. Build the cleaning, tokenization, vocabulary, training samples, unsupervised training sample\n",
        "3. BUild the model (Transformer Decoder Architecture)\n",
        "4. Pre-train model (Predict the next token)\n",
        "5. Evaluation on Pre-train model\n",
        "6. Fine Tuning model (sentiment classification)\n",
        "7. Evaluation on Pre-trained Fine Tuned model\n",
        "8. Generate the text"
      ],
      "metadata": {
        "id": "XBkTzRyZWjeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "O0rlpPyMlD-_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextProcessor: NLP Text Preprocessing & Tokenization Engine\n",
        "class TextProcessor:\n",
        "    \"\"\"\n",
        "    TextProcessor - Advanced Text Preprocessing & Vocabulary Management System\n",
        "\n",
        "    A comprehensive text processing class that handles:\n",
        "    - Text cleaning and normalization\n",
        "    - Frequency-based vocabulary building\n",
        "    - Bidirectional text-to-token conversion\n",
        "    - Special token management for NLP pipelines\n",
        "\n",
        "    Ideal for preparing text data for machine learning models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=500):\n",
        "        \"\"\"Initialize the TextProcessor\"\"\"\n",
        "        print(\"Constructor Called\")\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}\n",
        "        self.reverse_vocab = {}\n",
        "        self.word_counts = {}\n",
        "        print(\"Constructor Initialized\")\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        text = text.lower()\n",
        "        # Keep basic punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\.\\!\\?\\,]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "\n",
        "        # Count all words\n",
        "        for text in texts:\n",
        "            clean_text = self.clean_text(text)\n",
        "            words = clean_text.split()\n",
        "            for word in words:\n",
        "                self.word_counts[word] = self.word_counts.get(word, 0) + 1\n",
        "\n",
        "        # Get most frequent words\n",
        "        frequent_words = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.vocab = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<START>': 2,\n",
        "            '<END>': 3\n",
        "        }\n",
        "\n",
        "        # Add frequent words\n",
        "        for i, (word, _) in enumerate(frequent_words[:self.vocab_size-4]):\n",
        "            self.vocab[word] = i + 4\n",
        "\n",
        "        # Add common words if vocabulary is small\n",
        "        common_words = ['the', 'and', 'is', 'a', 'to', 'for', 'of', 'with', 'in', 'this',\n",
        "                       'that', 'it', 'on', 'as', 'are', 'was', 'at', 'be', 'or', 'an',\n",
        "                       'very', 'good', 'great', 'bad', 'love', 'hate', 'like', 'nice']\n",
        "        for word in common_words:\n",
        "            if word not in self.vocab and len(self.vocab) < self.vocab_size:\n",
        "                self.vocab[word] = len(self.vocab)\n",
        "\n",
        "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to token ids\"\"\"\n",
        "        clean_text = self.clean_text(text)\n",
        "        words = clean_text.split()\n",
        "        return [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"Convert token ids back to text\"\"\"\n",
        "        words = []\n",
        "        for token_id in token_ids:\n",
        "            word = self.reverse_vocab.get(token_id, '<UNK>')\n",
        "            if word not in ['<PAD>', '<START>', '<END>']:\n",
        "                words.append(word)\n",
        "        return ' '.join(words)\n",
        "\n",
        "\n",
        "# ============== TESTING THE TextProcessor CLASS ==============\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING TextProcessor CLASS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. CREATE AN OBJECT\n",
        "print(\"\\n1. Creating TextProcessor object:\")\n",
        "print(\"-\" * 40)\n",
        "processor = TextProcessor(vocab_size=50)  # Using smaller vocab for demonstration\n",
        "print(f\"Created TextProcessor with vocab_size: {processor.vocab_size}\")\n",
        "print(f\"Initial vocab: {processor.vocab}\")\n",
        "print(f\"Initial word_counts: {processor.word_counts}\")\n",
        "\n",
        "# 2. TEST clean_text() METHOD\n",
        "print(\"\\n2. Testing clean_text() method:\")\n",
        "print(\"-\" * 40)\n",
        "sample_texts = [\n",
        "    \"Hello World! How are you?\",\n",
        "    \"This is a GREAT example with @special #characters & symbols!\",\n",
        "    \"Multiple    spaces   and\\tTABS\\nand newlines\",\n",
        "    \"Punctuation: hello, world. Nice! Really?\",\n",
        "]\n",
        "\n",
        "for text in sample_texts:\n",
        "    cleaned = processor.clean_text(text)\n",
        "    print(f\"Original: '{text}'\")\n",
        "    print(f\"Cleaned:  '{cleaned}'\")\n",
        "    print()\n",
        "\n",
        "# 3. TEST build_vocab() METHOD\n",
        "print(\"\\n3. Testing build_vocab() method:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Sample training texts\n",
        "training_texts = [\n",
        "    \"I love machine learning and natural language processing\",\n",
        "    \"This is a great example of text processing\",\n",
        "    \"Machine learning is very interesting and useful\",\n",
        "    \"I really love working with text data\",\n",
        "    \"Natural language processing is a great field\",\n",
        "    \"Text processing and machine learning go together\",\n",
        "    \"This example shows how to process text data\",\n",
        "    \"I love this great example of processing\"\n",
        "]\n",
        "\n",
        "print(\"Training texts:\")\n",
        "for i, text in enumerate(training_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "\n",
        "print(\"\\nBuilding vocabulary from training texts...\")\n",
        "vocab = processor.build_vocab(training_texts)\n",
        "\n",
        "print(f\"\\nWord counts (top 10):\")\n",
        "sorted_counts = sorted(processor.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, count in sorted_counts[:10]:\n",
        "    print(f\"  '{word}': {count}\")\n",
        "\n",
        "print(f\"\\nBuilt vocabulary (first 20 items):\")\n",
        "vocab_items = list(processor.vocab.items())[:20]\n",
        "for word, idx in vocab_items:\n",
        "    print(f\"  '{word}': {idx}\")\n",
        "\n",
        "print(f\"\\nSpecial tokens:\")\n",
        "for token in ['<PAD>', '<UNK>', '<START>', '<END>']:\n",
        "    print(f\"  {token}: {processor.vocab[token]}\")\n",
        "\n",
        "# 4. TEST encode() METHOD\n",
        "print(\"\\n4. Testing encode() method:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "test_sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"This is a great example\",\n",
        "    \"Unknown words will be encoded as UNK tokens\",\n",
        "    \"Text processing is useful\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    encoded = processor.encode(sentence)\n",
        "    print(f\"Text: '{sentence}'\")\n",
        "    print(f\"Encoded: {encoded}\")\n",
        "\n",
        "    # Show word-to-token mapping\n",
        "    words = processor.clean_text(sentence).split()\n",
        "    word_token_pairs = []\n",
        "    for word in words:\n",
        "        token_id = processor.vocab.get(word, processor.vocab['<UNK>'])\n",
        "        word_token_pairs.append(f\"'{word}'→{token_id}\")\n",
        "    print(f\"Mapping: {' | '.join(word_token_pairs)}\")\n",
        "    print()\n",
        "\n",
        "# 5. TEST decode() METHOD\n",
        "print(\"\\n5. Testing decode() method:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test with some encoded sequences\n",
        "test_sequences = [\n",
        "    [4, 5, 6, 7],  # Should correspond to some words from vocab\n",
        "    [0, 1, 2, 3],  # Special tokens\n",
        "    [4, 1, 5, 1, 6],  # Mix of known and unknown tokens\n",
        "]\n",
        "\n",
        "for seq in test_sequences:\n",
        "    decoded = processor.decode(seq)\n",
        "    print(f\"Token sequence: {seq}\")\n",
        "    print(f\"Decoded text: '{decoded}'\")\n",
        "\n",
        "    # Show token-to-word mapping\n",
        "    token_word_pairs = []\n",
        "    for token_id in seq:\n",
        "        word = processor.reverse_vocab.get(token_id, '<UNK>')\n",
        "        token_word_pairs.append(f\"{token_id}→'{word}'\")\n",
        "    print(f\"Mapping: {' | '.join(token_word_pairs)}\")\n",
        "    print()\n",
        "\n",
        "# 6. ROUND-TRIP TEST (encode then decode)\n",
        "print(\"\\n6. Round-trip test (encode → decode):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "round_trip_texts = [\n",
        "    \"I love this great example\",\n",
        "    \"Machine learning is interesting\",\n",
        "    \"Some unknown words here: quantum blockchain\"\n",
        "]\n",
        "\n",
        "for text in round_trip_texts:\n",
        "    print(f\"Original: '{text}'\")\n",
        "    encoded = processor.encode(text)\n",
        "    print(f\"Encoded: {encoded}\")\n",
        "    decoded = processor.decode(encoded)\n",
        "    print(f\"Decoded: '{decoded}'\")\n",
        "    print(f\"Match: {text.lower() == decoded}\")\n",
        "    print()\n",
        "\n",
        "# 7. SHOW VOCABULARY STATISTICS\n",
        "print(\"\\n7. Vocabulary Statistics:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total vocabulary size: {len(processor.vocab)}\")\n",
        "print(f\"Total unique words in training: {len(processor.word_counts)}\")\n",
        "print(f\"Words included in vocab: {len(processor.vocab) - 4}\")  # Minus special tokens\n",
        "print(f\"Special tokens: 4 (<PAD>, <UNK>, <START>, <END>)\")\n",
        "\n",
        "# Show most and least frequent words in vocab\n",
        "print(f\"\\nMost frequent words in training data:\")\n",
        "for word, count in sorted_counts[:5]:\n",
        "    if word in processor.vocab:\n",
        "        print(f\"  '{word}': {count} occurrences, token_id: {processor.vocab[word]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_tgUBF-bdJa",
        "outputId": "45deef49-efff-4795-8aab-94b074560328"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TESTING TextProcessor CLASS\n",
            "============================================================\n",
            "\n",
            "1. Creating TextProcessor object:\n",
            "----------------------------------------\n",
            "Constructor Called\n",
            "Constructor Initialized\n",
            "Created TextProcessor with vocab_size: 50\n",
            "Initial vocab: {}\n",
            "Initial word_counts: {}\n",
            "\n",
            "2. Testing clean_text() method:\n",
            "----------------------------------------\n",
            "Original: 'Hello World! How are you?'\n",
            "Cleaned:  'hello world! how are you?'\n",
            "\n",
            "Original: 'This is a GREAT example with @special #characters & symbols!'\n",
            "Cleaned:  'this is a great example with special characters symbols!'\n",
            "\n",
            "Original: 'Multiple    spaces   and\tTABS\n",
            "and newlines'\n",
            "Cleaned:  'multiple spaces and tabs and newlines'\n",
            "\n",
            "Original: 'Punctuation: hello, world. Nice! Really?'\n",
            "Cleaned:  'punctuation hello, world. nice! really?'\n",
            "\n",
            "\n",
            "3. Testing build_vocab() method:\n",
            "----------------------------------------\n",
            "Training texts:\n",
            "1. I love machine learning and natural language processing\n",
            "2. This is a great example of text processing\n",
            "3. Machine learning is very interesting and useful\n",
            "4. I really love working with text data\n",
            "5. Natural language processing is a great field\n",
            "6. Text processing and machine learning go together\n",
            "7. This example shows how to process text data\n",
            "8. I love this great example of processing\n",
            "\n",
            "Building vocabulary from training texts...\n",
            "Building vocabulary...\n",
            "Vocabulary size: 50\n",
            "\n",
            "Word counts (top 10):\n",
            "  'processing': 5\n",
            "  'text': 4\n",
            "  'i': 3\n",
            "  'love': 3\n",
            "  'machine': 3\n",
            "  'learning': 3\n",
            "  'and': 3\n",
            "  'this': 3\n",
            "  'is': 3\n",
            "  'great': 3\n",
            "\n",
            "Built vocabulary (first 20 items):\n",
            "  '<PAD>': 0\n",
            "  '<UNK>': 1\n",
            "  '<START>': 2\n",
            "  '<END>': 3\n",
            "  'processing': 4\n",
            "  'text': 5\n",
            "  'i': 6\n",
            "  'love': 7\n",
            "  'machine': 8\n",
            "  'learning': 9\n",
            "  'and': 10\n",
            "  'this': 11\n",
            "  'is': 12\n",
            "  'great': 13\n",
            "  'example': 14\n",
            "  'natural': 15\n",
            "  'language': 16\n",
            "  'a': 17\n",
            "  'of': 18\n",
            "  'data': 19\n",
            "\n",
            "Special tokens:\n",
            "  <PAD>: 0\n",
            "  <UNK>: 1\n",
            "  <START>: 2\n",
            "  <END>: 3\n",
            "\n",
            "4. Testing encode() method:\n",
            "----------------------------------------\n",
            "Text: 'I love machine learning'\n",
            "Encoded: [6, 7, 8, 9]\n",
            "Mapping: 'i'→6 | 'love'→7 | 'machine'→8 | 'learning'→9\n",
            "\n",
            "Text: 'This is a great example'\n",
            "Encoded: [11, 12, 17, 13, 14]\n",
            "Mapping: 'this'→11 | 'is'→12 | 'a'→17 | 'great'→13 | 'example'→14\n",
            "\n",
            "Text: 'Unknown words will be encoded as UNK tokens'\n",
            "Encoded: [1, 1, 1, 43, 1, 39, 1, 1]\n",
            "Mapping: 'unknown'→1 | 'words'→1 | 'will'→1 | 'be'→43 | 'encoded'→1 | 'as'→39 | 'unk'→1 | 'tokens'→1\n",
            "\n",
            "Text: 'Text processing is useful'\n",
            "Encoded: [5, 4, 12, 22]\n",
            "Mapping: 'text'→5 | 'processing'→4 | 'is'→12 | 'useful'→22\n",
            "\n",
            "\n",
            "5. Testing decode() method:\n",
            "----------------------------------------\n",
            "Token sequence: [4, 5, 6, 7]\n",
            "Decoded text: 'processing text i love'\n",
            "Mapping: 4→'processing' | 5→'text' | 6→'i' | 7→'love'\n",
            "\n",
            "Token sequence: [0, 1, 2, 3]\n",
            "Decoded text: '<UNK>'\n",
            "Mapping: 0→'<PAD>' | 1→'<UNK>' | 2→'<START>' | 3→'<END>'\n",
            "\n",
            "Token sequence: [4, 1, 5, 1, 6]\n",
            "Decoded text: 'processing <UNK> text <UNK> i'\n",
            "Mapping: 4→'processing' | 1→'<UNK>' | 5→'text' | 1→'<UNK>' | 6→'i'\n",
            "\n",
            "\n",
            "6. Round-trip test (encode → decode):\n",
            "----------------------------------------\n",
            "Original: 'I love this great example'\n",
            "Encoded: [6, 7, 11, 13, 14]\n",
            "Decoded: 'i love this great example'\n",
            "Match: True\n",
            "\n",
            "Original: 'Machine learning is interesting'\n",
            "Encoded: [8, 9, 12, 21]\n",
            "Decoded: 'machine learning is interesting'\n",
            "Match: True\n",
            "\n",
            "Original: 'Some unknown words here: quantum blockchain'\n",
            "Encoded: [1, 1, 1, 1, 1, 1]\n",
            "Decoded: '<UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'\n",
            "Match: False\n",
            "\n",
            "\n",
            "7. Vocabulary Statistics:\n",
            "----------------------------------------\n",
            "Total vocabulary size: 50\n",
            "Total unique words in training: 29\n",
            "Words included in vocab: 46\n",
            "Special tokens: 4 (<PAD>, <UNK>, <START>, <END>)\n",
            "\n",
            "Most frequent words in training data:\n",
            "  'processing': 5 occurrences, token_id: 4\n",
            "  'text': 4 occurrences, token_id: 5\n",
            "  'i': 3 occurrences, token_id: 6\n",
            "  'love': 3 occurrences, token_id: 7\n",
            "  'machine': 3 occurrences, token_id: 8\n",
            "\n",
            "============================================================\n",
            "TESTING COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SimpleGPT: Educational Transformer Language Model\n",
        "\n",
        "class SimpleGPT(keras.Model):\n",
        "    \"\"\"\n",
        "    SimpleGPT - Lightweight GPT Implementation for Learning & Experimentation\n",
        "\n",
        "    A simplified but fully functional GPT (Generative Pre-trained Transformer) model\n",
        "    designed for educational purposes. Features:\n",
        "    - Causal self-attention with masking\n",
        "    - Positional embeddings and multi-head attention\n",
        "    - Autoregressive text generation capabilities\n",
        "    - Clean, understandable transformer architecture\n",
        "\n",
        "    Perfect for understanding how modern language models work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, dff=128, max_length=16):\n",
        "        super(SimpleGPT, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dff = dff\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = layers.Embedding(max_length, d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = []\n",
        "        for _ in range(num_layers):\n",
        "            self.transformer_blocks.append([\n",
        "                layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads),\n",
        "                layers.LayerNormalization(),\n",
        "                layers.Dense(dff, activation='relu'),\n",
        "                layers.Dense(d_model),\n",
        "                layers.LayerNormalization(),\n",
        "                layers.Dropout(0.1)\n",
        "            ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = layers.Dense(vocab_size)\n",
        "        self.dropout = layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Create position indices\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "\n",
        "        # Embeddings\n",
        "        token_emb = self.token_embedding(inputs)\n",
        "        pos_emb = self.position_embedding(positions)\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = self.create_causal_mask(seq_len)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for attention, norm1, ffn1, ffn2, norm2, dropout in self.transformer_blocks:\n",
        "            # Self-attention with residual connection\n",
        "            attn_output = attention(x, x, attention_mask=mask, training=training)\n",
        "            x = norm1(x + attn_output)\n",
        "\n",
        "            # Feed-forward with residual connection\n",
        "            ffn_output = ffn2(ffn1(x))\n",
        "            ffn_output = dropout(ffn_output, training=training)\n",
        "            x = norm2(x + ffn_output)\n",
        "\n",
        "        # Output projection\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def create_causal_mask(self, seq_len):\n",
        "        \"\"\"Create causal mask for self-attention\"\"\"\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return mask[tf.newaxis, tf.newaxis, :, :]\n",
        "\n",
        "# ============== TESTING THE SimpleGPT CLASS ==============\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING SimpleGPT MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. CREATE MODEL OBJECT\n",
        "print(\"\\n1. Creating SimpleGPT Model Object:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = 100\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "dff = 128\n",
        "max_length = 16\n",
        "batch_size = 2\n",
        "\n",
        "print(f\"Model Configuration:\")\n",
        "print(f\"  Vocabulary Size: {vocab_size}\")\n",
        "print(f\"  Model Dimension (d_model): {d_model}\")\n",
        "print(f\"  Number of Attention Heads: {num_heads}\")\n",
        "print(f\"  Number of Layers: {num_layers}\")\n",
        "print(f\"  Feed-Forward Dimension: {dff}\")\n",
        "print(f\"  Maximum Sequence Length: {max_length}\")\n",
        "\n",
        "# Create the model\n",
        "model = SimpleGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    dff=dff,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "print(f\"\\nModel created successfully!\")\n",
        "print(f\"Model type: {type(model)}\")\n",
        "\n",
        "# 2. EXAMINE MODEL STRUCTURE\n",
        "print(\"\\n2. Examining Model Structure:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(f\"Model attributes:\")\n",
        "print(f\"  d_model: {model.d_model}\")\n",
        "print(f\"  max_length: {model.max_length}\")\n",
        "print(f\"  vocab_size: {model.vocab_size}\")\n",
        "print(f\"  num_heads: {model.num_heads}\")\n",
        "print(f\"  num_layers: {model.num_layers}\")\n",
        "\n",
        "print(f\"\\nEmbedding layers:\")\n",
        "print(f\"  Token embedding: {model.token_embedding}\")\n",
        "print(f\"  Position embedding: {model.position_embedding}\")\n",
        "\n",
        "print(f\"\\nTransformer blocks: {len(model.transformer_blocks)} layers\")\n",
        "for i, block in enumerate(model.transformer_blocks):\n",
        "    print(f\"  Layer {i+1}: {len(block)} components\")\n",
        "    print(f\"    - MultiHeadAttention: {block[0]}\")\n",
        "    print(f\"    - LayerNormalization: {block[1]}\")\n",
        "    print(f\"    - Dense (FFN1): {block[2]}\")\n",
        "    print(f\"    - Dense (FFN2): {block[3]}\")\n",
        "    print(f\"    - LayerNormalization: {block[4]}\")\n",
        "    print(f\"    - Dropout: {block[5]}\")\n",
        "\n",
        "print(f\"\\nOutput layer: {model.output_layer}\")\n",
        "\n",
        "# 3. TEST create_causal_mask METHOD\n",
        "print(\"\\n3. Testing create_causal_mask() Method:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Test with different sequence lengths\n",
        "test_lengths = [3, 5, 8]\n",
        "\n",
        "for seq_len in test_lengths:\n",
        "    print(f\"\\nTesting causal mask for sequence length {seq_len}:\")\n",
        "    mask = model.create_causal_mask(seq_len)\n",
        "    print(f\"Mask shape: {mask.shape}\")\n",
        "    print(f\"Mask (first batch, first head):\")\n",
        "\n",
        "    # Convert to numpy for easier viewing\n",
        "    mask_np = mask.numpy()[0, 0]  # Remove batch and head dimensions\n",
        "\n",
        "    # Print the mask in a readable format\n",
        "    for i in range(seq_len):\n",
        "        row_str = \"  \"\n",
        "        for j in range(seq_len):\n",
        "            row_str += f\"{int(mask_np[i, j])} \"\n",
        "        print(row_str)\n",
        "\n",
        "    print(f\"Purpose: This mask ensures each position can only attend to previous positions\")\n",
        "    print(f\"1 = can attend, 0 = cannot attend (future positions)\")\n",
        "\n",
        "# 4. CREATE SAMPLE INPUT DATA\n",
        "print(\"\\n4. Creating Sample Input Data:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create sample input sequences (token IDs)\n",
        "sequence_length = 8\n",
        "sample_input = tf.random.uniform(\n",
        "    shape=(batch_size, sequence_length),\n",
        "    maxval=vocab_size,\n",
        "    dtype=tf.int32\n",
        ")\n",
        "\n",
        "print(f\"Sample input shape: {sample_input.shape}\")\n",
        "print(f\"Sample input data (token IDs):\")\n",
        "for i in range(batch_size):\n",
        "    print(f\"  Batch {i}: {sample_input[i].numpy()}\")\n",
        "\n",
        "print(f\"\\nInput explanation:\")\n",
        "print(f\"  - Shape: (batch_size={batch_size}, sequence_length={sequence_length})\")\n",
        "print(f\"  - Values: Random token IDs from 0 to {vocab_size-1}\")\n",
        "print(f\"  - This represents {batch_size} sequences of {sequence_length} tokens each\")\n",
        "\n",
        "# 5. TEST MODEL FORWARD PASS (call method)\n",
        "print(\"\\n5. Testing Model Forward Pass (call method):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Running forward pass...\")\n",
        "try:\n",
        "    # Forward pass\n",
        "    output = model(sample_input, training=False)\n",
        "\n",
        "    print(f\"✓ Forward pass successful!\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Expected shape: (batch_size={batch_size}, seq_len={sequence_length}, vocab_size={vocab_size})\")\n",
        "\n",
        "    print(f\"\\nOutput statistics:\")\n",
        "    print(f\"  Min value: {tf.reduce_min(output):.4f}\")\n",
        "    print(f\"  Max value: {tf.reduce_max(output):.4f}\")\n",
        "    print(f\"  Mean value: {tf.reduce_mean(output):.4f}\")\n",
        "\n",
        "    # Show sample predictions for first sequence, first few positions\n",
        "    print(f\"\\nSample output (logits) for first sequence, first 3 positions:\")\n",
        "    for pos in range(min(3, sequence_length)):\n",
        "        top_3_indices = tf.nn.top_k(output[0, pos], k=3).indices\n",
        "        top_3_values = tf.nn.top_k(output[0, pos], k=3).values\n",
        "        print(f\"  Position {pos}: Top 3 tokens = {top_3_indices.numpy()}, scores = {top_3_values.numpy()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Forward pass failed: {e}\")\n",
        "\n",
        "# 6. TEST WITH TRAINING MODE\n",
        "print(\"\\n6. Testing with Training Mode:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    # Forward pass in training mode\n",
        "    output_train = model(sample_input, training=True)\n",
        "    print(f\"✓ Training mode forward pass successful!\")\n",
        "    print(f\"Training output shape: {output_train.shape}\")\n",
        "\n",
        "    # Compare with inference mode\n",
        "    output_inference = model(sample_input, training=False)\n",
        "\n",
        "    # Check if outputs are different (due to dropout)\n",
        "    difference = tf.reduce_mean(tf.abs(output_train - output_inference))\n",
        "    print(f\"Difference between training and inference: {difference:.6f}\")\n",
        "    print(f\"(Difference > 0 indicates dropout is working)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Training mode failed: {e}\")\n",
        "\n",
        "# 7. MODEL SUMMARY\n",
        "print(\"\\n7. Model Summary:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    # Build the model first by calling it\n",
        "    _ = model(sample_input)\n",
        "\n",
        "    print(\"Model Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = model.count_params()\n",
        "    print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate summary: {e}\")\n",
        "\n",
        "# 8. UNDERSTANDING THE ARCHITECTURE\n",
        "print(\"\\n8. Understanding the GPT Architecture:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"🧠 SIMPLEGPT ARCHITECTURE BREAKDOWN:\")\n",
        "print()\n",
        "print(\"INPUT PROCESSING:\")\n",
        "print(\"  1. Token Embedding: Converts token IDs to dense vectors\")\n",
        "print(\"  2. Position Embedding: Adds positional information to tokens\")\n",
        "print(\"  3. Dropout: Regularization during training\")\n",
        "print()\n",
        "print(\"TRANSFORMER LAYERS (x2):\")\n",
        "print(\"  1. Multi-Head Self-Attention:\")\n",
        "print(\"     - Allows each token to attend to previous tokens\")\n",
        "print(\"     - Uses causal mask to prevent looking at future tokens\")\n",
        "print(\"  2. Layer Normalization + Residual Connection\")\n",
        "print(\"  3. Feed-Forward Network:\")\n",
        "print(\"     - Two dense layers with ReLU activation\")\n",
        "print(\"     - Processes attended information\")\n",
        "print(\"  4. Layer Normalization + Residual Connection\")\n",
        "print(\"  5. Dropout for regularization\")\n",
        "print()\n",
        "print(\"OUTPUT:\")\n",
        "print(\"  - Dense layer projects to vocabulary size\")\n",
        "print(\"  - Each position outputs probability distribution over vocabulary\")\n",
        "print(\"  - Can be used for next token prediction\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TESTING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 9. PRACTICAL USAGE EXAMPLE\n",
        "print(\"\\n9. Practical Usage Example:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Here's how you would typically use this model:\")\n",
        "print()\n",
        "print(\"# For training:\")\n",
        "print(\"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\")\n",
        "print(\"model.fit(train_data, epochs=10)\")\n",
        "print()\n",
        "print(\"# For text generation:\")\n",
        "print(\"def generate_text(model, start_tokens, max_length=20):\")\n",
        "print(\"    for _ in range(max_length):\")\n",
        "print(\"        predictions = model(start_tokens)\")\n",
        "print(\"        next_token = tf.argmax(predictions[:, -1, :], axis=-1)\")\n",
        "print(\"        start_tokens = tf.concat([start_tokens, next_token[:, None]], axis=1)\")\n",
        "print(\"    return start_tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pl4fnyyLbfp7",
        "outputId": "2233ad30-fc77-4510-ca52-6aa6705da9a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TESTING SimpleGPT MODEL\n",
            "======================================================================\n",
            "\n",
            "1. Creating SimpleGPT Model Object:\n",
            "--------------------------------------------------\n",
            "Model Configuration:\n",
            "  Vocabulary Size: 100\n",
            "  Model Dimension (d_model): 64\n",
            "  Number of Attention Heads: 4\n",
            "  Number of Layers: 2\n",
            "  Feed-Forward Dimension: 128\n",
            "  Maximum Sequence Length: 16\n",
            "\n",
            "Model created successfully!\n",
            "Model type: <class '__main__.SimpleGPT'>\n",
            "\n",
            "2. Examining Model Structure:\n",
            "--------------------------------------------------\n",
            "Model attributes:\n",
            "  d_model: 64\n",
            "  max_length: 16\n",
            "  vocab_size: 100\n",
            "  num_heads: 4\n",
            "  num_layers: 2\n",
            "\n",
            "Embedding layers:\n",
            "  Token embedding: <Embedding name=embedding, built=False>\n",
            "  Position embedding: <Embedding name=embedding_1, built=False>\n",
            "\n",
            "Transformer blocks: 2 layers\n",
            "  Layer 1: 6 components\n",
            "    - MultiHeadAttention: <MultiHeadAttention name=multi_head_attention, built=False>\n",
            "    - LayerNormalization: <LayerNormalization name=layer_normalization, built=False>\n",
            "    - Dense (FFN1): <Dense name=dense, built=False>\n",
            "    - Dense (FFN2): <Dense name=dense_1, built=False>\n",
            "    - LayerNormalization: <LayerNormalization name=layer_normalization_1, built=False>\n",
            "    - Dropout: <Dropout name=dropout, built=True>\n",
            "  Layer 2: 6 components\n",
            "    - MultiHeadAttention: <MultiHeadAttention name=multi_head_attention_1, built=False>\n",
            "    - LayerNormalization: <LayerNormalization name=layer_normalization_2, built=False>\n",
            "    - Dense (FFN1): <Dense name=dense_2, built=False>\n",
            "    - Dense (FFN2): <Dense name=dense_3, built=False>\n",
            "    - LayerNormalization: <LayerNormalization name=layer_normalization_3, built=False>\n",
            "    - Dropout: <Dropout name=dropout_1, built=True>\n",
            "\n",
            "Output layer: <Dense name=dense_4, built=False>\n",
            "\n",
            "3. Testing create_causal_mask() Method:\n",
            "--------------------------------------------------\n",
            "\n",
            "Testing causal mask for sequence length 3:\n",
            "Mask shape: (1, 1, 3, 3)\n",
            "Mask (first batch, first head):\n",
            "  1 0 0 \n",
            "  1 1 0 \n",
            "  1 1 1 \n",
            "Purpose: This mask ensures each position can only attend to previous positions\n",
            "1 = can attend, 0 = cannot attend (future positions)\n",
            "\n",
            "Testing causal mask for sequence length 5:\n",
            "Mask shape: (1, 1, 5, 5)\n",
            "Mask (first batch, first head):\n",
            "  1 0 0 0 0 \n",
            "  1 1 0 0 0 \n",
            "  1 1 1 0 0 \n",
            "  1 1 1 1 0 \n",
            "  1 1 1 1 1 \n",
            "Purpose: This mask ensures each position can only attend to previous positions\n",
            "1 = can attend, 0 = cannot attend (future positions)\n",
            "\n",
            "Testing causal mask for sequence length 8:\n",
            "Mask shape: (1, 1, 8, 8)\n",
            "Mask (first batch, first head):\n",
            "  1 0 0 0 0 0 0 0 \n",
            "  1 1 0 0 0 0 0 0 \n",
            "  1 1 1 0 0 0 0 0 \n",
            "  1 1 1 1 0 0 0 0 \n",
            "  1 1 1 1 1 0 0 0 \n",
            "  1 1 1 1 1 1 0 0 \n",
            "  1 1 1 1 1 1 1 0 \n",
            "  1 1 1 1 1 1 1 1 \n",
            "Purpose: This mask ensures each position can only attend to previous positions\n",
            "1 = can attend, 0 = cannot attend (future positions)\n",
            "\n",
            "4. Creating Sample Input Data:\n",
            "--------------------------------------------------\n",
            "Sample input shape: (2, 8)\n",
            "Sample input data (token IDs):\n",
            "  Batch 0: [52  6 66 47 69 42 66 11]\n",
            "  Batch 1: [50 29 57 69 33 74 51 95]\n",
            "\n",
            "Input explanation:\n",
            "  - Shape: (batch_size=2, sequence_length=8)\n",
            "  - Values: Random token IDs from 0 to 99\n",
            "  - This represents 2 sequences of 8 tokens each\n",
            "\n",
            "5. Testing Model Forward Pass (call method):\n",
            "--------------------------------------------------\n",
            "Running forward pass...\n",
            "✓ Forward pass successful!\n",
            "Output shape: (2, 8, 100)\n",
            "Expected shape: (batch_size=2, seq_len=8, vocab_size=100)\n",
            "\n",
            "Output statistics:\n",
            "  Min value: -2.5653\n",
            "  Max value: 2.8902\n",
            "  Mean value: 0.0312\n",
            "\n",
            "Sample output (logits) for first sequence, first 3 positions:\n",
            "  Position 0: Top 3 tokens = [73 37 54], scores = [2.0401268 1.8436693 1.6861988]\n",
            "  Position 1: Top 3 tokens = [90 72 31], scores = [2.457957  1.8884536 1.8431845]\n",
            "  Position 2: Top 3 tokens = [ 5 70 89], scores = [2.1303566 1.9337503 1.8274647]\n",
            "\n",
            "6. Testing with Training Mode:\n",
            "--------------------------------------------------\n",
            "✓ Training mode forward pass successful!\n",
            "Training output shape: (2, 8, 100)\n",
            "Difference between training and inference: 0.292281\n",
            "(Difference > 0 indicates dropout is working)\n",
            "\n",
            "7. Model Summary:\n",
            "--------------------------------------------------\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"simple_gpt\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple_gpt\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_head_attention            │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,640\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization             │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_1           │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_head_attention_1          │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,640\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_2           │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_3           │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m6,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_head_attention            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_1           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_head_attention_1          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_2           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_3           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,868\u001b[0m (315.89 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,868</span> (315.89 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,868\u001b[0m (315.89 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,868</span> (315.89 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total trainable parameters: 80,868\n",
            "\n",
            "8. Understanding the GPT Architecture:\n",
            "--------------------------------------------------\n",
            "🧠 SIMPLEGPT ARCHITECTURE BREAKDOWN:\n",
            "\n",
            "INPUT PROCESSING:\n",
            "  1. Token Embedding: Converts token IDs to dense vectors\n",
            "  2. Position Embedding: Adds positional information to tokens\n",
            "  3. Dropout: Regularization during training\n",
            "\n",
            "TRANSFORMER LAYERS (x2):\n",
            "  1. Multi-Head Self-Attention:\n",
            "     - Allows each token to attend to previous tokens\n",
            "     - Uses causal mask to prevent looking at future tokens\n",
            "  2. Layer Normalization + Residual Connection\n",
            "  3. Feed-Forward Network:\n",
            "     - Two dense layers with ReLU activation\n",
            "     - Processes attended information\n",
            "  4. Layer Normalization + Residual Connection\n",
            "  5. Dropout for regularization\n",
            "\n",
            "OUTPUT:\n",
            "  - Dense layer projects to vocabulary size\n",
            "  - Each position outputs probability distribution over vocabulary\n",
            "  - Can be used for next token prediction\n",
            "\n",
            "======================================================================\n",
            "TESTING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "9. Practical Usage Example:\n",
            "--------------------------------------------------\n",
            "Here's how you would typically use this model:\n",
            "\n",
            "# For training:\n",
            "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
            "model.fit(train_data, epochs=10)\n",
            "\n",
            "# For text generation:\n",
            "def generate_text(model, start_tokens, max_length=20):\n",
            "    for _ in range(max_length):\n",
            "        predictions = model(start_tokens)\n",
            "        next_token = tf.argmax(predictions[:, -1, :], axis=-1)\n",
            "        start_tokens = tf.concat([start_tokens, next_token[:, None]], axis=1)\n",
            "    return start_tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Summary:\n",
        "# Model: \"simple_gpt\"\n",
        "# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "# ┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "# ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "# │ embedding (Embedding)           │ (2, 8, 64)             │         6,400 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ embedding_1 (Embedding)         │ (8, 64)                │         1,024 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ multi_head_attention            │ (2, 8, 64)             │        16,640 │\n",
        "# │ (MultiHeadAttention)            │                        │               │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ layer_normalization             │ (2, 8, 64)             │           128 │\n",
        "# │ (LayerNormalization)            │                        │               │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dense (Dense)                   │ (2, 8, 128)            │         8,320 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dense_1 (Dense)                 │ (2, 8, 64)             │         8,256 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ layer_normalization_1           │ (2, 8, 64)             │           128 │\n",
        "# │ (LayerNormalization)            │                        │               │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dropout (Dropout)               │ ?                      │             0 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ multi_head_attention_1          │ (2, 8, 64)             │        16,640 │\n",
        "# │ (MultiHeadAttention)            │                        │               │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ layer_normalization_2           │ (2, 8, 64)             │           128 │\n",
        "# │ (LayerNormalization)            │                        │               │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dense_2 (Dense)                 │ (2, 8, 128)            │         8,320 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dense_3 (Dense)                 │ (2, 8, 64)             │         8,256 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ layer_normalization_3           │ (2, 8, 64)             │           128 │\n",
        "# │ (LayerNormalization)            │                        │               │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dropout_1 (Dropout)             │ ?                      │             0 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dense_4 (Dense)                 │ (2, 8, 100)            │         6,500 │\n",
        "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "# │ dropout_2 (Dropout)             │ ?                      │             0 │\n",
        "# └─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        "#  Total params: 80,868 (315.89 KB)\n",
        "#  Trainable params: 80,868 (315.89 KB)\n",
        "#  Non-trainable params: 0 (0.00 B)\n",
        "\n",
        "# Total trainable parameters: 80,868\n",
        "\n",
        "\n",
        "#Explain this numbers with justification?\n",
        "\n",
        "# 🎯 Why Token Embedding Has 6,400 Parameters\n",
        "\n",
        "## The Math\n",
        "```\n",
        "vocab_size × d_model = parameters\n",
        "100 × 64 = 6,400 parameters\n",
        "```\n",
        "\n",
        "## 📊 What This Looks Like\n",
        "\n",
        "The token embedding is essentially a **lookup table** (matrix) where:\n",
        "- **Rows**: Each of the 100 possible tokens in vocabulary  \n",
        "- **Columns**: Each of the 64 dimensions in the embedding vector\n",
        "\n",
        "```\n",
        "        Dimension →\n",
        "Token ↓  1    2    3    4   ...  64\n",
        "──────────────────────────────────────\n",
        "  0   │ 0.12 -0.45 0.78 -0.23 ... 0.56 │\n",
        "  1   │ -0.67 0.34 -0.12 0.89 ... -0.45│  \n",
        "  2   │ 0.23 0.91 -0.56 0.12 ... 0.78 │\n",
        "  3   │ -0.34 -0.12 0.67 -0.89 ... 0.23│\n",
        " ... │  ...   ...   ...   ...  ...  ...│\n",
        " 99   │ 0.45 -0.78 0.23 0.56 ... -0.12│\n",
        "──────────────────────────────────────\n",
        "     100 rows × 64 columns = 6,400 numbers\n",
        "```\n",
        "\n",
        "## 🔍 How It Works\n",
        "\n",
        "### Input Process:\n",
        "1. **Token ID comes in**: `[5, 23, 7, 12]` (sequence of token IDs)\n",
        "2. **Lookup happens**: Each ID gets its corresponding row from the matrix\n",
        "3. **Vectors retrieved**:\n",
        "   - Token 5 → `[0.12, -0.45, 0.78, ..., 0.56]` (64 numbers)\n",
        "   - Token 23 → `[-0.67, 0.34, -0.12, ..., -0.45]` (64 numbers)\n",
        "   - Token 7 → `[0.23, 0.91, -0.56, ..., 0.78]` (64 numbers)\n",
        "   - Token 12 → `[-0.34, -0.12, 0.67, ..., 0.23]` (64 numbers)\n",
        "\n",
        "### Why 64 Dimensions?\n",
        "- **Rich Representation**: 64 numbers can capture many aspects of a word's meaning\n",
        "- **Computational Efficiency**: Not too large (like 1024) but not too small (like 8)\n",
        "- **Learning Capacity**: Enough dimensions to distinguish between different token meanings\n",
        "\n",
        "## 🧠 What the Model Learns\n",
        "\n",
        "During training, these 6,400 parameters adjust so that:\n",
        "\n",
        "### Similar Tokens Get Similar Vectors:\n",
        "```\n",
        "\"good\"     → [0.2, 0.8, -0.1, 0.5, ...]\n",
        "\"great\"    → [0.3, 0.7, -0.2, 0.4, ...]  # Similar values!\n",
        "\"excellent\"→ [0.1, 0.9, -0.1, 0.6, ...]\n",
        "```\n",
        "\n",
        "### Different Tokens Get Different Vectors:\n",
        "```\n",
        "\"good\"  → [0.2, 0.8, -0.1, 0.5, ...]\n",
        "\"cat\"   → [-0.4, 0.1, 0.9, -0.2, ...]  # Very different!\n",
        "\"run\"   → [0.6, -0.3, 0.2, 0.8, ...]\n",
        "```\n",
        "\n",
        "## 📈 Parameter Breakdown Across Model\n",
        "\n",
        "| Layer Type | Parameters | Calculation |\n",
        "|------------|------------|-------------|\n",
        "| **Token Embedding** | **6,400** | **100 × 64** |\n",
        "| Position Embedding | 1,024 | 16 × 64 |\n",
        "| Multi-Head Attention | 16,640 | Complex matrix ops |\n",
        "| Feed-Forward | 16,576 | Dense layer weights |\n",
        "| Layer Norm | 128 | Scaling parameters |\n",
        "| Output Layer | 6,500 | 64 × 100 + 100 |\n",
        "| **Total** | **80,868** | |\n",
        "\n",
        "## 💡 Key Insights\n",
        "\n",
        "1. **Embedding = Learned Dictionary**: Each token learns what it \"means\" as a 64-dimensional vector\n",
        "2. **No Fixed Rules**: The model discovers these representations during training\n",
        "3. **Context Aware**: Similar contexts push similar tokens to have similar embeddings\n",
        "4. **Foundation Layer**: All other processing builds on these learned token representations\n",
        "\n",
        "## 🎯 Real Example\n",
        "\n",
        "If your vocabulary included:\n",
        "```\n",
        "Token 0: <PAD>     → [0.0, 0.0, 0.0, ...]     # Padding\n",
        "Token 1: <UNK>     → [0.1, 0.1, 0.1, ...]     # Unknown\n",
        "Token 4: \"love\"    → [0.8, 0.9, 0.2, ...]     # Positive emotion\n",
        "Token 15: \"hate\"   → [-0.8, -0.9, 0.1, ...]   # Opposite emotion\n",
        "Token 23: \"book\"   → [0.3, 0.1, 0.8, ...]     # Object\n",
        "```\n",
        "\n",
        "The model learns these patterns automatically by seeing millions of examples during training!\n"
      ],
      "metadata": {
        "id": "iA6gKSI4zsIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the main GPTTrainer class\n",
        "class GPTTrainer:\n",
        "    \"\"\"Main trainer class for educational GPT demonstration\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=400, d_model=64, num_heads=4, num_layers=2, dff=128, max_length=12):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dff = dff\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.processor = TextProcessor(vocab_size)\n",
        "        self.model = None\n",
        "        self.classification_model = None\n",
        "        self.history = {'pretrain': None, 'finetune': None}\n",
        "\n",
        "    def create_training_data(self):\n",
        "        \"\"\"Create comprehensive training data\"\"\"\n",
        "\n",
        "        pretrain_texts = [\n",
        "            \"I love reading books about science and technology.\",\n",
        "            \"The weather today is sunny and beautiful.\",\n",
        "            \"Learning new things is always exciting and fun.\",\n",
        "            \"Movies and music make me happy and relaxed.\",\n",
        "            \"Good food brings people together for dinner.\",\n",
        "            \"Exercise is important for health and wellness.\",\n",
        "            \"Travel helps us see new places and cultures.\",\n",
        "            \"Education opens doors to many opportunities.\",\n",
        "            \"Art and creativity inspire people every day.\",\n",
        "            \"Friends and family are very important to me.\",\n",
        "            \"Technology changes how we work and live.\",\n",
        "            \"Nature and animals are beautiful and amazing.\",\n",
        "            \"Sports and games bring joy to many people.\",\n",
        "            \"Reading helps improve vocabulary and knowledge.\",\n",
        "            \"Music has the power to change our mood.\",\n",
        "            \"Cooking is both art and science combined.\",\n",
        "            \"Science helps us understand the world better.\",\n",
        "            \"History teaches us about past events.\",\n",
        "            \"Mathematics is useful in everyday life.\",\n",
        "            \"Languages connect people from different countries.\",\n",
        "            \"Gardens and flowers make spaces more beautiful.\",\n",
        "            \"Photography captures important memories forever.\",\n",
        "            \"Writing helps express thoughts and ideas clearly.\",\n",
        "            \"Dancing is a wonderful form of artistic expression.\",\n",
        "            \"Architecture combines beauty with practical function.\"\n",
        "        ]\n",
        "\n",
        "        positive_texts = [\n",
        "            \"I absolutely love this amazing product!\",\n",
        "            \"This is fantastic and wonderful experience.\",\n",
        "            \"Great job and excellent work done here.\",\n",
        "            \"Beautiful day with perfect weather today.\",\n",
        "            \"Amazing food and outstanding service provided.\",\n",
        "            \"Wonderful time with family and friends.\",\n",
        "            \"Excellent quality and great value for money.\",\n",
        "            \"Perfect solution to my problem solved.\",\n",
        "            \"Outstanding performance and great results achieved.\",\n",
        "            \"Incredible experience that exceeded all expectations.\",\n",
        "            \"Fantastic movie with great acting throughout.\",\n",
        "            \"Delicious meal at this wonderful restaurant.\",\n",
        "            \"Amazing vacation with beautiful scenery everywhere.\",\n",
        "            \"Great book that I really enjoyed reading.\",\n",
        "            \"Wonderful music that makes me feel happy.\"\n",
        "        ]\n",
        "\n",
        "        negative_texts = [\n",
        "            \"This is terrible and completely disappointing.\",\n",
        "            \"Awful experience and poor quality service.\",\n",
        "            \"Horrible weather ruined my entire day.\",\n",
        "            \"Bad food and slow service at restaurant.\",\n",
        "            \"Disappointing movie with boring plot line.\",\n",
        "            \"Poor quality product that broke immediately.\",\n",
        "            \"Terrible customer service and rude staff.\",\n",
        "            \"Awful traffic and long delays everywhere.\",\n",
        "            \"Bad news and disappointing results received.\",\n",
        "            \"Horrible experience that wasted my time.\",\n",
        "            \"Terrible book with confusing story line.\",\n",
        "            \"Poor performance and disappointing outcome achieved.\",\n",
        "            \"Bad weather cancelled all outdoor plans.\",\n",
        "            \"Awful mistake that caused many problems.\",\n",
        "            \"Disappointing vacation with many problems encountered.\"\n",
        "        ]\n",
        "\n",
        "        finetune_texts = positive_texts + negative_texts\n",
        "        finetune_labels = [1] * len(positive_texts) + [0] * len(negative_texts)\n",
        "\n",
        "        return pretrain_texts, finetune_texts, finetune_labels\n",
        "\n",
        "    def prepare_sequences(self, texts, seq_length=8):\n",
        "        \"\"\"Create training sequences\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self.processor.encode(text)\n",
        "\n",
        "            for i in range(len(tokens) - seq_length):\n",
        "                if i + seq_length + 1 <= len(tokens):\n",
        "                    seq = tokens[i:i + seq_length + 1]\n",
        "                    sequences.append(seq)\n",
        "\n",
        "            if len(tokens) >= 4 and len(tokens) <= seq_length:\n",
        "                padded = tokens + [0] * (seq_length + 1 - len(tokens))\n",
        "                sequences.append(padded[:seq_length + 1])\n",
        "\n",
        "        inputs = [seq[:-1] for seq in sequences]\n",
        "        targets = [seq[1:] for seq in sequences]\n",
        "\n",
        "        return np.array(inputs), np.array(targets)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build the GPT model\"\"\"\n",
        "        print(\"Building GPT model...\")\n",
        "        self.model = SimpleGPT(\n",
        "            vocab_size=self.vocab_size,\n",
        "            d_model=self.d_model,\n",
        "            num_heads=self.num_heads,\n",
        "            num_layers=self.num_layers,\n",
        "            dff=self.dff,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        dummy_input = tf.constant([[1, 2, 3, 4, 5]])\n",
        "        self.model(dummy_input)\n",
        "\n",
        "        print(f\"Model built with {self.model.count_params()} parameters\")\n",
        "\n",
        "    def pretrain(self, texts, epochs=10, batch_size=8, seq_length=8):\n",
        "        \"\"\"Pre-training phase\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PHASE 1: UNSUPERVISED PRE-TRAINING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        self.processor.build_vocab(texts)\n",
        "        X, y = self.prepare_sequences(texts, seq_length)\n",
        "\n",
        "        print(f\"Created {len(X)} training sequences\")\n",
        "\n",
        "        if len(X) == 0:\n",
        "            raise ValueError(\"No training sequences created!\")\n",
        "\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        print(\"Starting pre-training...\")\n",
        "        history = self.model.fit(\n",
        "            X, y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['pretrain'] = history.history\n",
        "        print(\"Pre-training completed!\")\n",
        "\n",
        "        print(\"\\n📝 Testing text generation:\")\n",
        "        test_prompts = [\"i love\", \"this is\", \"the weather\"]\n",
        "        for prompt in test_prompts:\n",
        "            try:\n",
        "                generated = self.generate_text(prompt, max_length=6)\n",
        "                print(f\"'{prompt}' → '{generated}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"'{prompt}' → Generation failed: {e}\")\n",
        "\n",
        "        return history.history\n",
        "\n",
        "    def finetune_classification(self, texts, labels, epochs=8):\n",
        "        \"\"\"Fine-tuning for classification\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PHASE 2: SUPERVISED FINE-TUNING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be pre-trained first!\")\n",
        "\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            tokens = self.processor.encode(text)\n",
        "            if len(tokens) > self.max_length - 1:\n",
        "                tokens = tokens[:self.max_length - 1]\n",
        "            else:\n",
        "                tokens = tokens + [0] * (self.max_length - 1 - len(tokens))\n",
        "            sequences.append(tokens)\n",
        "\n",
        "        X = np.array(sequences)\n",
        "        y = np.array(labels)\n",
        "\n",
        "        print(f\"Fine-tuning on {len(X)} examples\")\n",
        "\n",
        "        inputs = keras.Input(shape=(self.max_length - 1,))\n",
        "\n",
        "        for layer in self.model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        gpt_features = self.model(inputs)\n",
        "\n",
        "        pooled = layers.GlobalAveragePooling1D()(gpt_features)\n",
        "        dense1 = layers.Dense(32, activation='relu')(pooled)\n",
        "        dropout1 = layers.Dropout(0.5)(dense1)\n",
        "        outputs = layers.Dense(2, activation='softmax')(dropout1)\n",
        "\n",
        "        self.classification_model = keras.Model(inputs, outputs)\n",
        "\n",
        "        self.classification_model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        print(\"Training classification head...\")\n",
        "        history = self.classification_model.fit(\n",
        "            X, y,\n",
        "            epochs=epochs,\n",
        "            batch_size=4,\n",
        "            validation_split=0.2,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['finetune'] = history.history\n",
        "        print(\"Fine-tuning completed!\")\n",
        "\n",
        "        return history.history\n",
        "\n",
        "    def generate_text(self, prompt, max_length=8, temperature=0.8):\n",
        "        \"\"\"Generate text safely\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be trained first!\")\n",
        "\n",
        "        tokens = self.processor.encode(prompt)\n",
        "        if len(tokens) == 0:\n",
        "            tokens = [1]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            current_tokens = tokens[-(self.max_length-1):]\n",
        "\n",
        "            input_tokens = current_tokens + [0] * (self.max_length - 1 - len(current_tokens))\n",
        "            input_tokens = input_tokens[:self.max_length - 1]\n",
        "\n",
        "            input_tensor = tf.constant([input_tokens])\n",
        "            predictions = self.model(input_tensor, training=False)\n",
        "\n",
        "            last_pos = min(len(current_tokens) - 1, self.max_length - 2)\n",
        "            if last_pos >= 0:\n",
        "                logits = predictions[0, last_pos, :]\n",
        "\n",
        "                logits = logits / temperature\n",
        "                probabilities = tf.nn.softmax(logits)\n",
        "                next_token = tf.random.categorical([tf.math.log(probabilities)], 1)[0, 0]\n",
        "                next_token = int(next_token)\n",
        "\n",
        "                if next_token == 0 or next_token >= len(self.processor.vocab):\n",
        "                    break\n",
        "\n",
        "                tokens.append(next_token)\n",
        "\n",
        "        return self.processor.decode(tokens)\n",
        "\n",
        "    def predict_sentiment(self, text):\n",
        "        \"\"\"Predict sentiment safely\"\"\"\n",
        "        if self.classification_model is None:\n",
        "            raise ValueError(\"Classification model not trained!\")\n",
        "\n",
        "        tokens = self.processor.encode(text)\n",
        "        if len(tokens) > self.max_length - 1:\n",
        "            tokens = tokens[:self.max_length - 1]\n",
        "        else:\n",
        "            tokens = tokens + [0] * (self.max_length - 1 - len(tokens))\n",
        "\n",
        "        prediction = self.classification_model(tf.constant([tokens]))\n",
        "        return prediction.numpy()[0]\n",
        "\n",
        "    def evaluate_model(self, test_texts, test_labels):\n",
        "        \"\"\"Evaluate the classification model\"\"\"\n",
        "        print(\"\\n🔍 Model Evaluation:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        correct = 0\n",
        "        total = len(test_texts)\n",
        "\n",
        "        for text, true_label in zip(test_texts, test_labels):\n",
        "            pred_probs = self.predict_sentiment(text)\n",
        "            pred_label = 1 if pred_probs[1] > pred_probs[0] else 0\n",
        "\n",
        "            if pred_label == true_label:\n",
        "                correct += 1\n",
        "\n",
        "            sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
        "            confidence = max(pred_probs) * 100\n",
        "            mark = \"✓\" if pred_label == true_label else \"✗\"\n",
        "\n",
        "            print(f\"{mark} '{text[:40]}{'...' if len(text) > 40 else ''}'\")\n",
        "            print(f\"   → {sentiment} ({confidence:.1f}%)\")\n",
        "\n",
        "        accuracy = correct / total * 100\n",
        "        print(f\"\\n📊 Final Accuracy: {accuracy:.1f}% ({correct}/{total})\")\n",
        "        return accuracy\n",
        "\n",
        "# ============== TESTING THE GPTTrainer CLASS ==============\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE GPTTrainer TESTING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. CREATE GPTTrainer OBJECT\n",
        "print(\"\\n1. Creating GPTTrainer Object:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "trainer = GPTTrainer(\n",
        "    vocab_size=200,    # Smaller for demo\n",
        "    d_model=32,        # Smaller for faster training\n",
        "    num_heads=2,       # Fewer heads\n",
        "    num_layers=1,      # Single layer\n",
        "    dff=64,           # Smaller feed-forward\n",
        "    max_length=10     # Shorter sequences\n",
        ")\n",
        "\n",
        "print(f\"✓ GPTTrainer created with configuration:\")\n",
        "print(f\"  Vocabulary Size: {trainer.vocab_size}\")\n",
        "print(f\"  Model Dimension: {trainer.d_model}\")\n",
        "print(f\"  Attention Heads: {trainer.num_heads}\")\n",
        "print(f\"  Transformer Layers: {trainer.num_layers}\")\n",
        "print(f\"  Feed-Forward Size: {trainer.dff}\")\n",
        "print(f\"  Max Sequence Length: {trainer.max_length}\")\n",
        "\n",
        "print(f\"\\nInitial state:\")\n",
        "print(f\"  Text Processor: {type(trainer.processor)}\")\n",
        "print(f\"  GPT Model: {trainer.model}\")\n",
        "print(f\"  Classification Model: {trainer.classification_model}\")\n",
        "print(f\"  Training History: {trainer.history}\")\n",
        "\n",
        "# 2. TEST create_training_data METHOD\n",
        "print(\"\\n2. Testing create_training_data() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "pretrain_texts, finetune_texts, finetune_labels = trainer.create_training_data()\n",
        "\n",
        "print(f\"✓ Training data created:\")\n",
        "print(f\"  Pre-training texts: {len(pretrain_texts)} samples\")\n",
        "print(f\"  Fine-tuning texts: {len(finetune_texts)} samples\")\n",
        "print(f\"  Fine-tuning labels: {len(finetune_labels)} labels\")\n",
        "\n",
        "print(f\"\\nSample pre-training texts (first 3):\")\n",
        "for i, text in enumerate(pretrain_texts[:3]):\n",
        "    print(f\"  {i+1}. {text}\")\n",
        "\n",
        "print(f\"\\nSample fine-tuning data:\")\n",
        "print(f\"  Positive examples: {sum(finetune_labels)} samples\")\n",
        "print(f\"  Negative examples: {len(finetune_labels) - sum(finetune_labels)} samples\")\n",
        "\n",
        "for i in range(2):\n",
        "    label = \"Positive\" if finetune_labels[i] == 1 else \"Negative\"\n",
        "    print(f\"  {label}: {finetune_texts[i]}\")\n",
        "\n",
        "# 3. TEST prepare_sequences METHOD\n",
        "print(\"\\n3. Testing prepare_sequences() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Build vocabulary first (needed for sequence preparation)\n",
        "trainer.processor.build_vocab(pretrain_texts[:5])  # Use subset for demo\n",
        "\n",
        "# Test with a few texts\n",
        "test_texts = pretrain_texts[:3]\n",
        "X, y = trainer.prepare_sequences(test_texts, seq_length=6)\n",
        "\n",
        "print(f\"✓ Sequences prepared from {len(test_texts)} texts:\")\n",
        "print(f\"  Input sequences (X): {X.shape}\")\n",
        "print(f\"  Target sequences (y): {y.shape}\")\n",
        "\n",
        "print(f\"\\nExample sequences:\")\n",
        "for i in range(min(3, len(X))):\n",
        "    print(f\"  Sequence {i+1}:\")\n",
        "    print(f\"    Input:  {X[i]} → '{trainer.processor.decode(X[i])}'\")\n",
        "    print(f\"    Target: {y[i]} → '{trainer.processor.decode(y[i])}'\")\n",
        "\n",
        "# 4. TEST build_model METHOD\n",
        "print(\"\\n4. Testing build_model() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "trainer.build_model()\n",
        "\n",
        "print(f\"✓ Model built successfully:\")\n",
        "print(f\"  Model type: {type(trainer.model)}\")\n",
        "print(f\"  Model parameters: {trainer.model.count_params():,}\")\n",
        "\n",
        "# Test model with dummy input\n",
        "dummy_input = tf.constant([[1, 2, 3, 4, 5]])\n",
        "dummy_output = trainer.model(dummy_input)\n",
        "print(f\"  Test output shape: {dummy_output.shape}\")\n",
        "print(f\"  Expected shape: [1, 5, {trainer.vocab_size}]\")\n",
        "\n",
        "# 5. DEMO PRE-TRAINING (shortened for demo)\n",
        "print(\"\\n5. Testing pretrain() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(\"Starting mini pre-training session...\")\n",
        "try:\n",
        "    # Use subset for quick demo\n",
        "    demo_texts = pretrain_texts[:10]\n",
        "    history = trainer.pretrain(demo_texts, epochs=3, batch_size=4, seq_length=6)\n",
        "\n",
        "    print(f\"✓ Pre-training completed!\")\n",
        "    print(f\"  Final loss: {history['loss'][-1]:.4f}\")\n",
        "    print(f\"  Final accuracy: {history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Pre-training failed: {e}\")\n",
        "\n",
        "# 6. TEST generate_text METHOD\n",
        "print(\"\\n6. Testing generate_text() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if trainer.model is not None:\n",
        "    test_prompts = [\"i love\", \"the weather\", \"this is\"]\n",
        "\n",
        "    print(\"Testing text generation:\")\n",
        "    for prompt in test_prompts:\n",
        "        try:\n",
        "            generated = trainer.generate_text(prompt, max_length=5, temperature=0.7)\n",
        "            print(f\"  '{prompt}' → '{generated}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"  '{prompt}' → Failed: {e}\")\n",
        "else:\n",
        "    print(\"Model not available for text generation\")\n",
        "\n",
        "# 7. DEMO FINE-TUNING (shortened for demo)\n",
        "print(\"\\n7. Testing finetune_classification() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if trainer.model is not None:\n",
        "    try:\n",
        "        # Use subset for quick demo\n",
        "        demo_finetune_texts = finetune_texts[:20]\n",
        "        demo_finetune_labels = finetune_labels[:20]\n",
        "\n",
        "        print(\"Starting mini fine-tuning session...\")\n",
        "        ft_history = trainer.finetune_classification(\n",
        "            demo_finetune_texts,\n",
        "            demo_finetune_labels,\n",
        "            epochs=3\n",
        "        )\n",
        "\n",
        "        print(f\"✓ Fine-tuning completed!\")\n",
        "        print(f\"  Final loss: {ft_history['loss'][-1]:.4f}\")\n",
        "        print(f\"  Final accuracy: {ft_history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Fine-tuning failed: {e}\")\n",
        "else:\n",
        "    print(\"Model not available for fine-tuning\")\n",
        "\n",
        "# 8. TEST predict_sentiment METHOD\n",
        "print(\"\\n8. Testing predict_sentiment() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if trainer.classification_model is not None:\n",
        "    test_sentences = [\n",
        "        \"This is amazing and wonderful!\",\n",
        "        \"I hate this terrible product.\",\n",
        "        \"Great job and excellent work!\",\n",
        "        \"Awful experience and poor service.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Testing sentiment prediction:\")\n",
        "    for sentence in test_sentences:\n",
        "        try:\n",
        "            probs = trainer.predict_sentiment(sentence)\n",
        "            sentiment = \"Positive\" if probs[1] > probs[0] else \"Negative\"\n",
        "            confidence = max(probs) * 100\n",
        "\n",
        "            print(f\"  '{sentence}'\")\n",
        "            print(f\"    → {sentiment} ({confidence:.1f}% confidence)\")\n",
        "            print(f\"    → Probabilities: [Neg: {probs[0]:.3f}, Pos: {probs[1]:.3f}]\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  '{sentence}' → Failed: {e}\")\n",
        "else:\n",
        "    print(\"Classification model not available\")\n",
        "\n",
        "# 9. TEST evaluate_model METHOD\n",
        "print(\"\\n9. Testing evaluate_model() Method:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if trainer.classification_model is not None:\n",
        "    test_eval_texts = [\n",
        "        \"This is fantastic and amazing!\",\n",
        "        \"Terrible and disappointing experience.\",\n",
        "        \"Great quality and excellent service!\",\n",
        "        \"Poor performance and bad results.\"\n",
        "    ]\n",
        "    test_eval_labels = [1, 0, 1, 0]  # 1=positive, 0=negative\n",
        "\n",
        "    try:\n",
        "        accuracy = trainer.evaluate_model(test_eval_texts, test_eval_labels)\n",
        "        print(f\"✓ Evaluation completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Evaluation failed: {e}\")\n",
        "else:\n",
        "    print(\"Classification model not available for evaluation\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GPTTrainer TESTING COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 10. SUMMARY OF WHAT WE LEARNED\n",
        "print(\"\\n10. Summary - What Each Method Does:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(\"🏗️  GPTTrainer.__init__(): Sets up trainer with model configuration\")\n",
        "print(\"📚  create_training_data(): Generates pre-training and classification datasets\")\n",
        "print(\"🔢  prepare_sequences(): Converts texts to training sequences for language modeling\")\n",
        "print(\"🧠  build_model(): Creates the SimpleGPT neural network\")\n",
        "print(\"📖  pretrain(): Trains model to predict next tokens (unsupervised learning)\")\n",
        "print(\"🎯  finetune_classification(): Adds classification head for sentiment analysis\")\n",
        "print(\"✍️   generate_text(): Uses trained model to generate new text\")\n",
        "print(\"😊  predict_sentiment(): Classifies text as positive/negative\")\n",
        "print(\"📊  evaluate_model(): Tests classification accuracy on new data\")\n",
        "\n",
        "print(\"\\n🎓 LEARNING OUTCOMES:\")\n",
        "print(\"   ✓ Understand GPT training pipeline\")\n",
        "print(\"   ✓ See pre-training → fine-tuning workflow\")\n",
        "print(\"   ✓ Learn autoregressive text generation\")\n",
        "print(\"   ✓ Experience transfer learning in NLP\")\n",
        "print(\"   ✓ Practice model evaluation techniques\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJv75ahAkuyB",
        "outputId": "7d0bcd9e-7cc9-488f-9abc-145166fa7670"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPREHENSIVE GPTTrainer TESTING\n",
            "================================================================================\n",
            "\n",
            "1. Creating GPTTrainer Object:\n",
            "------------------------------------------------------------\n",
            "Constructor Called\n",
            "Constructor Initialized\n",
            "✓ GPTTrainer created with configuration:\n",
            "  Vocabulary Size: 200\n",
            "  Model Dimension: 32\n",
            "  Attention Heads: 2\n",
            "  Transformer Layers: 1\n",
            "  Feed-Forward Size: 64\n",
            "  Max Sequence Length: 10\n",
            "\n",
            "Initial state:\n",
            "  Text Processor: <class '__main__.TextProcessor'>\n",
            "  GPT Model: None\n",
            "  Classification Model: None\n",
            "  Training History: {'pretrain': None, 'finetune': None}\n",
            "\n",
            "2. Testing create_training_data() Method:\n",
            "------------------------------------------------------------\n",
            "✓ Training data created:\n",
            "  Pre-training texts: 25 samples\n",
            "  Fine-tuning texts: 30 samples\n",
            "  Fine-tuning labels: 30 labels\n",
            "\n",
            "Sample pre-training texts (first 3):\n",
            "  1. I love reading books about science and technology.\n",
            "  2. The weather today is sunny and beautiful.\n",
            "  3. Learning new things is always exciting and fun.\n",
            "\n",
            "Sample fine-tuning data:\n",
            "  Positive examples: 15 samples\n",
            "  Negative examples: 15 samples\n",
            "  Positive: I absolutely love this amazing product!\n",
            "  Positive: This is fantastic and wonderful experience.\n",
            "\n",
            "3. Testing prepare_sequences() Method:\n",
            "------------------------------------------------------------\n",
            "Building vocabulary...\n",
            "Vocabulary size: 59\n",
            "✓ Sequences prepared from 3 texts:\n",
            "  Input sequences (X): (5, 6)\n",
            "  Target sequences (y): (5, 6)\n",
            "\n",
            "Example sequences:\n",
            "  Sequence 1:\n",
            "    Input:  [ 6  7  8  9 10 11] → 'i love reading books about science'\n",
            "    Target: [ 7  8  9 10 11  4] → 'love reading books about science and'\n",
            "  Sequence 2:\n",
            "    Input:  [ 7  8  9 10 11  4] → 'love reading books about science and'\n",
            "    Target: [ 8  9 10 11  4 12] → 'reading books about science and technology.'\n",
            "  Sequence 3:\n",
            "    Input:  [13 14 15  5 16  4] → 'the weather today is sunny and'\n",
            "    Target: [14 15  5 16  4 17] → 'weather today is sunny and beautiful.'\n",
            "\n",
            "4. Testing build_model() Method:\n",
            "------------------------------------------------------------\n",
            "Building GPT model...\n",
            "Model built with 21864 parameters\n",
            "✓ Model built successfully:\n",
            "  Model type: <class '__main__.SimpleGPT'>\n",
            "  Model parameters: 21,864\n",
            "  Test output shape: (1, 5, 200)\n",
            "  Expected shape: [1, 5, 200]\n",
            "\n",
            "5. Testing pretrain() Method:\n",
            "------------------------------------------------------------\n",
            "Starting mini pre-training session...\n",
            "\n",
            "==================================================\n",
            "PHASE 1: UNSUPERVISED PRE-TRAINING\n",
            "==================================================\n",
            "Building vocabulary...\n",
            "Vocabulary size: 82\n",
            "Created 15 training sequences\n",
            "Starting pre-training...\n",
            "Epoch 1/3\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - accuracy: 0.0000e+00 - loss: 5.3035 - val_accuracy: 0.0000e+00 - val_loss: 5.4595\n",
            "Epoch 2/3\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0627 - loss: 4.9879 - val_accuracy: 0.0000e+00 - val_loss: 5.4224\n",
            "Epoch 3/3\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.1671 - loss: 4.7151 - val_accuracy: 0.0833 - val_loss: 5.3826\n",
            "Pre-training completed!\n",
            "\n",
            "📝 Testing text generation:\n",
            "'i love' → 'i love opens me.'\n",
            "'this is' → 'this is'\n",
            "'the weather' → 'the weather'\n",
            "✓ Pre-training completed!\n",
            "  Final loss: 4.7346\n",
            "  Final accuracy: 0.1538\n",
            "\n",
            "6. Testing generate_text() Method:\n",
            "------------------------------------------------------------\n",
            "Testing text generation:\n",
            "  'i love' → 'i love and'\n",
            "  'the weather' → 'the weather places'\n",
            "  'this is' → 'this is me. beautiful.'\n",
            "\n",
            "7. Testing finetune_classification() Method:\n",
            "------------------------------------------------------------\n",
            "Starting mini fine-tuning session...\n",
            "\n",
            "==================================================\n",
            "PHASE 2: SUPERVISED FINE-TUNING\n",
            "==================================================\n",
            "Fine-tuning on 20 examples\n",
            "Training classification head...\n",
            "Epoch 1/3\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.2500 - loss: 0.9680 - val_accuracy: 0.0000e+00 - val_loss: 1.0090\n",
            "Epoch 2/3\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7917 - loss: 0.4996 - val_accuracy: 0.0000e+00 - val_loss: 1.7211\n",
            "Epoch 3/3\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9333 - loss: 0.3571 - val_accuracy: 0.0000e+00 - val_loss: 2.2458\n",
            "Fine-tuning completed!\n",
            "✓ Fine-tuning completed!\n",
            "  Final loss: 0.3286\n",
            "  Final accuracy: 0.0000\n",
            "\n",
            "8. Testing predict_sentiment() Method:\n",
            "------------------------------------------------------------\n",
            "Testing sentiment prediction:\n",
            "  'This is amazing and wonderful!'\n",
            "    → Positive (86.9% confidence)\n",
            "    → Probabilities: [Neg: 0.131, Pos: 0.869]\n",
            "  'I hate this terrible product.'\n",
            "    → Positive (89.9% confidence)\n",
            "    → Probabilities: [Neg: 0.101, Pos: 0.899]\n",
            "  'Great job and excellent work!'\n",
            "    → Positive (88.6% confidence)\n",
            "    → Probabilities: [Neg: 0.114, Pos: 0.886]\n",
            "  'Awful experience and poor service.'\n",
            "    → Positive (89.2% confidence)\n",
            "    → Probabilities: [Neg: 0.108, Pos: 0.892]\n",
            "\n",
            "9. Testing evaluate_model() Method:\n",
            "------------------------------------------------------------\n",
            "\n",
            "🔍 Model Evaluation:\n",
            "----------------------------------------\n",
            "✓ 'This is fantastic and amazing!'\n",
            "   → Positive (86.9%)\n",
            "✗ 'Terrible and disappointing experience.'\n",
            "   → Positive (87.4%)\n",
            "✓ 'Great quality and excellent service!'\n",
            "   → Positive (88.6%)\n",
            "✗ 'Poor performance and bad results.'\n",
            "   → Positive (88.9%)\n",
            "\n",
            "📊 Final Accuracy: 50.0% (2/4)\n",
            "✓ Evaluation completed!\n",
            "\n",
            "================================================================================\n",
            "GPTTrainer TESTING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "10. Summary - What Each Method Does:\n",
            "------------------------------------------------------------\n",
            "🏗️  GPTTrainer.__init__(): Sets up trainer with model configuration\n",
            "📚  create_training_data(): Generates pre-training and classification datasets\n",
            "🔢  prepare_sequences(): Converts texts to training sequences for language modeling\n",
            "🧠  build_model(): Creates the SimpleGPT neural network\n",
            "📖  pretrain(): Trains model to predict next tokens (unsupervised learning)\n",
            "🎯  finetune_classification(): Adds classification head for sentiment analysis\n",
            "✍️   generate_text(): Uses trained model to generate new text\n",
            "😊  predict_sentiment(): Classifies text as positive/negative\n",
            "📊  evaluate_model(): Tests classification accuracy on new data\n",
            "\n",
            "🎓 LEARNING OUTCOMES:\n",
            "   ✓ Understand GPT training pipeline\n",
            "   ✓ See pre-training → fine-tuning workflow\n",
            "   ✓ Learn autoregressive text generation\n",
            "   ✓ Experience transfer learning in NLP\n",
            "   ✓ Practice model evaluation techniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main demonstration function\"\"\"\n",
        "    print(\"🤖 GPT Training Demonstration\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = GPTTrainer(\n",
        "        vocab_size=300,\n",
        "        d_model=48,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        dff=96,\n",
        "        max_length=10\n",
        "    )\n",
        "\n",
        "    # Get training data\n",
        "    pretrain_texts, finetune_texts, finetune_labels = trainer.create_training_data()\n",
        "\n",
        "    print(f\"📚 Training data prepared:\")\n",
        "    print(f\"  - Pre-training texts: {len(pretrain_texts)}\")\n",
        "    print(f\"  - Fine-tuning examples: {len(finetune_texts)}\")\n",
        "\n",
        "    # Phase 1: Pre-training\n",
        "    trainer.pretrain(pretrain_texts, epochs=8, seq_length=6)\n",
        "\n",
        "    # Phase 2: Fine-tuning\n",
        "    trainer.finetune_classification(finetune_texts, finetune_labels, epochs=6)\n",
        "\n",
        "    # Evaluation\n",
        "    test_texts = [\n",
        "        \"I love this wonderful experience!\",\n",
        "        \"This is terrible and awful.\",\n",
        "        \"Great job and amazing work!\",\n",
        "        \"Poor quality and disappointing.\",\n",
        "        \"Fantastic results and excellent!\",\n",
        "        \"Bad service and horrible experience.\"\n",
        "    ]\n",
        "    test_labels = [1, 0, 1, 0, 1, 0]\n",
        "\n",
        "    accuracy = trainer.evaluate_model(test_texts, test_labels)\n",
        "\n",
        "    # Final text generation test\n",
        "    print(\"\\n📝 Final Text Generation:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    final_prompts = [\"i love\", \"this is great\", \"the weather\"]\n",
        "    for prompt in final_prompts:\n",
        "        try:\n",
        "            generated = trainer.generate_text(prompt, max_length=5)\n",
        "            print(f\"'{prompt}' → '{generated}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"'{prompt}' → Error: {e}\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n🎯 DEMONSTRATION COMPLETE!\")\n",
        "    print(f\"📊 Classification Accuracy: {accuracy:.1f}%\")\n",
        "    print(f\"🧠 Model Parameters: {trainer.model.count_params():,}\")\n",
        "    print(f\"📖 Vocabulary Size: {len(trainer.processor.vocab)}\")\n",
        "\n",
        "    if accuracy > 70:\n",
        "        print(\"🎉 Excellent! The model learned effectively!\")\n",
        "    elif accuracy > 50:\n",
        "        print(\"👍 Good! The model shows clear learning!\")\n",
        "    else:\n",
        "        print(\"📚 Model demonstrates core concepts!\")\n",
        "\n",
        "    print(\"\\nKey concepts demonstrated:\")\n",
        "    print(\"✓ Transformer architecture\")\n",
        "    print(\"✓ Self-attention mechanisms\")\n",
        "    print(\"✓ Transfer learning pipeline\")\n",
        "    print(\"✓ Language model pre-training\")\n",
        "    print(\"✓ Supervised fine-tuning\")\n",
        "    print(\"✓ Text generation\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsNmqBmhmSiD",
        "outputId": "1dab86f8-96b9-4bb9-d39e-696d79750512"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 GPT Training Demonstration\n",
            "==================================================\n",
            "Constructor Called\n",
            "Constructor Initialized\n",
            "📚 Training data prepared:\n",
            "  - Pre-training texts: 25\n",
            "  - Fine-tuning examples: 30\n",
            "\n",
            "==================================================\n",
            "PHASE 1: UNSUPERVISED PRE-TRAINING\n",
            "==================================================\n",
            "Building vocabulary...\n",
            "Vocabulary size: 147\n",
            "Created 33 training sequences\n",
            "Building GPT model...\n",
            "Model built with 67500 parameters\n",
            "Starting pre-training...\n",
            "Epoch 1/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 281ms/step - accuracy: 0.0083 - loss: 5.7472 - val_accuracy: 0.0000e+00 - val_loss: 5.8358\n",
            "Epoch 2/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.0627 - loss: 5.2620 - val_accuracy: 0.0417 - val_loss: 5.8016\n",
            "Epoch 3/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1210 - loss: 4.9465 - val_accuracy: 0.0417 - val_loss: 5.7897\n",
            "Epoch 4/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1212 - loss: 4.7307 - val_accuracy: 0.0417 - val_loss: 5.7851\n",
            "Epoch 5/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1620 - loss: 4.5033 - val_accuracy: 0.0417 - val_loss: 5.8014\n",
            "Epoch 6/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2341 - loss: 4.1969 - val_accuracy: 0.0417 - val_loss: 5.8111\n",
            "Epoch 7/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3010 - loss: 3.9902 - val_accuracy: 0.0417 - val_loss: 5.8411\n",
            "Epoch 8/8\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3394 - loss: 3.8254 - val_accuracy: 0.0417 - val_loss: 5.8751\n",
            "Pre-training completed!\n",
            "\n",
            "📝 Testing text generation:\n",
            "'i love' → 'i love'\n",
            "'this is' → 'this is'\n",
            "'the weather' → 'the weather useful mood. and make'\n",
            "\n",
            "==================================================\n",
            "PHASE 2: SUPERVISED FINE-TUNING\n",
            "==================================================\n",
            "Fine-tuning on 30 examples\n",
            "Training classification head...\n",
            "Epoch 1/6\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.3506 - loss: 1.8167 - val_accuracy: 1.0000 - val_loss: 0.6060\n",
            "Epoch 2/6\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6512 - loss: 0.6093 - val_accuracy: 0.8333 - val_loss: 0.6405\n",
            "Epoch 3/6\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4024 - loss: 0.9016 - val_accuracy: 0.0000e+00 - val_loss: 1.5015\n",
            "Epoch 4/6\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6452 - loss: 0.6696 - val_accuracy: 0.0000e+00 - val_loss: 1.5907\n",
            "Epoch 5/6\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6762 - loss: 0.7311 - val_accuracy: 0.0000e+00 - val_loss: 1.2356\n",
            "Epoch 6/6\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5071 - loss: 0.8298 - val_accuracy: 0.5000 - val_loss: 0.7152\n",
            "Fine-tuning completed!\n",
            "\n",
            "🔍 Model Evaluation:\n",
            "----------------------------------------\n",
            "✓ 'I love this wonderful experience!'\n",
            "   → Positive (56.9%)\n",
            "✗ 'This is terrible and awful.'\n",
            "   → Positive (57.0%)\n",
            "✓ 'Great job and amazing work!'\n",
            "   → Positive (54.8%)\n",
            "✗ 'Poor quality and disappointing.'\n",
            "   → Positive (55.7%)\n",
            "✓ 'Fantastic results and excellent!'\n",
            "   → Positive (55.7%)\n",
            "✗ 'Bad service and horrible experience.'\n",
            "   → Positive (57.3%)\n",
            "\n",
            "📊 Final Accuracy: 50.0% (3/6)\n",
            "\n",
            "📝 Final Text Generation:\n",
            "------------------------------\n",
            "'i love' → 'i love'\n",
            "'this is great' → 'this is great make the from'\n",
            "'the weather' → 'the weather'\n",
            "\n",
            "🎯 DEMONSTRATION COMPLETE!\n",
            "📊 Classification Accuracy: 50.0%\n",
            "🧠 Model Parameters: 67,500\n",
            "📖 Vocabulary Size: 147\n",
            "📚 Model demonstrates core concepts!\n",
            "\n",
            "Key concepts demonstrated:\n",
            "✓ Transformer architecture\n",
            "✓ Self-attention mechanisms\n",
            "✓ Transfer learning pipeline\n",
            "✓ Language model pre-training\n",
            "✓ Supervised fine-tuning\n",
            "✓ Text generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompts = [\"Cooking is a \", \"life is beautiful\", \"the beauty of knowledge\"]\n",
        "for prompt in final_prompts:\n",
        "  try:\n",
        "    generated = trainer.generate_text(prompt, max_length=50)\n",
        "    print(f\"'{prompt}' → '{generated}'\")\n",
        "  except Exception as e:\n",
        "    print(f\"'{prompt}' → Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpksp8Tfp9E9",
        "outputId": "7bdd2b54-d9f1-469c-9395-3f4089bfc0d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Cooking is a ' → Error: name 'trainer' is not defined\n",
            "'life is beautiful' → Error: name 'trainer' is not defined\n",
            "'the beauty of knowledge' → Error: name 'trainer' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How to train the structured data in the domain specific cases in organization?"
      ],
      "metadata": {
        "id": "UQeoKOO-rgH5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain-Specific GPT Pretraining with Structured Data\n",
        "## Strategic Guide for Organizations\n",
        "\n",
        "### **🎯 The Challenge: From Tables to Text**\n",
        "\n",
        "Traditional GPT models are trained on natural language text, but organizational data is often in structured formats:\n",
        "- **Databases**: Customer records, financial transactions, inventory data\n",
        "- **CSV files**: Sales reports, employee data, operational metrics  \n",
        "- **Spreadsheets**: Financial models, project tracking, performance data\n",
        "- **JSON/XML**: API responses, configuration files, log data\n",
        "\n",
        "**The key insight**: We need to convert structured data into meaningful text that GPT can learn from.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔄 The Structured Data → Text Transformation Process**\n",
        "\n",
        "### **1. Data Serialization Strategy**\n",
        "\n",
        "**Turn Tables into Stories**\n",
        "\n",
        "Instead of keeping data in rows and columns, transform it into natural language descriptions:\n",
        "\n",
        "**Before (CSV format):**\n",
        "```\n",
        "Customer_ID, Name, Purchase_Amount, Product, Date, Satisfaction\n",
        "12345, John Smith, $299.99, Laptop, 2024-01-15, 4.5\n",
        "```\n",
        "\n",
        "**After (Text format):**\n",
        "```\n",
        "\"Customer John Smith (ID: 12345) purchased a Laptop for $299.99 on January 15th, 2024.\n",
        "The customer rated their satisfaction as 4.5 out of 5 stars, indicating high satisfaction\n",
        "with the purchase experience.\"\n",
        "```\n",
        "\n",
        "### **2. Context-Rich Data Narratives**\n",
        "\n",
        "**Add Business Context**\n",
        "\n",
        "Transform raw data points into business-meaningful narratives:\n",
        "\n",
        "**Financial Data Example:**\n",
        "- **Raw**: `Q1_Revenue: $2.5M, Growth: 15%, Profit_Margin: 22%`\n",
        "- **Narrative**: `\"In Q1, the company achieved $2.5 million in revenue, representing a 15% growth compared to the previous quarter. The profit margin of 22% indicates strong operational efficiency and cost management.\"`\n",
        "\n",
        "**HR Data Example:**\n",
        "- **Raw**: `Employee_ID: 789, Department: Engineering, Performance: 4.2, Promotion: Yes`\n",
        "- **Narrative**: `\"Employee 789 from the Engineering department achieved a performance rating of 4.2 out of 5.0, demonstrating excellent work quality and was approved for promotion based on consistent high performance.\"`\n",
        "\n",
        "\n",
        "## **🏢 Organization-Specific Implementation Strategies**\n",
        "\n",
        "### **For Financial Services:**\n",
        "\n",
        "**Transform Trading Data:**\n",
        "- **Raw**: `AAPL, 150.25, +2.3%, 1M_shares, 09:30`\n",
        "- **Narrative**: `\"Apple (AAPL) opened at $150.25, up 2.3% from yesterday's close. Trading volume of 1 million shares at 9:30 AM indicates strong investor interest, potentially driven by positive earnings expectations.\"`\n",
        "\n",
        "**Transform Credit Risk Data:**\n",
        "- **Raw**: `Customer_567, Credit_Score: 720, Debt_Ratio: 0.35, Income: $75K`\n",
        "- **Narrative**: `\"Customer 567 presents a moderate credit risk profile with a credit score of 720, indicating good creditworthiness. Their debt-to-income ratio of 35% is within acceptable limits for their $75,000 annual income.\"`\n",
        "\n",
        "### **For Healthcare Organizations:**\n",
        "\n",
        "**Transform Patient Data:**\n",
        "- **Raw**: `Patient_123, Age: 45, BP: 140/90, BMI: 28.5, Risk: Medium`\n",
        "- **Narrative**: `\"45-year-old patient presents with elevated blood pressure (140/90) and BMI of 28.5, indicating overweight status. Combined factors suggest medium cardiovascular risk requiring lifestyle modifications and regular monitoring.\"`\n",
        "\n",
        "### **For Retail Companies:**\n",
        "\n",
        "**Transform Inventory Data:**\n",
        "- **Raw**: `SKU_789, Stock: 45, Reorder: 20, Sales_Velocity: 5/day`\n",
        "- **Narrative**: `\"Product SKU-789 currently has 45 units in stock with a reorder point set at 20 units. Based on current sales velocity of 5 units per day, the product will reach reorder levels in 5 days, ensuring adequate inventory for customer demand.\"`\n",
        "\n",
        "### **For Manufacturing:**\n",
        "\n",
        "**Transform Production Data:**\n",
        "- **Raw**: `Line_A, Efficiency: 87%, Downtime: 2hrs, Output: 1250_units`\n",
        "- **Narrative**: `\"Production Line A operated at 87% efficiency today with 2 hours of planned maintenance downtime. The line produced 1,250 units, meeting daily targets despite the maintenance window.\"`\n"
      ],
      "metadata": {
        "id": "dJAc7GFPyN_R"
      }
    }
  ]
}